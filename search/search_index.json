{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Ben's Code Journal May it aid you in your coding adventures if a google search lands you here. aws_code-journal","title":"Home"},{"location":"#welcome-to-bens-code-journal","text":"May it aid you in your coding adventures if a google search lands you here. aws_code-journal","title":"Welcome to Ben's Code Journal"},{"location":"notes/MKDocs/Quick%20Notes/","text":"Quick Notes Deploy the server\\site Be sure you are in ./code-journal/ or ./<project root> code-journal |-docs |-site |-mkdocs.yml Benjamins-MBP-2:code-journal bbearce$ pwd /Users/bbearce/Documents/Code/code-journal Use this code to serve the developer site $ mkdocs serve Use this code to build the site $ mkdocs build To push to github do this: cd ../bbearce.github.io/ mkdocs gh-deploy --config-file ../code-journal/mkdocs.yml --remote-branch master Detailed Github Notes github deploy Note for the venv pip freeze lists these packages: Click==7.0 Jinja2==2.10.1 livereload==2.6.1 Markdown==3.1.1 MarkupSafe==1.1.1 mkdocs==1.0.4 mkdocs-rtd-dropdown==1.0.2 mkdocs-windmill-dark==0.2.0 PyYAML==5.1.2 six==1.12.0 tornado==6.0.3 I am pretty sure I only installed mkdocs and mkdocs-windmill-dark","title":"MKDocs"},{"location":"notes/MKDocs/Quick%20Notes/#quick-notes","text":"","title":"Quick Notes"},{"location":"notes/MKDocs/Quick%20Notes/#deploy-the-serversite","text":"Be sure you are in ./code-journal/ or ./<project root> code-journal |-docs |-site |-mkdocs.yml Benjamins-MBP-2:code-journal bbearce$ pwd /Users/bbearce/Documents/Code/code-journal Use this code to serve the developer site $ mkdocs serve Use this code to build the site $ mkdocs build To push to github do this: cd ../bbearce.github.io/ mkdocs gh-deploy --config-file ../code-journal/mkdocs.yml --remote-branch master","title":"Deploy the server\\site"},{"location":"notes/MKDocs/Quick%20Notes/#detailed-github-notes","text":"github deploy","title":"Detailed Github Notes"},{"location":"notes/MKDocs/Quick%20Notes/#note-for-the-venv","text":"pip freeze lists these packages: Click==7.0 Jinja2==2.10.1 livereload==2.6.1 Markdown==3.1.1 MarkupSafe==1.1.1 mkdocs==1.0.4 mkdocs-rtd-dropdown==1.0.2 mkdocs-windmill-dark==0.2.0 PyYAML==5.1.2 six==1.12.0 tornado==6.0.3 I am pretty sure I only installed mkdocs and mkdocs-windmill-dark","title":"Note for the venv"},{"location":"notes/algorithms/python/alphabet_rangoli/","text":"Alphabet Rangoli An alphabet diamond design in a square You are given an integer, N. Your task is to print an alphabet rangoli of size N. (Rangoli is a form of Indian folk art based on creation of patterns.) Different sizes of alphabet rangoli are shown below: #size 3 -> 9 ----c---- --c-b-c-- c-b-a-b-c --c-b-c-- ----c---- #size 5 -> 17 --------e-------- ------e-d-e------ ----e-d-c-d-e---- --e-d-c-b-c-d-e-- e-d-c-b-a-b-c-d-e --e-d-c-b-c-d-e-- ----e-d-c-d-e---- ------e-d-e------ --------e-------- #size 10 -> 37 ------------------j------------------ ----------------j-i-j---------------- --------------j-i-h-i-j-------------- ------------j-i-h-g-h-i-j------------ ----------j-i-h-g-f-g-h-i-j---------- --------j-i-h-g-f-e-f-g-h-i-j-------- ------j-i-h-g-f-e-d-e-f-g-h-i-j------ ----j-i-h-g-f-e-d-c-d-e-f-g-h-i-j---- --j-i-h-g-f-e-d-c-b-c-d-e-f-g-h-i-j-- j-i-h-g-f-e-d-c-b-a-b-c-d-e-f-g-h-i-j --j-i-h-g-f-e-d-c-b-c-d-e-f-g-h-i-j-- ----j-i-h-g-f-e-d-c-d-e-f-g-h-i-j---- ------j-i-h-g-f-e-d-e-f-g-h-i-j------ --------j-i-h-g-f-e-f-g-h-i-j-------- ----------j-i-h-g-f-g-h-i-j---------- ------------j-i-h-g-h-i-j------------ --------------j-i-h-i-j-------------- ----------------j-i-j---------------- ------------------j------------------ The center of the rangoli has the first alphabet letter a, and the boundary has the Nth alphabet letter (in alphabetical order). Input Format Only one line of input containing N, the size of the rangoli. Constraints 0< N< 27 Output Format Print the alphabet rangoli in the format explained above. Sample Input 5 Sample Output --------e-------- ------e-d-e------ ----e-d-c-d-e---- --e-d-c-b-c-d-e-- e-d-c-b-a-b-c-d-e --e-d-c-b-c-d-e-- ----e-d-c-d-e---- ------e-d-e------ --------e-------- The code: import string def print_rangoli(size): # your code goes here N = size width = 4*(N-1)+1 letters=string.ascii_letters[0:26] # triangle up for i in range(N-1): s = '{}'.format(letters[N-i-1]) for n in range(i): s = '{}-'.format(letters[N-i+n]) + s + '-{}'.format(letters[N-i+n]) s = s.center(width, '-') print(s); del(s) # center s = '{}'.format(letters[0]) for n in range(N-1): s = '{}-'.format(letters[n+1]) + s + '-{}'.format(letters[n+1]) print(s); del(s) # triangle down for i in range(N-2,-1,-1): s = '{}'.format(letters[N-i-1]) for n in range(i): s = '{}-'.format(letters[N-i+n]) + s + '-{}'.format(letters[N-i+n]) s = s.center(width, '-') print(s); del(s) if __name__ == '__main__': n = int(input()) print_rangoli(n)","title":"Alphabet Rangoli"},{"location":"notes/algorithms/python/alphabet_rangoli/#alphabet-rangoli","text":"An alphabet diamond design in a square You are given an integer, N. Your task is to print an alphabet rangoli of size N. (Rangoli is a form of Indian folk art based on creation of patterns.) Different sizes of alphabet rangoli are shown below: #size 3 -> 9 ----c---- --c-b-c-- c-b-a-b-c --c-b-c-- ----c---- #size 5 -> 17 --------e-------- ------e-d-e------ ----e-d-c-d-e---- --e-d-c-b-c-d-e-- e-d-c-b-a-b-c-d-e --e-d-c-b-c-d-e-- ----e-d-c-d-e---- ------e-d-e------ --------e-------- #size 10 -> 37 ------------------j------------------ ----------------j-i-j---------------- --------------j-i-h-i-j-------------- ------------j-i-h-g-h-i-j------------ ----------j-i-h-g-f-g-h-i-j---------- --------j-i-h-g-f-e-f-g-h-i-j-------- ------j-i-h-g-f-e-d-e-f-g-h-i-j------ ----j-i-h-g-f-e-d-c-d-e-f-g-h-i-j---- --j-i-h-g-f-e-d-c-b-c-d-e-f-g-h-i-j-- j-i-h-g-f-e-d-c-b-a-b-c-d-e-f-g-h-i-j --j-i-h-g-f-e-d-c-b-c-d-e-f-g-h-i-j-- ----j-i-h-g-f-e-d-c-d-e-f-g-h-i-j---- ------j-i-h-g-f-e-d-e-f-g-h-i-j------ --------j-i-h-g-f-e-f-g-h-i-j-------- ----------j-i-h-g-f-g-h-i-j---------- ------------j-i-h-g-h-i-j------------ --------------j-i-h-i-j-------------- ----------------j-i-j---------------- ------------------j------------------ The center of the rangoli has the first alphabet letter a, and the boundary has the Nth alphabet letter (in alphabetical order). Input Format Only one line of input containing N, the size of the rangoli. Constraints 0< N< 27 Output Format Print the alphabet rangoli in the format explained above. Sample Input 5 Sample Output --------e-------- ------e-d-e------ ----e-d-c-d-e---- --e-d-c-b-c-d-e-- e-d-c-b-a-b-c-d-e --e-d-c-b-c-d-e-- ----e-d-c-d-e---- ------e-d-e------ --------e-------- The code: import string def print_rangoli(size): # your code goes here N = size width = 4*(N-1)+1 letters=string.ascii_letters[0:26] # triangle up for i in range(N-1): s = '{}'.format(letters[N-i-1]) for n in range(i): s = '{}-'.format(letters[N-i+n]) + s + '-{}'.format(letters[N-i+n]) s = s.center(width, '-') print(s); del(s) # center s = '{}'.format(letters[0]) for n in range(N-1): s = '{}-'.format(letters[n+1]) + s + '-{}'.format(letters[n+1]) print(s); del(s) # triangle down for i in range(N-2,-1,-1): s = '{}'.format(letters[N-i-1]) for n in range(i): s = '{}-'.format(letters[N-i+n]) + s + '-{}'.format(letters[N-i+n]) s = s.center(width, '-') print(s); del(s) if __name__ == '__main__': n = int(input()) print_rangoli(n)","title":"Alphabet Rangoli"},{"location":"notes/algorithms/python/averages/","text":"Averages Find the average of a student given a list of students You have a record of N students. Each record contains the student's name, and their percent marks in Maths, Physics and Chemistry. The marks can be floating values. The user enters some integer N followed by the names and marks for N students. You are required to save the record in a dictionary data type. The user then enters a student's name. Output the average percentage marks obtained by that student, correct to two decimal places. Input Format: The first line contains the integer N, the number of students. The next N lines contains the name and marks obtained by that student separated by a space. The final line contains the name of a particular student previously listed. Constraints: 2<=N<=10 0<=Marks<=100 Output Format: Print one line: The average of the marks obtained by the particular student correct to 2 decimal places. Sample Input 3 Krishna 67 68 69 Arjun 70 98 63 Malika 52 56 60 Malika Sample Output 56.00 Implementation: if __name__ == '__main__': # By accepting input as an int we can loop to grab all other inputs n = int(input()) # Initialize marks dict student_marks = {} for _ in range(n): # Nice trick to grab first value as name and the rest in a list called line name, *line = input().split() scores = list(map(float, line)) # Fill the student marks dict student_marks[name] = scores # Whose score do we want? query_name = input() selected_avg = round(sum(student_marks[query_name])/3,2) print(\"{0:.2f}\".format(selected_avg))","title":"Averages"},{"location":"notes/algorithms/python/averages/#averages","text":"Find the average of a student given a list of students You have a record of N students. Each record contains the student's name, and their percent marks in Maths, Physics and Chemistry. The marks can be floating values. The user enters some integer N followed by the names and marks for N students. You are required to save the record in a dictionary data type. The user then enters a student's name. Output the average percentage marks obtained by that student, correct to two decimal places. Input Format: The first line contains the integer N, the number of students. The next N lines contains the name and marks obtained by that student separated by a space. The final line contains the name of a particular student previously listed. Constraints: 2<=N<=10 0<=Marks<=100 Output Format: Print one line: The average of the marks obtained by the particular student correct to 2 decimal places. Sample Input 3 Krishna 67 68 69 Arjun 70 98 63 Malika 52 56 60 Malika Sample Output 56.00 Implementation: if __name__ == '__main__': # By accepting input as an int we can loop to grab all other inputs n = int(input()) # Initialize marks dict student_marks = {} for _ in range(n): # Nice trick to grab first value as name and the rest in a list called line name, *line = input().split() scores = list(map(float, line)) # Fill the student marks dict student_marks[name] = scores # Whose score do we want? query_name = input() selected_avg = round(sum(student_marks[query_name])/3,2) print(\"{0:.2f}\".format(selected_avg))","title":"Averages"},{"location":"notes/algorithms/python/breadth_first_search/","text":"Breadth First Search Solve a maze! In the breadth first search we start by imagining a grid. grid = [\"..........\", \"...#...##.\", \"..##...#..\", \".....###..\", \"......*...\"] Also define a key. wall, clear, goal = \"#\", \".\", \"*\" The constraints are that you need to provide a function that takes a grid and a starting point and finds the shortest path to the goal. Provide the starting point as a tuple (0,0) . Here is a basic example: import collections, pdb def bfs(grid, start): queue = collections.deque([[start]]) seen = set([start]) while queue: # pdb.set_trace() path = queue.popleft() x, y = path[-1] if grid[y][x] == goal: return path for x2, y2 in ((x+1,y), (x-1,y), (x,y+1), (x,y-1)): if 0 <= x2 < width and 0 <= y2 < height and grid[y2][x2] != wall and (x2, y2) not in seen: queue.append(path + [(x2, y2)]) seen.add((x2, y2)) # pdb.set_trace(); # For when you get stuck I have another version without the collections package to see a base python implementation. lot = [[1,1,1,1,1,1,1], [1,1,0,1,1,1,1], [0,1,1,0,0,0,1], [0,0,1,1,1,1,9],] def bfs(grid, start): # queue=[start] width = len(lot[0]) height = len(lot) queue = [[start]] seen = set([start]) loop_limit = 3000 count=0 while queue and count < loop_limit: path = queue.pop(0) x, y = path[-1] if lot[y][x] == 9: return (path,len(path)) for x2, y2 in ((x+1,y),(x-1,y),(x,y+1),(x,y-1)): if 0 <= x2 < width and 0 <= y2 < height and lot[y][x] != 0 and (x2,y2) not in seen: queue.append(path + [(x2,y2)]) seen.add((x2,y2)) count=count+1 #print(count) #print(path) return [\"we didn't get to finish\",path] shortest_path = bfs(lot, (0,0)) print(shortest_path) This prints out the following: ([(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (6, 1), (6, 2), (6, 3)], 10) A list that contains the path traversed and the the number of points in the traversed path. Finally there is a graph based approach: # Python3 Program to print BFS traversal # from a given source vertex. BFS(int s) # traverses vertices reachable from s. from collections import defaultdict import pdb # This class represents a directed graph # using adjacency list representation class Graph: # Constructor def __init__(self): # default dictionary to store graph self.graph = defaultdict(list) # function to add an edge to graph def addEdge(self,u,v): self.graph[u].append(v) # Function to print a BFS of graph def BFS(self, s): # Mark all the vertices as not visited visited = [False] * (len(self.graph)) # Create a queue for BFS queue = [] # Mark the source node as # visited and enqueue it queue.append(s) visited[s] = True while queue: pdb.set_trace() # Dequeue a vertex from # queue and print it s = queue.pop(0) print (s, end = \" \") # Get all adjacent vertices of the # dequeued vertex s. If a adjacent # has not been visited, then mark it # visited and enqueue it for i in self.graph[s]: if visited[i] == False: queue.append(i) visited[i] = True # Driver code # Create a graph given in # the above diagram g = Graph() g.addEdge(0, 1) g.addEdge(0, 2) g.addEdge(1, 2) g.addEdge(2, 0) g.addEdge(2, 3) g.addEdge(3, 3) print (\"Following is Breadth First Traversal\" \" (starting from vertex 2)\") g.BFS(2) # This code is contributed by Neelam Yadav","title":"Breadth First Search"},{"location":"notes/algorithms/python/breadth_first_search/#breadth-first-search","text":"Solve a maze! In the breadth first search we start by imagining a grid. grid = [\"..........\", \"...#...##.\", \"..##...#..\", \".....###..\", \"......*...\"] Also define a key. wall, clear, goal = \"#\", \".\", \"*\" The constraints are that you need to provide a function that takes a grid and a starting point and finds the shortest path to the goal. Provide the starting point as a tuple (0,0) . Here is a basic example: import collections, pdb def bfs(grid, start): queue = collections.deque([[start]]) seen = set([start]) while queue: # pdb.set_trace() path = queue.popleft() x, y = path[-1] if grid[y][x] == goal: return path for x2, y2 in ((x+1,y), (x-1,y), (x,y+1), (x,y-1)): if 0 <= x2 < width and 0 <= y2 < height and grid[y2][x2] != wall and (x2, y2) not in seen: queue.append(path + [(x2, y2)]) seen.add((x2, y2)) # pdb.set_trace(); # For when you get stuck I have another version without the collections package to see a base python implementation. lot = [[1,1,1,1,1,1,1], [1,1,0,1,1,1,1], [0,1,1,0,0,0,1], [0,0,1,1,1,1,9],] def bfs(grid, start): # queue=[start] width = len(lot[0]) height = len(lot) queue = [[start]] seen = set([start]) loop_limit = 3000 count=0 while queue and count < loop_limit: path = queue.pop(0) x, y = path[-1] if lot[y][x] == 9: return (path,len(path)) for x2, y2 in ((x+1,y),(x-1,y),(x,y+1),(x,y-1)): if 0 <= x2 < width and 0 <= y2 < height and lot[y][x] != 0 and (x2,y2) not in seen: queue.append(path + [(x2,y2)]) seen.add((x2,y2)) count=count+1 #print(count) #print(path) return [\"we didn't get to finish\",path] shortest_path = bfs(lot, (0,0)) print(shortest_path) This prints out the following: ([(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (6, 1), (6, 2), (6, 3)], 10) A list that contains the path traversed and the the number of points in the traversed path. Finally there is a graph based approach: # Python3 Program to print BFS traversal # from a given source vertex. BFS(int s) # traverses vertices reachable from s. from collections import defaultdict import pdb # This class represents a directed graph # using adjacency list representation class Graph: # Constructor def __init__(self): # default dictionary to store graph self.graph = defaultdict(list) # function to add an edge to graph def addEdge(self,u,v): self.graph[u].append(v) # Function to print a BFS of graph def BFS(self, s): # Mark all the vertices as not visited visited = [False] * (len(self.graph)) # Create a queue for BFS queue = [] # Mark the source node as # visited and enqueue it queue.append(s) visited[s] = True while queue: pdb.set_trace() # Dequeue a vertex from # queue and print it s = queue.pop(0) print (s, end = \" \") # Get all adjacent vertices of the # dequeued vertex s. If a adjacent # has not been visited, then mark it # visited and enqueue it for i in self.graph[s]: if visited[i] == False: queue.append(i) visited[i] = True # Driver code # Create a graph given in # the above diagram g = Graph() g.addEdge(0, 1) g.addEdge(0, 2) g.addEdge(1, 2) g.addEdge(2, 0) g.addEdge(2, 3) g.addEdge(3, 3) print (\"Following is Breadth First Traversal\" \" (starting from vertex 2)\") g.BFS(2) # This code is contributed by Neelam Yadav","title":"Breadth First Search"},{"location":"notes/algorithms/python/class_quiz/","text":"Class Quiz! Consider the following code, what will it output? class A(object): def go(self): print(\"go A go!\") def stop(self): print(\"stop A stop!\") def pause(self): raise Exception(\"Not Implemented\") class B(A): def go(self): super(B, self).go() print(\"go B go!\") class C(A): def go(self): super(C, self).go() print(\"go C go!\") def stop(self): super(C, self).stop() print(\"stop C stop!\") class D(B,C): def go(self): super(D, self).go() print(\"go D go!\") def stop(self): super(D, self).stop() print(\"stop D stop!\") def pause(self): print(\"wait D wait!\") class E(B,C): pass a = A() b = B() c = C() d = D() e = E() # specify output from here onwards a.go() # go A go! b.go() # go A go! # go B go! c.go() # go A go! # go C go! d.go() # go A go! # go C go! # go B go! # go D go! e.go() # go A go! # go C go! # go B go! a.stop() # stop A stop! b.stop() # stop A stop! c.stop() # stop A stop! # stop C stop! d.stop() # stop A stop! # stop C stop! # stop D stop! e.stop() # stop A stop! a.pause() # ... Exception: Not Implemented b.pause() # ... Exception: Not Implemented c.pause() # ... Exception: Not Implemented d.pause() # wait D wait! e.pause() # ...Exception: Not Implemented","title":"Class Quiz"},{"location":"notes/algorithms/python/class_quiz/#class-quiz","text":"Consider the following code, what will it output? class A(object): def go(self): print(\"go A go!\") def stop(self): print(\"stop A stop!\") def pause(self): raise Exception(\"Not Implemented\") class B(A): def go(self): super(B, self).go() print(\"go B go!\") class C(A): def go(self): super(C, self).go() print(\"go C go!\") def stop(self): super(C, self).stop() print(\"stop C stop!\") class D(B,C): def go(self): super(D, self).go() print(\"go D go!\") def stop(self): super(D, self).stop() print(\"stop D stop!\") def pause(self): print(\"wait D wait!\") class E(B,C): pass a = A() b = B() c = C() d = D() e = E() # specify output from here onwards a.go() # go A go! b.go() # go A go! # go B go! c.go() # go A go! # go C go! d.go() # go A go! # go C go! # go B go! # go D go! e.go() # go A go! # go C go! # go B go! a.stop() # stop A stop! b.stop() # stop A stop! c.stop() # stop A stop! # stop C stop! d.stop() # stop A stop! # stop C stop! # stop D stop! e.stop() # stop A stop! a.pause() # ... Exception: Not Implemented b.pause() # ... Exception: Not Implemented c.pause() # ... Exception: Not Implemented d.pause() # wait D wait! e.pause() # ...Exception: Not Implemented","title":"Class Quiz!"},{"location":"notes/algorithms/python/collections_counter/","text":"Collections Counter Your task is to compute how much money Raghu earned. Raghu is a shoe shop owner. His shop has X number of shoes.He has a list containing the size of each shoe he has in his shop. There are N number of customers who are willing to pay x_i amount of money only if they get the shoe of their desired size. Your task is to compute how much money Raghu earned. Input Format The first line contains X, the number of shoes. The second line contains the space separated list of all the shoe sizes in the shop. The third line contains N, the number of customers. The next N lines contain the space separated values of the shoe_size desired by the customer and x_i, the price of the shoe. Enter your code here. Read input from STDIN. Print output to STDOUT Sample Input: 10 --> Number of Shoes 2 3 4 5 6 8 7 6 5 18 --> all shoe sizes 6 -- > Number of Customers 6 55 6 45 6 55 4 40 18 60 10 50 S ample Output: 200 Explanation Customer 1: Purchased size 6 shoe for $55. Customer 2: Purchased size 6 shoe for $45. Customer 3: Size 6 no longer available, so no purchase. Customer 4: Purchased size 4 shoe for $40. Customer 5: Purchased size 18 shoe for $60. Customer 6: Size 10 not available, so no purchase. Total money earned = $200 from collections import Counter if __name__ == \"__main__\": num_of_shoes = int(input()) all_shoe_sizes = Counter(map(int, input().split())) num_customers = int(input()) total_earned = 0 for n in range(num_customers): size, price = map(int, input().split()) if all_shoe_sizes[size] != 0: total_earned = total_earned + price all_shoe_sizes[size] -= 1 print(total_earned)","title":"Collections Counter"},{"location":"notes/algorithms/python/collections_counter/#collections-counter","text":"Your task is to compute how much money Raghu earned. Raghu is a shoe shop owner. His shop has X number of shoes.He has a list containing the size of each shoe he has in his shop. There are N number of customers who are willing to pay x_i amount of money only if they get the shoe of their desired size. Your task is to compute how much money Raghu earned. Input Format The first line contains X, the number of shoes. The second line contains the space separated list of all the shoe sizes in the shop. The third line contains N, the number of customers. The next N lines contain the space separated values of the shoe_size desired by the customer and x_i, the price of the shoe. Enter your code here. Read input from STDIN. Print output to STDOUT Sample Input: 10 --> Number of Shoes 2 3 4 5 6 8 7 6 5 18 --> all shoe sizes 6 -- > Number of Customers 6 55 6 45 6 55 4 40 18 60 10 50 S ample Output: 200 Explanation Customer 1: Purchased size 6 shoe for $55. Customer 2: Purchased size 6 shoe for $45. Customer 3: Size 6 no longer available, so no purchase. Customer 4: Purchased size 4 shoe for $40. Customer 5: Purchased size 18 shoe for $60. Customer 6: Size 10 not available, so no purchase. Total money earned = $200 from collections import Counter if __name__ == \"__main__\": num_of_shoes = int(input()) all_shoe_sizes = Counter(map(int, input().split())) num_customers = int(input()) total_earned = 0 for n in range(num_customers): size, price = map(int, input().split()) if all_shoe_sizes[size] != 0: total_earned = total_earned + price all_shoe_sizes[size] -= 1 print(total_earned)","title":"Collections Counter"},{"location":"notes/algorithms/python/crossing_rects/","text":"Crossing Rects Find the total overlapping area without double counting. Problem: Imagine a grid with rectangles on it. Find the total overlapping area that does not repeat. In other words if a grid square is overlapped by multiple rectangles, it will only contribute 1 unit of area to the total overlapping area. Example 1: __________________________________> (x) |_| | | A (0,0), (1,1) |___| | B (0,0), (2,2) |_____| C (0,0), (3,3) | | \\/ (y) The answer is: 4 Example 2: Key: *(x,y), +(w, l) * - Top Left Corner + - Width and Length A (0,0), (9,3) _________________________________> (x) | __|____ B (6,1), (8,4) | | | _| |_____|__|__|_| C (13,2), (1,3) | | | | | |_____|_| \\/ (y) The answer is: 9 Here's the anser: def overlapping_area(rects=[ [(0,0),(1,1)],[(0,0),(2,2)] ]): \"\"\" This function takes a list of lists where each inner list is a rectangle. Each list will contain two tuples representing the top left corner of each rectangle and it's x-width and y-length. # Example: rect = [(x,y),(w,l)] \"\"\" def get_coordinates(rect=[(0,0), (0,0)]): \"\"\" Takes a list of tuples defining the top left corner coordinates of each square and the x-width, y-length and return all coordinates of rectangle. Below is the easy to see to setup the for loop, that way seeingwhat the comprehension is doing is much easier. This generates all x,y coordinates of a rect. coor = [] for i in range(rect[1][0]): for j in range(rect[1][1]): coor.append((rect[0][0]+i,rect[0][1]+j)) \"\"\" # List comprehension (list comprehensions are ordered backwards): coor = [(rect[0][0]+i,rect[0][1]+j) for j in range(rect[1][1]) for i in range(rect[1][0])] return coor # Get all rects coordinates rects_coors = [get_coordinates(r) for r in rects] area = 0 # Number of rectangles n = len(rects) seen = set() while n > 1: # Find overlap coors_in_common = set(rects_coors[n-1]) & set(rects_coors[n-2]) # Subtract out any previous overlap squares new_coors = coors_in_common - seen # Add fresh squares to area area += len(new_coors) # Mark seen area seen = set(rects_coors[n-1]) & set(rects_coors[n-2]) n -= 1 print(\"area of overlap is: {}\".format(area)) # Default overlapping_area() >>> area of overlap is: 1 # Trial 1 rects = [[(0,0), (1,1)], [(0,0), (2,2)], [(0,0), (3,3)]] overlapping_area(rects) >>> area of overlap is: 1 # Trial 2 rects = [[(0,0), (9,3)], [(6,1), (8,4)], [(13,2),(1,3)]] overlapping_area(rects) >>> area of overlap is: 9","title":"Crossing Rects"},{"location":"notes/algorithms/python/crossing_rects/#crossing-rects","text":"Find the total overlapping area without double counting. Problem: Imagine a grid with rectangles on it. Find the total overlapping area that does not repeat. In other words if a grid square is overlapped by multiple rectangles, it will only contribute 1 unit of area to the total overlapping area. Example 1: __________________________________> (x) |_| | | A (0,0), (1,1) |___| | B (0,0), (2,2) |_____| C (0,0), (3,3) | | \\/ (y) The answer is: 4 Example 2: Key: *(x,y), +(w, l) * - Top Left Corner + - Width and Length A (0,0), (9,3) _________________________________> (x) | __|____ B (6,1), (8,4) | | | _| |_____|__|__|_| C (13,2), (1,3) | | | | | |_____|_| \\/ (y) The answer is: 9 Here's the anser: def overlapping_area(rects=[ [(0,0),(1,1)],[(0,0),(2,2)] ]): \"\"\" This function takes a list of lists where each inner list is a rectangle. Each list will contain two tuples representing the top left corner of each rectangle and it's x-width and y-length. # Example: rect = [(x,y),(w,l)] \"\"\" def get_coordinates(rect=[(0,0), (0,0)]): \"\"\" Takes a list of tuples defining the top left corner coordinates of each square and the x-width, y-length and return all coordinates of rectangle. Below is the easy to see to setup the for loop, that way seeingwhat the comprehension is doing is much easier. This generates all x,y coordinates of a rect. coor = [] for i in range(rect[1][0]): for j in range(rect[1][1]): coor.append((rect[0][0]+i,rect[0][1]+j)) \"\"\" # List comprehension (list comprehensions are ordered backwards): coor = [(rect[0][0]+i,rect[0][1]+j) for j in range(rect[1][1]) for i in range(rect[1][0])] return coor # Get all rects coordinates rects_coors = [get_coordinates(r) for r in rects] area = 0 # Number of rectangles n = len(rects) seen = set() while n > 1: # Find overlap coors_in_common = set(rects_coors[n-1]) & set(rects_coors[n-2]) # Subtract out any previous overlap squares new_coors = coors_in_common - seen # Add fresh squares to area area += len(new_coors) # Mark seen area seen = set(rects_coors[n-1]) & set(rects_coors[n-2]) n -= 1 print(\"area of overlap is: {}\".format(area)) # Default overlapping_area() >>> area of overlap is: 1 # Trial 1 rects = [[(0,0), (1,1)], [(0,0), (2,2)], [(0,0), (3,3)]] overlapping_area(rects) >>> area of overlap is: 1 # Trial 2 rects = [[(0,0), (9,3)], [(6,1), (8,4)], [(13,2),(1,3)]] overlapping_area(rects) >>> area of overlap is: 9","title":"Crossing Rects"},{"location":"notes/algorithms/python/csv_loader/","text":"CSV Loader Packages needed: os pandas sqlite3 numpy This sets of scripts works as 2 independent situations: [1] Bulk folder load The function in this script looks inside a folder and using all *.csv begins a process of concatenating all csvs together and removing duplpicates on each merge. import pandas as pd, os, pdb, sqlite3, numpy as np pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show death_folder = 'data_Death' intubation_folder = 'data_Intubation' demographics_folder = 'data_demographics' covid_test_results_folder = 'data_COVID_TestResults' chest_imaging_reports_folder = 'data_chestImagingReports' db_dir = 'db' # Death example for function (Could require lots of Ram and DB grows in size...poorly coded to scale, but good for initializing a DB.) def create_table_from_folder_csvs(folder=None, demographic_table=False): # Could freeze 8 GB RAM machines as this lazily reads all csvs into RAM # HOWEVER IF YOU HAVE A MONSTER MACHINE USE IT...IT'S ABOUT 50+ TIMES FASTER TO USE THIS FUNCTION IF YOU CAN # Recommended: Use only a coulple files to define scheme and then use load csvs to load 1 at a time. conn = sqlite3.connect(os.path.join(db_dir,'covid.db')) if not isinstance(folder, str): return(\"Not a valid folder\") files = [i for i in os.listdir(folder) if i.find('.csv') != -1] print(files) # base frame for all but demographics data data = pd.DataFrame() # base frame for demographics data (2 table types) demographics = pd.DataFrame() socialhistory = pd.DataFrame() for file in files: csv = pd.read_csv(os.path.join(folder, file)) # demographic tables have 2 schemes if demographic_table == True: if file.find('Demographics') != -1: demographics = pd.concat([demographics, csv], axis=0) print(demographics.shape) demographics = demographics[~demographics.duplicated(keep='first')] elif file.find('SocialHistory') != -1: socialhistory = pd.concat([socialhistory, csv], axis=0) print(socialhistory.shape) socialhistory = socialhistory[~socialhistory.duplicated(keep='first')] else: data = pd.concat([data, csv], axis=0) print(data.shape) data = data[~data.duplicated(keep='first')] frames = {'data':data, 'demographics':demographics, 'socialhistory':socialhistory} for key in frames.keys(): frame = frames[key] if frame.shape[0] != 0: # pdb.set_trace() if key == \"demographics\": frame.to_sql(f\"{folder}_demographics\", conn, if_exists=\"replace\", index=False) elif key == \"socialhistory\": frame.to_sql(f\"{folder}_socialhistory\", conn, if_exists=\"replace\", index=False) else: frame.to_sql(f\"{folder}\", conn, if_exists=\"replace\", index=False) create_table_from_folder_csvs(death_folder) create_table_from_folder_csvs(intubation_folder) create_table_from_folder_csvs(demographics_folder, demographic_table=True) create_table_from_folder_csvs(covid_test_results_folder) create_table_from_folder_csvs(chest_imaging_reports_folder) [2] Load a new csv This script could load every csv but it much much slower. Use this once a bulk database is made andd you want to add a new csv. import pandas as pd, os, pdb, sqlite3, numpy as np pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show death_folder = 'data_Death' intubation_folder = 'data_Intubation' demographics_folder = 'data_demographics' covid_test_results_folder = 'data_COVID_TestResults' chest_imaging_reports_folder = 'data_chestImagingReports' db_dir = 'db' def load_csv(folder=None, file=None, demographic_table=False): # Proper input check if not isinstance(folder, str): return(\"Not a valid folder\\\\table name\") if not isinstance(file, str): return(\"Not a valid file\") # DB Connection conn = sqlite3.connect(os.path.join(db_dir,'covid.db')) c = conn.cursor() # Utility def check_if_table_exists_in_db(table): c.execute(f''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='{table}' ''') if c.fetchone()[0]==1 : print('Table exists.') return(True) else: print('Table does not exist.') return(False) ### table name table = folder if demographic_table: if file.find('Demographics') != -1: table = folder+'_demographic' elif file.find('SocialHistory') != -1: table = folder+'_socialhistory' # Load Data csv = pd.read_csv(os.path.join(folder, file)) # Create Table if it doesn't exist if not check_if_table_exists_in_db(table): print(\"creating table\") schema = '' for col in csv.columns: type_key = {'object':'TEXT', 'int64':'INTEGER', 'float64':'FLOAT'} schema += f\"'{col}' {type_key[str(csv.dtypes[col])]}, \" schema = schema[0:-2] c.execute(f\"\"\" create table '{table}' ({schema})\"\"\") def check_row_against_table(df_row, table=table ,conn=conn): # Here I had to pass in conn as it wasn't able to be seen otherwise. # This is because this is apart of the \"apply\" function in pandas (probably some sort of scoping issue) # See \"csv.apply(check_row_against_table, axis=1)\" below this function definition print(df_row) # Optional and makes things slower, but it's a good visual where_filter = '' insert_statements = '' for col in df_row.keys(): if pd.isnull(df_row[col]) == True: # Build element for queries escaping single quotes in text string for sqlite where_filter += f\"{col} is null and \" insert_statements += f\"null, \" else: value = str(df_row[col]).replace('\\'','\\'\\'') # escape quotes for use in sqlite where_filter += f\"{col} = '{value}' and \" insert_statements += f\"'{value}', \" # pdb.set_trace() # Trim commas where_filter = where_filter[0:-5] # trimming \" and \" insert_statements = insert_statements[0:-2] # trimming \", \" # Execute search for current record query = f'''select * from {table} where {where_filter};''' c.execute(query) # c.execute(\"select * from data_Death where MRN = 'not a chance'\") # Use this for debugging try: # Record is a duplicate if it doesn't throw exception original_record = c.fetchone()[0] except: # Record is brand new # pdb.set_trace() c.execute(f\"insert into {table} values ({insert_statements});\") # commit the changes to db conn.commit() # Loop over every row in csv and call above function csv.apply(check_row_against_table, axis=1) #close the connection conn.close() # #xample executions: # data_Death load_csv(folder=death_folder, file='Mortality_20200414110000.csv') load_csv(folder=death_folder, file='Mortality_20200423110000.csv') load_csv(folder=death_folder, file='Mortality_20200417110000.csv') load_csv(folder=death_folder, file='Mortality_20200422110000.csv') load_csv(folder=death_folder, file='Mortality_20200420090000.csv') load_csv(folder=death_folder, file='Mortality_20200421110000.csv') load_csv(folder=death_folder, file='Mortality_20200424080000.csv') load_csv(folder=death_folder, file='Mortality_20200418090000.csv') load_csv(folder=death_folder, file='Mortality_20200419090000.csv') load_csv(folder=death_folder, file='Mortality_20200416090000.csv') load_csv(folder=death_folder, file='Mortality_20200415090000.csv') # data_Intubation load_csv(folder=intubation_folder, file='Intubation_20200421090000.csv') load_csv(folder=intubation_folder, file='Intubation_20200418180000.csv') load_csv(folder=intubation_folder, file='Intubation_20200420180000.csv') load_csv(folder=intubation_folder, file='Intubation_20200416100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200423110000.csv') load_csv(folder=intubation_folder, file='Intubation_20200422090000.csv') load_csv(folder=intubation_folder, file='Intubation_20200415100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200424080000.csv') load_csv(folder=intubation_folder, file='Intubation_20200414160000.csv') load_csv(folder=intubation_folder, file='Intubation_20200417100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200419180000.csv') # data_COVID_TestResults load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200421090000.csv') load_csv(folder=covid_test_results_folder, file='CVOID_testResults_20200423110000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200415180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200418180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200416180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200422090000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200417180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200420180000.csv') load_csv(folder=covid_test_results_folder, file='CVOID_testResults_20200424080000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200419180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200414160000.csv') # data_chestImagingReports load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200421090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200417100000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200414110000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200422090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200419180000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200424080000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200423110000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200416090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200418160000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200420180000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200415110000.csv') # data_demographics load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200415110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200417180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200416180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200416090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200417140000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200416180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200415110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200417180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200416090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200417140000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200422090000.csv', demographic_table=True)","title":"CSV Loader"},{"location":"notes/algorithms/python/csv_loader/#csv-loader","text":"Packages needed: os pandas sqlite3 numpy This sets of scripts works as 2 independent situations:","title":"CSV Loader"},{"location":"notes/algorithms/python/csv_loader/#1-bulk-folder-load","text":"The function in this script looks inside a folder and using all *.csv begins a process of concatenating all csvs together and removing duplpicates on each merge. import pandas as pd, os, pdb, sqlite3, numpy as np pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show death_folder = 'data_Death' intubation_folder = 'data_Intubation' demographics_folder = 'data_demographics' covid_test_results_folder = 'data_COVID_TestResults' chest_imaging_reports_folder = 'data_chestImagingReports' db_dir = 'db' # Death example for function (Could require lots of Ram and DB grows in size...poorly coded to scale, but good for initializing a DB.) def create_table_from_folder_csvs(folder=None, demographic_table=False): # Could freeze 8 GB RAM machines as this lazily reads all csvs into RAM # HOWEVER IF YOU HAVE A MONSTER MACHINE USE IT...IT'S ABOUT 50+ TIMES FASTER TO USE THIS FUNCTION IF YOU CAN # Recommended: Use only a coulple files to define scheme and then use load csvs to load 1 at a time. conn = sqlite3.connect(os.path.join(db_dir,'covid.db')) if not isinstance(folder, str): return(\"Not a valid folder\") files = [i for i in os.listdir(folder) if i.find('.csv') != -1] print(files) # base frame for all but demographics data data = pd.DataFrame() # base frame for demographics data (2 table types) demographics = pd.DataFrame() socialhistory = pd.DataFrame() for file in files: csv = pd.read_csv(os.path.join(folder, file)) # demographic tables have 2 schemes if demographic_table == True: if file.find('Demographics') != -1: demographics = pd.concat([demographics, csv], axis=0) print(demographics.shape) demographics = demographics[~demographics.duplicated(keep='first')] elif file.find('SocialHistory') != -1: socialhistory = pd.concat([socialhistory, csv], axis=0) print(socialhistory.shape) socialhistory = socialhistory[~socialhistory.duplicated(keep='first')] else: data = pd.concat([data, csv], axis=0) print(data.shape) data = data[~data.duplicated(keep='first')] frames = {'data':data, 'demographics':demographics, 'socialhistory':socialhistory} for key in frames.keys(): frame = frames[key] if frame.shape[0] != 0: # pdb.set_trace() if key == \"demographics\": frame.to_sql(f\"{folder}_demographics\", conn, if_exists=\"replace\", index=False) elif key == \"socialhistory\": frame.to_sql(f\"{folder}_socialhistory\", conn, if_exists=\"replace\", index=False) else: frame.to_sql(f\"{folder}\", conn, if_exists=\"replace\", index=False) create_table_from_folder_csvs(death_folder) create_table_from_folder_csvs(intubation_folder) create_table_from_folder_csvs(demographics_folder, demographic_table=True) create_table_from_folder_csvs(covid_test_results_folder) create_table_from_folder_csvs(chest_imaging_reports_folder)","title":"[1] Bulk folder load"},{"location":"notes/algorithms/python/csv_loader/#2-load-a-new-csv","text":"This script could load every csv but it much much slower. Use this once a bulk database is made andd you want to add a new csv. import pandas as pd, os, pdb, sqlite3, numpy as np pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show death_folder = 'data_Death' intubation_folder = 'data_Intubation' demographics_folder = 'data_demographics' covid_test_results_folder = 'data_COVID_TestResults' chest_imaging_reports_folder = 'data_chestImagingReports' db_dir = 'db' def load_csv(folder=None, file=None, demographic_table=False): # Proper input check if not isinstance(folder, str): return(\"Not a valid folder\\\\table name\") if not isinstance(file, str): return(\"Not a valid file\") # DB Connection conn = sqlite3.connect(os.path.join(db_dir,'covid.db')) c = conn.cursor() # Utility def check_if_table_exists_in_db(table): c.execute(f''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='{table}' ''') if c.fetchone()[0]==1 : print('Table exists.') return(True) else: print('Table does not exist.') return(False) ### table name table = folder if demographic_table: if file.find('Demographics') != -1: table = folder+'_demographic' elif file.find('SocialHistory') != -1: table = folder+'_socialhistory' # Load Data csv = pd.read_csv(os.path.join(folder, file)) # Create Table if it doesn't exist if not check_if_table_exists_in_db(table): print(\"creating table\") schema = '' for col in csv.columns: type_key = {'object':'TEXT', 'int64':'INTEGER', 'float64':'FLOAT'} schema += f\"'{col}' {type_key[str(csv.dtypes[col])]}, \" schema = schema[0:-2] c.execute(f\"\"\" create table '{table}' ({schema})\"\"\") def check_row_against_table(df_row, table=table ,conn=conn): # Here I had to pass in conn as it wasn't able to be seen otherwise. # This is because this is apart of the \"apply\" function in pandas (probably some sort of scoping issue) # See \"csv.apply(check_row_against_table, axis=1)\" below this function definition print(df_row) # Optional and makes things slower, but it's a good visual where_filter = '' insert_statements = '' for col in df_row.keys(): if pd.isnull(df_row[col]) == True: # Build element for queries escaping single quotes in text string for sqlite where_filter += f\"{col} is null and \" insert_statements += f\"null, \" else: value = str(df_row[col]).replace('\\'','\\'\\'') # escape quotes for use in sqlite where_filter += f\"{col} = '{value}' and \" insert_statements += f\"'{value}', \" # pdb.set_trace() # Trim commas where_filter = where_filter[0:-5] # trimming \" and \" insert_statements = insert_statements[0:-2] # trimming \", \" # Execute search for current record query = f'''select * from {table} where {where_filter};''' c.execute(query) # c.execute(\"select * from data_Death where MRN = 'not a chance'\") # Use this for debugging try: # Record is a duplicate if it doesn't throw exception original_record = c.fetchone()[0] except: # Record is brand new # pdb.set_trace() c.execute(f\"insert into {table} values ({insert_statements});\") # commit the changes to db conn.commit() # Loop over every row in csv and call above function csv.apply(check_row_against_table, axis=1) #close the connection conn.close() # #xample executions: # data_Death load_csv(folder=death_folder, file='Mortality_20200414110000.csv') load_csv(folder=death_folder, file='Mortality_20200423110000.csv') load_csv(folder=death_folder, file='Mortality_20200417110000.csv') load_csv(folder=death_folder, file='Mortality_20200422110000.csv') load_csv(folder=death_folder, file='Mortality_20200420090000.csv') load_csv(folder=death_folder, file='Mortality_20200421110000.csv') load_csv(folder=death_folder, file='Mortality_20200424080000.csv') load_csv(folder=death_folder, file='Mortality_20200418090000.csv') load_csv(folder=death_folder, file='Mortality_20200419090000.csv') load_csv(folder=death_folder, file='Mortality_20200416090000.csv') load_csv(folder=death_folder, file='Mortality_20200415090000.csv') # data_Intubation load_csv(folder=intubation_folder, file='Intubation_20200421090000.csv') load_csv(folder=intubation_folder, file='Intubation_20200418180000.csv') load_csv(folder=intubation_folder, file='Intubation_20200420180000.csv') load_csv(folder=intubation_folder, file='Intubation_20200416100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200423110000.csv') load_csv(folder=intubation_folder, file='Intubation_20200422090000.csv') load_csv(folder=intubation_folder, file='Intubation_20200415100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200424080000.csv') load_csv(folder=intubation_folder, file='Intubation_20200414160000.csv') load_csv(folder=intubation_folder, file='Intubation_20200417100000.csv') load_csv(folder=intubation_folder, file='Intubation_20200419180000.csv') # data_COVID_TestResults load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200421090000.csv') load_csv(folder=covid_test_results_folder, file='CVOID_testResults_20200423110000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200415180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200418180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200416180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200422090000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200417180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200420180000.csv') load_csv(folder=covid_test_results_folder, file='CVOID_testResults_20200424080000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200419180000.csv') load_csv(folder=covid_test_results_folder, file='COVID_testResults_20200414160000.csv') # data_chestImagingReports load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200421090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200417100000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200414110000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200422090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200419180000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200424080000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200423110000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200416090000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200418160000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200420180000.csv') load_csv(folder=chest_imaging_reports_folder, file='XRChest_20200415110000.csv') # data_demographics load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200415110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200417180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200416180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200416090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200417140000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200418160000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200416180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200423110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200415110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200414110000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200422090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200417180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200416090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200420180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200415180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_Demographics_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200421090000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_SocialHistory_20200419180000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='Inpatient_ADT_Demographics_20200417140000.csv', demographic_table=True) load_csv(folder=demographics_folder, file='RIC_ED_visits_SocialHistory_20200422090000.csv', demographic_table=True)","title":"[2] Load a new csv"},{"location":"notes/algorithms/python/default_dict/","text":"Default Dict Print the indices of each occurrence of m in group A. In this challenge, you will be given integers, n and m. There are m words, which might repeat, in word group A. There are m words belonging to word group B. For each m words, check whether the word has appeared in group A or not. Print the indices of each occurrence of m in group A.If it does not appear, print -1. Sample Input: 5 2 a a b a b a b Sample Output: 1 2 4 3 5 Code: from collections import defaultdict if __name__ == '__main__': n, m = map(int,input().split()) A = []; A = [input() for i in range(n)] B = []; B = [input() for i in range(m)] B_dict = defaultdict(list) for wordB in B: for A_indice in range(len(A)): if wordB == A[A_indice]: B_dict[wordB].append(A_indice+1) if B_dict[wordB] == []: B_dict[wordB].append(-1) for word in B_dict: print(\" \".join([str(i) for i in B_dict[word]]))","title":"Default Dict"},{"location":"notes/algorithms/python/default_dict/#default-dict","text":"Print the indices of each occurrence of m in group A. In this challenge, you will be given integers, n and m. There are m words, which might repeat, in word group A. There are m words belonging to word group B. For each m words, check whether the word has appeared in group A or not. Print the indices of each occurrence of m in group A.If it does not appear, print -1. Sample Input: 5 2 a a b a b a b Sample Output: 1 2 4 3 5 Code: from collections import defaultdict if __name__ == '__main__': n, m = map(int,input().split()) A = []; A = [input() for i in range(n)] B = []; B = [input() for i in range(m)] B_dict = defaultdict(list) for wordB in B: for A_indice in range(len(A)): if wordB == A[A_indice]: B_dict[wordB].append(A_indice+1) if B_dict[wordB] == []: B_dict[wordB].append(-1) for word in B_dict: print(\" \".join([str(i) for i in B_dict[word]]))","title":"Default Dict"},{"location":"notes/algorithms/python/directory_compare/","text":"Directory and File Compare Source stackoverflow List out files appended to their paths: import os def listfiles(path): files = [] for dirName, subdirList, fileList in os.walk(path): dir = dirName.replace(path, '') for fname in fileList: files.append(os.path.join(dir, fname)) return files x = listfiles('D:\\\\xfiles') y = listfiles('D:\\\\yfiles') You can use list comprehensions to compare, but it's slow compared to sets : q = [filename for filename in x if filename not in y] Use sets in python to make compares: files_only_in_x = set(x) - set(y) files_only_in_y = set(y) - set(x) files_only_in_either = set(x) ^ set(y) files_in_both = set(x) & set(y) all_files = set(x) | set(y)","title":"Folder\\File Compare"},{"location":"notes/algorithms/python/directory_compare/#directory-and-file-compare","text":"Source stackoverflow","title":"Directory and File Compare"},{"location":"notes/algorithms/python/directory_compare/#list-out-files-appended-to-their-paths","text":"import os def listfiles(path): files = [] for dirName, subdirList, fileList in os.walk(path): dir = dirName.replace(path, '') for fname in fileList: files.append(os.path.join(dir, fname)) return files x = listfiles('D:\\\\xfiles') y = listfiles('D:\\\\yfiles')","title":"List out files appended to their paths:"},{"location":"notes/algorithms/python/directory_compare/#you-can-use-list-comprehensions-to-compare-but-its-slow-compared-to-sets","text":"q = [filename for filename in x if filename not in y]","title":"You can use list comprehensions to compare, but it's slow compared to sets:"},{"location":"notes/algorithms/python/directory_compare/#use-sets-in-python-to-make-compares","text":"files_only_in_x = set(x) - set(y) files_only_in_y = set(y) - set(x) files_only_in_either = set(x) ^ set(y) files_in_both = set(x) & set(y) all_files = set(x) | set(y)","title":"Use sets in python to make compares:"},{"location":"notes/algorithms/python/door_mat/","text":"Door Mat Make a doormat sized NxM, and with 'Welcome' in the middle. Mr. Vincent works in a door mat manufacturing company. One day, he designed a new door mat with the following specifications: Mat size must be NxM. (N is an odd natural number, and M is 3 times N.) The design should have 'WELCOME' written in the center. The design pattern should only use these characters: | . - Sample Designs Size: 7 x 21 ---------.|.--------- ------.|..|..|.------ ---.|..|..|..|..|.--- -------WELCOME------- ---.|..|..|..|..|.--- ------.|..|..|.------ ---------.|.--------- Size: 11 x 33 ---------------.|.--------------- ------------.|..|..|.------------ ---------.|..|..|..|..|.--------- ------.|..|..|..|..|..|..|.------ ---.|..|..|..|..|..|..|..|..|.--- -------------WELCOME------------- ---.|..|..|..|..|..|..|..|..|.--- ------.|..|..|..|..|..|..|.------ ---------.|..|..|..|..|.--------- ------------.|..|..|.------------ ---------------.|.--------------- Input Format A single line containing the space separated values of N and M. Constraints: 5<N<101 15<M<303 Example Sample Input: 9 27 Sample Output: ------------.|.------------ ---------.|..|..|.--------- ------.|..|..|..|..|.------ ---.|..|..|..|..|..|..|.--- ----------WELCOME---------- ---.|..|..|..|..|..|..|.--- ------.|..|..|..|..|.------ ---------.|..|..|.--------- ------------.|.------------ if __name__ == '__main__': N, M = map(int,input().split()) for i in range(int((N-1)/2)): print(((2*i+1)*\".|.\").center(M,'-')) print('WELCOME'.center(M,'-')) for i in range(int((N-1)/2)): print((((N-2)-2*i)*\".|.\").center(M,'-'))","title":"Door Mat"},{"location":"notes/algorithms/python/door_mat/#door-mat","text":"Make a doormat sized NxM, and with 'Welcome' in the middle. Mr. Vincent works in a door mat manufacturing company. One day, he designed a new door mat with the following specifications: Mat size must be NxM. (N is an odd natural number, and M is 3 times N.) The design should have 'WELCOME' written in the center. The design pattern should only use these characters: | . - Sample Designs Size: 7 x 21 ---------.|.--------- ------.|..|..|.------ ---.|..|..|..|..|.--- -------WELCOME------- ---.|..|..|..|..|.--- ------.|..|..|.------ ---------.|.--------- Size: 11 x 33 ---------------.|.--------------- ------------.|..|..|.------------ ---------.|..|..|..|..|.--------- ------.|..|..|..|..|..|..|.------ ---.|..|..|..|..|..|..|..|..|.--- -------------WELCOME------------- ---.|..|..|..|..|..|..|..|..|.--- ------.|..|..|..|..|..|..|.------ ---------.|..|..|..|..|.--------- ------------.|..|..|.------------ ---------------.|.--------------- Input Format A single line containing the space separated values of N and M. Constraints: 5<N<101 15<M<303 Example Sample Input: 9 27 Sample Output: ------------.|.------------ ---------.|..|..|.--------- ------.|..|..|..|..|.------ ---.|..|..|..|..|..|..|.--- ----------WELCOME---------- ---.|..|..|..|..|..|..|.--- ------.|..|..|..|..|.------ ---------.|..|..|.--------- ------------.|.------------ if __name__ == '__main__': N, M = map(int,input().split()) for i in range(int((N-1)/2)): print(((2*i+1)*\".|.\").center(M,'-')) print('WELCOME'.center(M,'-')) for i in range(int((N-1)/2)): print((((N-2)-2*i)*\".|.\").center(M,'-'))","title":"Door Mat"},{"location":"notes/algorithms/python/find_a_string/","text":"Find a string Find all occurrences of a substring in a string In this challenge, the user enters a string and a substring. You have to print the number of times that the substring occurs in the given string. String traversal will take place from left to right, not from right to left. NOTE: String letters are case-sensitive. Input Format: The first line of input contains the original string. The next line contains the substring. Constraints: 1 <= len(string) <= 200 Each character in the string is an ascii character. Output Format: Output the integer number indicating the total number of occurrences of the substring in the original string. Sample Input: ABCDCDC CDC Sample Output: 2 Concept def count_substring(string, sub_string): index = 0 counts = 0 while index < len(string): index = string.find(sub_string, index) if index == -1: break counts += 1 index += 1 return counts if __name__ == '__main__': string = input().strip() sub_string = input().strip() count = count_substring(string, sub_string) print(count)","title":"Find a String"},{"location":"notes/algorithms/python/find_a_string/#find-a-string","text":"Find all occurrences of a substring in a string In this challenge, the user enters a string and a substring. You have to print the number of times that the substring occurs in the given string. String traversal will take place from left to right, not from right to left. NOTE: String letters are case-sensitive. Input Format: The first line of input contains the original string. The next line contains the substring. Constraints: 1 <= len(string) <= 200 Each character in the string is an ascii character. Output Format: Output the integer number indicating the total number of occurrences of the substring in the original string. Sample Input: ABCDCDC CDC Sample Output: 2 Concept def count_substring(string, sub_string): index = 0 counts = 0 while index < len(string): index = string.find(sub_string, index) if index == -1: break counts += 1 index += 1 return counts if __name__ == '__main__': string = input().strip() sub_string = input().strip() count = count_substring(string, sub_string) print(count)","title":"Find a string"},{"location":"notes/algorithms/python/names_and_dates/","text":"Names and Dates Who has the same birthday? This algorithm is a simple, find all people with the same birthday. # Initial data that we are searching through data = {'Kate Thompson':'11/23/2000','Ben Patterson':'05/12/1989','Joseph Bernanke':'11/23/1970','Lindsey Harrison':'05/12/2003','Jenny Bennington':'07/09/1998','Jeremy English':'02/27/1967'} def names_and_dates(data): unique_dates = set((i[0:5] for i in data.values())) dates_dict = {date:[] for date in unique_dates} for date in dates_dict: for name in data.keys(): if data[name][0:5] == date: dates_dict[date].append(name) same_birthdays = [dates_dict[date] for date in dates_dict.keys() if len(dates_dict[date]) > 1] return same_birthdays same_birthdays = names_and_dates(data) [print(names, \"Have the same birthday\") for names in same_birthdays]","title":"Names and Dates"},{"location":"notes/algorithms/python/names_and_dates/#names-and-dates","text":"Who has the same birthday? This algorithm is a simple, find all people with the same birthday. # Initial data that we are searching through data = {'Kate Thompson':'11/23/2000','Ben Patterson':'05/12/1989','Joseph Bernanke':'11/23/1970','Lindsey Harrison':'05/12/2003','Jenny Bennington':'07/09/1998','Jeremy English':'02/27/1967'} def names_and_dates(data): unique_dates = set((i[0:5] for i in data.values())) dates_dict = {date:[] for date in unique_dates} for date in dates_dict: for name in data.keys(): if data[name][0:5] == date: dates_dict[date].append(name) same_birthdays = [dates_dict[date] for date in dates_dict.keys() if len(dates_dict[date]) > 1] return same_birthdays same_birthdays = names_and_dates(data) [print(names, \"Have the same birthday\") for names in same_birthdays]","title":"Names and Dates"},{"location":"notes/algorithms/python/nodes_w_classes/","text":"Nodes with Classes Build a parent/child relationship with classes Consider the following code, what will it output? class Node(object): def __init__(self,sName): self._lChildren = [] self.sName = sName def __repr__(self): return \"<Node '{}'>\".format(self.sName) def append(self,*args,**kwargs): self._lChildren.append(*args,**kwargs) def print_all_1(self): print(self) for oChild in self._lChildren: oChild.print_all_1() def print_all_2(self): def gen(o): lAll = [o,] while lAll: oNext = lAll.pop(0) lAll.extend(oNext._lChildren) yield oNext for oNode in gen(self): print(oNode) oRoot = Node(\"root\") oChild1 = Node(\"child1\") oChild2 = Node(\"child2\") oChild3 = Node(\"child3\") oChild4 = Node(\"child4\") oChild5 = Node(\"child5\") oChild6 = Node(\"child6\") oChild7 = Node(\"child7\") oChild8 = Node(\"child8\") oChild9 = Node(\"child9\") oChild10 = Node(\"child10\") oRoot.append(oChild1) oRoot.append(oChild2) oRoot.append(oChild3) oChild1.append(oChild5) oChild2.append(oChild6) oChild4.append(oChild7) oChild3.append(oChild8) oChild3.append(oChild9) oChild6.append(oChild10) # specify output from here onwards oRoot.print_all_1() oRoot.print_all_2() Let's discuss what happens. Below is the map of the parents and children: oRoot() _ _| _ | | | oRoot1() oRoot2() oRoot3() | | _|____ oRoot5() oRoot6() | | | oRoot8() oRoot9() oRoot10() oRoot4() | oRoot7() The append method is taking the calling object and appending the passed object to an instance variable '_lChildren'. Once the above tree is setup, it's time to investigate the two print methods. oRoot().print_all_1() def print_all_1(self): print(self) for oChild in self._lChildren: oChild.print_all_1() The first action is to print the starting root. print(self). Then we loop through the children and call the print_all_1() method for each child. It's important to note that oChild5() is called before oChild2(). This is because before oChild2() is called oChild1() called print_all1() on it's children. This has the affect of traveling down branches as opposed to across them. oRoot.print_all_1() oRoot().print_all_2() def print_all_2(self): def gen(o): lAll = [o,] while lAll: oNext = lAll.pop(0) lAll.extend(oNext._lChildren) yield oNext for oNode in gen(self): print(oNode) First loop in the for loop: Do you see the generator? print_all_2() actually creates a generator function. First it creates a function called gen(o) and then creates lAll, a list for storing objects. This is an interesting use of a generator; since lAll is remembered between successive yields, it can theoretically traverse an infinite tree. So the first value to gen is self. In that for loop the first yield is hit and pauses the function after yeilding oRoot(). The for loop is looping through an iterator so the iterator is telling the for loop to keep going. So finish the first for loop and print the oRoot(). The second: So the for loop picks up and rechecks the while condition, which is true as we added oRoot()'s direct children to lAll. Now we pop oChild1() off of lAll and add it's children to lAll. Here is what lAll looks like at this point: [oChild2(), oChild3(), oChild5()]. We yield and print oChild1() and check the while again. Its true so we grab the next element off lAll which is oChild2(). Append its children and yield it and continue horizontaly and left to right across the tree. Check out the final route taken: oRoot.print_all_2()","title":"Node w Classes"},{"location":"notes/algorithms/python/nodes_w_classes/#nodes-with-classes","text":"Build a parent/child relationship with classes Consider the following code, what will it output? class Node(object): def __init__(self,sName): self._lChildren = [] self.sName = sName def __repr__(self): return \"<Node '{}'>\".format(self.sName) def append(self,*args,**kwargs): self._lChildren.append(*args,**kwargs) def print_all_1(self): print(self) for oChild in self._lChildren: oChild.print_all_1() def print_all_2(self): def gen(o): lAll = [o,] while lAll: oNext = lAll.pop(0) lAll.extend(oNext._lChildren) yield oNext for oNode in gen(self): print(oNode) oRoot = Node(\"root\") oChild1 = Node(\"child1\") oChild2 = Node(\"child2\") oChild3 = Node(\"child3\") oChild4 = Node(\"child4\") oChild5 = Node(\"child5\") oChild6 = Node(\"child6\") oChild7 = Node(\"child7\") oChild8 = Node(\"child8\") oChild9 = Node(\"child9\") oChild10 = Node(\"child10\") oRoot.append(oChild1) oRoot.append(oChild2) oRoot.append(oChild3) oChild1.append(oChild5) oChild2.append(oChild6) oChild4.append(oChild7) oChild3.append(oChild8) oChild3.append(oChild9) oChild6.append(oChild10) # specify output from here onwards oRoot.print_all_1() oRoot.print_all_2() Let's discuss what happens. Below is the map of the parents and children: oRoot() _ _| _ | | | oRoot1() oRoot2() oRoot3() | | _|____ oRoot5() oRoot6() | | | oRoot8() oRoot9() oRoot10() oRoot4() | oRoot7() The append method is taking the calling object and appending the passed object to an instance variable '_lChildren'. Once the above tree is setup, it's time to investigate the two print methods. oRoot().print_all_1() def print_all_1(self): print(self) for oChild in self._lChildren: oChild.print_all_1() The first action is to print the starting root. print(self). Then we loop through the children and call the print_all_1() method for each child. It's important to note that oChild5() is called before oChild2(). This is because before oChild2() is called oChild1() called print_all1() on it's children. This has the affect of traveling down branches as opposed to across them. oRoot.print_all_1() oRoot().print_all_2() def print_all_2(self): def gen(o): lAll = [o,] while lAll: oNext = lAll.pop(0) lAll.extend(oNext._lChildren) yield oNext for oNode in gen(self): print(oNode) First loop in the for loop: Do you see the generator? print_all_2() actually creates a generator function. First it creates a function called gen(o) and then creates lAll, a list for storing objects. This is an interesting use of a generator; since lAll is remembered between successive yields, it can theoretically traverse an infinite tree. So the first value to gen is self. In that for loop the first yield is hit and pauses the function after yeilding oRoot(). The for loop is looping through an iterator so the iterator is telling the for loop to keep going. So finish the first for loop and print the oRoot(). The second: So the for loop picks up and rechecks the while condition, which is true as we added oRoot()'s direct children to lAll. Now we pop oChild1() off of lAll and add it's children to lAll. Here is what lAll looks like at this point: [oChild2(), oChild3(), oChild5()]. We yield and print oChild1() and check the while again. Its true so we grab the next element off lAll which is oChild2(). Append its children and yield it and continue horizontaly and left to right across the tree. Check out the final route taken: oRoot.print_all_2()","title":"Nodes with Classes"},{"location":"notes/algorithms/python/print_directory/","text":"Print Directory This function takes the name of a directory and prints out the paths files within that directory as well as any files contained in contained directories. This function is similar to os.walk. Please don't use os.walk in your answer. We are interested in your ability to work with nested structures. def print_directory_contents(sPath): import os for sChild in os.listdir(sPath): # Everything gets turned into a new path # Then we see which can go deeper... sChildPath = os.path.join(sPath,sChild) if os.path.isdir(sChildPath): # If we have a folder, recall this function print_directory_contents(sChildPath) else: # If we hit a base case then print as we have found a file print(sChildPath)","title":"Print Directory"},{"location":"notes/algorithms/python/print_directory/#print-directory","text":"This function takes the name of a directory and prints out the paths files within that directory as well as any files contained in contained directories. This function is similar to os.walk. Please don't use os.walk in your answer. We are interested in your ability to work with nested structures. def print_directory_contents(sPath): import os for sChild in os.listdir(sPath): # Everything gets turned into a new path # Then we see which can go deeper... sChildPath = os.path.join(sPath,sChild) if os.path.isdir(sChildPath): # If we have a folder, recall this function print_directory_contents(sChildPath) else: # If we hit a base case then print as we have found a file print(sChildPath)","title":"Print Directory"},{"location":"notes/algorithms/python/retry/","text":"retry() Retry a GET request a maximum of 5 times. Let's assume you want to get some response from a get command. For all intensive purposes we will assume this is all happening inside a function called get_response(choice) Next youy want to make a function try_again(response='pass') that will keep trying to execute get_response(choice) if we get a '401' response, which means that the client could not be authenticated. The catch is that we want the maximum retries to be 5 before accepting the failed authentication. Furthermore each time the retry function is called the function should sleep for n^2 seconds. It shouldn't wait anytime to execute the first get_response(choice) , which is the one initially passed to try_again() . Finally if we get '404' then return a '404 Not Found Error'. We start by defining get_response() def get_response(choice=3): choices = {1:'404',2:'pass',3:'401'} return choices[choice] Now we need try_again(). I have to give credit to Zagaran which you can find a link to on the Sources tab for this algorithm. During an interview I attempted a recursive solution, but they said a for loop would of done. So for extra practice I implemented both. def try_again_recursive(response, try_count=1): \"\"\" This function will take a response object and do the following: - If we get 'pass', return 'we connected!' - If we get '404', return '404 Not Found Error' - If we get '401', try again, however each time you try again wait the square of the try seconds before trying again. Try a maximum of 5 times before returning 'client could not be authenticated.' Example: try1: wait 1 seconds to retry try2: wait 4 seconds to retry try3: wait 9 seconds to retry try4: wait 16 seconds to retry try5: wait 0 seconds to retry! Since the max tries is 5, there is no need to wait the additional 25 seconds before exiting. \"\"\" if response == '404': return '404 Not Found Error' elif response == '401': if try_count == 5: print('for n = {0}, waiting {1} seconds.'.format(try_count, 0)) print('client could not be authenticated.') else: print('for n = {0}, waiting {1} seconds.'.format(try_count, try_count**2)) time.sleep(try_count**2) try_again_recursive(get_response(), try_count=try_count+1) else: return 'we connected!' return response Check out the output below # Tests # >>>try_again_recursive(get_response()) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' One thing to note, I cheated a little. For the sake of practicing the multiple calls, I made sure the get_response() function always returned a '401'. We will address this a little later. For now let's implement the for loop version. # For Loop Solution: def try_again_for_loop(response): \"\"\" This function will take a response object and do the following: - If we get 'pass', return 'we connected!' - If we get '404', return '404 Not Found Error' - If we get '404', try again, however each time you try again wait the square of the try seconds before trying again. Try a maximum of 5 times before returning 'client could not be authenticated.' Example: try1: wait 1 seconds to retry try2: wait 4 seconds to retry try3: wait 9 seconds to retry try4: wait 16 seconds to retry try5: wait 0 seconds to retry! Since the max tries is 5, there is no need to wait the additional 25 seconds before exiting. \"\"\" for i in range(1,6): # [1,2,3,4,5] if response == '404': return '404 Not Found Error' elif response == '401': if i == 5: print('for n = {0}, waiting {1} seconds.'.format(i, 0)) print('client could not be authenticated.') else: # pdb.set_trace() print('for n = {0}, waiting {1} seconds.'.format(i, i**2)) time.sleep(i**2) response = get_response() else: return 'we connected!' return response Output: # >>> try_again_for_loop(get_response()) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' Now we need to simulate reality a little better to give the other logic a chance to be executed. Let's make it so get_response() could return any of the other values ('pass','404'). Go into the recursive version of try_again() and find this line try_again_recursive(get_response(), try_count=try_count+1) , and change it to try_again_recursive(get_response(random.randint(1,3)), try_count=try_count+1) . You will need to import random at the top of your script. Then also add to your call of try_again_recursive() Results: # Simulation # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # 'we connected!' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # '404 Not Found Error' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' Again, the for loop version changes the same thing, ie. response = get_response( to this response = get_response(random.rendint(1,3)) . # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # '404 Not Found Error' # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # 'we connected!' # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # 'we connected!' # >>> try_again_for_loop(get_response(random.randint(1,3))) # '404 Not Found Error'","title":"Retry"},{"location":"notes/algorithms/python/retry/#retry","text":"Retry a GET request a maximum of 5 times. Let's assume you want to get some response from a get command. For all intensive purposes we will assume this is all happening inside a function called get_response(choice) Next youy want to make a function try_again(response='pass') that will keep trying to execute get_response(choice) if we get a '401' response, which means that the client could not be authenticated. The catch is that we want the maximum retries to be 5 before accepting the failed authentication. Furthermore each time the retry function is called the function should sleep for n^2 seconds. It shouldn't wait anytime to execute the first get_response(choice) , which is the one initially passed to try_again() . Finally if we get '404' then return a '404 Not Found Error'. We start by defining get_response() def get_response(choice=3): choices = {1:'404',2:'pass',3:'401'} return choices[choice] Now we need try_again(). I have to give credit to Zagaran which you can find a link to on the Sources tab for this algorithm. During an interview I attempted a recursive solution, but they said a for loop would of done. So for extra practice I implemented both. def try_again_recursive(response, try_count=1): \"\"\" This function will take a response object and do the following: - If we get 'pass', return 'we connected!' - If we get '404', return '404 Not Found Error' - If we get '401', try again, however each time you try again wait the square of the try seconds before trying again. Try a maximum of 5 times before returning 'client could not be authenticated.' Example: try1: wait 1 seconds to retry try2: wait 4 seconds to retry try3: wait 9 seconds to retry try4: wait 16 seconds to retry try5: wait 0 seconds to retry! Since the max tries is 5, there is no need to wait the additional 25 seconds before exiting. \"\"\" if response == '404': return '404 Not Found Error' elif response == '401': if try_count == 5: print('for n = {0}, waiting {1} seconds.'.format(try_count, 0)) print('client could not be authenticated.') else: print('for n = {0}, waiting {1} seconds.'.format(try_count, try_count**2)) time.sleep(try_count**2) try_again_recursive(get_response(), try_count=try_count+1) else: return 'we connected!' return response Check out the output below # Tests # >>>try_again_recursive(get_response()) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' One thing to note, I cheated a little. For the sake of practicing the multiple calls, I made sure the get_response() function always returned a '401'. We will address this a little later. For now let's implement the for loop version. # For Loop Solution: def try_again_for_loop(response): \"\"\" This function will take a response object and do the following: - If we get 'pass', return 'we connected!' - If we get '404', return '404 Not Found Error' - If we get '404', try again, however each time you try again wait the square of the try seconds before trying again. Try a maximum of 5 times before returning 'client could not be authenticated.' Example: try1: wait 1 seconds to retry try2: wait 4 seconds to retry try3: wait 9 seconds to retry try4: wait 16 seconds to retry try5: wait 0 seconds to retry! Since the max tries is 5, there is no need to wait the additional 25 seconds before exiting. \"\"\" for i in range(1,6): # [1,2,3,4,5] if response == '404': return '404 Not Found Error' elif response == '401': if i == 5: print('for n = {0}, waiting {1} seconds.'.format(i, 0)) print('client could not be authenticated.') else: # pdb.set_trace() print('for n = {0}, waiting {1} seconds.'.format(i, i**2)) time.sleep(i**2) response = get_response() else: return 'we connected!' return response Output: # >>> try_again_for_loop(get_response()) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' Now we need to simulate reality a little better to give the other logic a chance to be executed. Let's make it so get_response() could return any of the other values ('pass','404'). Go into the recursive version of try_again() and find this line try_again_recursive(get_response(), try_count=try_count+1) , and change it to try_again_recursive(get_response(random.randint(1,3)), try_count=try_count+1) . You will need to import random at the top of your script. Then also add to your call of try_again_recursive() Results: # Simulation # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # 'we connected!' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # '401' # >>> try_again_recursive(get_response(random.randint(1,3))) # '404 Not Found Error' # >>> try_again_recursive(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # for n = 4, waiting 16 seconds. # for n = 5, waiting 0 seconds. # client could not be authenticated. # '401' Again, the for loop version changes the same thing, ie. response = get_response( to this response = get_response(random.rendint(1,3)) . # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # '404 Not Found Error' # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # 'we connected!' # >>> try_again_for_loop(get_response(random.randint(1,3))) # for n = 1, waiting 1 seconds. # for n = 2, waiting 4 seconds. # for n = 3, waiting 9 seconds. # 'we connected!' # >>> try_again_for_loop(get_response(random.randint(1,3))) # '404 Not Found Error'","title":"retry()"},{"location":"notes/algorithms/python/stairs/","text":"Stairs How many ways can you traverse n stairs if you can only take 1 or 2 steps. This is a cool little chunck of code illustrating and intersting way recurtsion solves a tough problem. _ |_ |_ |... how many ways can you traverse n stairs if you can only take 1 or 2 steps. Example: n = 1 (1) answer = 1 Example: n = 2 (1,1), (2) answer = 2 Example: n = 3 (1,1,1), (1,2), (2,1) answer = 3 Example: n = 4 (1,1,1,1), (1,1,2), (1,2,1), (2,1,1), (2,2) answer = 5 Example: n = 5 (1,1,1,1,1), (1,1,1,2), (1,1,2,1), (1,2,1,1), (2,1,1,1), (1,2,2), (2,1,2), (2,2,1) answer = 8 Pattern: n stairs_combo(n) 1 1 2 2 3 3 4 5 5 8 What do we notice? Answer Pattern: n stairs_combo(n) 1 1 = pattern doesn't hold 2 2 = pattern doesn't hold 3 3 = stairs_combo(2) + stairs_combo(1) = 2 + 1 = 3 4 5 = stairs_combo(3) + stairs_combo(2) = 3 + 2 = 5 5 8 = stairs_combo(4) + stairs_combo(3) = 5 + 3 = 8 We can use recursion on this since the answer depends on answers before it. For the first two values n can have (1,2) we will setup base cases to kill the recursive stack. Here is the answer: def stairs_combo(n=0): if n == 0: return 0 elif n == 1: return 1 elif n == 2: return 2 else: return stairs_combo(n-1) + stairs_combo(n-2) print(stairs_combo()) # 0 print(stairs_combo(1)) # 1 print(stairs_combo(2)) # 2 print(stairs_combo(3)) # 3 print(stairs_combo(4)) # 5 print(stairs_combo(5)) # 8 The other solution is to figure out the combinations for real! ;)","title":"Stairs"},{"location":"notes/algorithms/python/stairs/#stairs","text":"How many ways can you traverse n stairs if you can only take 1 or 2 steps. This is a cool little chunck of code illustrating and intersting way recurtsion solves a tough problem. _ |_ |_ |... how many ways can you traverse n stairs if you can only take 1 or 2 steps. Example: n = 1 (1) answer = 1 Example: n = 2 (1,1), (2) answer = 2 Example: n = 3 (1,1,1), (1,2), (2,1) answer = 3 Example: n = 4 (1,1,1,1), (1,1,2), (1,2,1), (2,1,1), (2,2) answer = 5 Example: n = 5 (1,1,1,1,1), (1,1,1,2), (1,1,2,1), (1,2,1,1), (2,1,1,1), (1,2,2), (2,1,2), (2,2,1) answer = 8 Pattern: n stairs_combo(n) 1 1 2 2 3 3 4 5 5 8 What do we notice? Answer Pattern: n stairs_combo(n) 1 1 = pattern doesn't hold 2 2 = pattern doesn't hold 3 3 = stairs_combo(2) + stairs_combo(1) = 2 + 1 = 3 4 5 = stairs_combo(3) + stairs_combo(2) = 3 + 2 = 5 5 8 = stairs_combo(4) + stairs_combo(3) = 5 + 3 = 8 We can use recursion on this since the answer depends on answers before it. For the first two values n can have (1,2) we will setup base cases to kill the recursive stack. Here is the answer: def stairs_combo(n=0): if n == 0: return 0 elif n == 1: return 1 elif n == 2: return 2 else: return stairs_combo(n-1) + stairs_combo(n-2) print(stairs_combo()) # 0 print(stairs_combo(1)) # 1 print(stairs_combo(2)) # 2 print(stairs_combo(3)) # 3 print(stairs_combo(4)) # 5 print(stairs_combo(5)) # 8 The other solution is to figure out the combinations for real! ;)","title":"Stairs"},{"location":"notes/algorithms/python/swap_case/","text":"sWAP cASE Switch case of all letters in a string. You are given a string and your task is to swap cases. In other words, convert all lowercase letters to uppercase letters and vice versa. For Example: Www.HackerRank.com \u2192 wWW.hACKERrANK.COM Pythonist 2 \u2192 pYTHONIST 2 Input Format: A single line containing a string S. Constraints: * 0<len(S)<=1000 Output Format: Print the modified string S. Sample Input: HackerRank.com presents \"Pythonist 2\". Sample Output: hACKERrANK.COM PRESENTS \"pYTHONIST 2\". Code: A = 'HackerRank.com presents \"Pythonist 2\".' def swap_case(s): S = list(s) for _ in range(len(S)): if S[_] == S[_].lower(): S[_] = S[_].upper() elif S[_] == S[_].upper(): S[_] = S[_].lower() return ''.join(S) print(swap_case(A))","title":"sWAP cASE"},{"location":"notes/algorithms/python/swap_case/#swap-case","text":"Switch case of all letters in a string. You are given a string and your task is to swap cases. In other words, convert all lowercase letters to uppercase letters and vice versa. For Example: Www.HackerRank.com \u2192 wWW.hACKERrANK.COM Pythonist 2 \u2192 pYTHONIST 2 Input Format: A single line containing a string S. Constraints: * 0<len(S)<=1000 Output Format: Print the modified string S. Sample Input: HackerRank.com presents \"Pythonist 2\". Sample Output: hACKERrANK.COM PRESENTS \"pYTHONIST 2\". Code: A = 'HackerRank.com presents \"Pythonist 2\".' def swap_case(s): S = list(s) for _ in range(len(S)): if S[_] == S[_].lower(): S[_] = S[_].upper() elif S[_] == S[_].upper(): S[_] = S[_].lower() return ''.join(S) print(swap_case(A))","title":"sWAP cASE"},{"location":"notes/algorithms/python/wheel/wheel/","text":"Make a Spinning Wheel To start we need picture like states. We can make these with strings. Import what you need import os import sys import time import math state_1 = \"\"\" -|- | / | \\\\ | | | | | | | | | | | | \\\\ | / | | -|- \"\"\" state_2 = \"\"\" / / / / / / / / / / / / / / \"\"\" state_3 = \"\"\" --------------- --------------- --------------- \"\"\" state_4 = \"\"\" - / \\\\ ---------------------------------------- \\\\ / - \"\"\" state_5 = \"\"\" --------------- --------------- --------------- \"\"\" state_6 = \"\"\" \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \"\"\" Now define a generator to access the wheel states infintely: def spinning_cursor(): while True: for cursor in [state_1,state_2,state_3,state_4,state_5,state_6]: yield cursor Next wheel size and set placeholders to keep track of the elapsed time and velocity : radius = 1 # ft spinner = spinning_cursor() elapsed_time = 0 velocity = 0 Choose the speed in terms of rotations per second: rotations_per_second = 3 rotations_desired = 2 \"DJ spin that shit!\" - KS - for _ in range(6*rotations_desired): # there are 6 states and we need all of them per rotation state = next(spinner) # call the next state if state == state_1 and elapsed_time != 0: # Once we've done a full rotation we should have enough info to calculate the velocity velocity = 2*math.pi*radius / elapsed_time # ft/s # In 1 second we do 6 states * rotations_per_second. # We can divide 1 second into these partitions to find out # how much time has passed between states. elapsed_time += 1.0 / (6*rotations_per_second) # seconds sys.stdout.write(state) # see the state sys.stdout.flush() # clear screen time.sleep(1.0/(6*rotations_per_second)) # control the rate of execution os.system('clear') # clear the screen of any states os.system('clear') # clear the screen of any states print(\"velocity: {} in miles/hour\".format(velocity/5280*60*60)) # velocity is currently in ft/s. # 5280 ft in a mile # 60 seconds in a min # 60 min in an hour Your browser does not support HTML5 video.","title":"Wheel"},{"location":"notes/algorithms/python/wheel/wheel/#make-a-spinning-wheel","text":"To start we need picture like states. We can make these with strings. Import what you need import os import sys import time import math state_1 = \"\"\" -|- | / | \\\\ | | | | | | | | | | | | \\\\ | / | | -|- \"\"\" state_2 = \"\"\" / / / / / / / / / / / / / / \"\"\" state_3 = \"\"\" --------------- --------------- --------------- \"\"\" state_4 = \"\"\" - / \\\\ ---------------------------------------- \\\\ / - \"\"\" state_5 = \"\"\" --------------- --------------- --------------- \"\"\" state_6 = \"\"\" \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \"\"\" Now define a generator to access the wheel states infintely: def spinning_cursor(): while True: for cursor in [state_1,state_2,state_3,state_4,state_5,state_6]: yield cursor Next wheel size and set placeholders to keep track of the elapsed time and velocity : radius = 1 # ft spinner = spinning_cursor() elapsed_time = 0 velocity = 0 Choose the speed in terms of rotations per second: rotations_per_second = 3 rotations_desired = 2 \"DJ spin that shit!\" - KS - for _ in range(6*rotations_desired): # there are 6 states and we need all of them per rotation state = next(spinner) # call the next state if state == state_1 and elapsed_time != 0: # Once we've done a full rotation we should have enough info to calculate the velocity velocity = 2*math.pi*radius / elapsed_time # ft/s # In 1 second we do 6 states * rotations_per_second. # We can divide 1 second into these partitions to find out # how much time has passed between states. elapsed_time += 1.0 / (6*rotations_per_second) # seconds sys.stdout.write(state) # see the state sys.stdout.flush() # clear screen time.sleep(1.0/(6*rotations_per_second)) # control the rate of execution os.system('clear') # clear the screen of any states os.system('clear') # clear the screen of any states print(\"velocity: {} in miles/hour\".format(velocity/5280*60*60)) # velocity is currently in ft/s. # 5280 ft in a mile # 60 seconds in a min # 60 min in an hour Your browser does not support HTML5 video.","title":"Make a Spinning Wheel"},{"location":"notes/bash/basics/","text":"Basic bash commands Help from the man ;) This small section is to show you that help is on the way with man . Type this prefacing any other command in bash and you can get help on it: $ man ls NAME ls - list directory contents . . . Where are you? Find your location with pwd : bbearce@bbearce-XPS-15-9560:~$ pwd /home/bbearce Look around with ls : bbearce@bbearce-XPS-15-9560:~$ ls check-config.sh gems Slicer-4.10.2-linux-amd64 Desktop Music snap docker_practice pgAdmin4 src Documents Pictures Templates Downloads Public Videos Dropbox (Partners HealthCare) R wget-log examples.desktop seaborn-data -l With -l you can see things listed out vertically bbearce@bbearce-XPS-15-9560:~$ ls -l total 2268 -rw-rw-r-- 1 bbearce bbearce 10314 May 20 09:40 check-config.sh drwxr-xr-x 2 bbearce bbearce 4096 Sep 18 11:15 Desktop drwxrwxr-x 2 bbearce bbearce 4096 Sep 5 18:11 docker_practice drwxr-xr-x 14 bbearce bbearce 4096 Sep 18 13:37 Documents drwxr-xr-x 4 bbearce bbearce 4096 Sep 19 14:44 Downloads drwx------ 7 bbearce bbearce 4096 Sep 18 11:22 Dropbox (Partners HealthCare) -rw-r--r-- 1 bbearce bbearce 8980 May 16 16:13 examples.desktop drwxrwxr-x 9 bbearce bbearce 4096 Jul 15 13:53 gems drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Music drwxrwxr-x 3 bbearce bbearce 4096 Aug 8 09:10 pgAdmin4 drwxr-xr-x 2 bbearce bbearce 4096 Jun 17 15:07 Pictures drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Public drwxrwxr-x 3 bbearce bbearce 4096 May 17 09:02 R drwxrwxr-x 2 bbearce bbearce 4096 Jul 19 17:36 seaborn-data drwxrwxr-x 8 bbearce bbearce 4096 Jun 4 13:06 Slicer-4.10.2-linux-amd64 drwxr-xr-x 8 bbearce bbearce 4096 Jul 25 16:43 snap drwxrwxr-x 3 bbearce bbearce 4096 May 23 16:12 src drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Templates drwxr-xr-x 2 bbearce bbearce 4096 Aug 30 13:55 Videos -rw-rw-r-- 1 bbearce bbearce 2220414 Aug 16 17:05 wget-log The columns are using ls -lai : +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ | index number | file permissions | number of links | owner | group | size | month | day | time | filename | +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ | 933442 | -rwxrw-r-- | 10 | root | root | 2048 | Jan | 13 | 07:11 | afile.exe | +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ -a With -a you show hidden files: Hidden files are denoted with a .<file_name> bbearce@bbearce-XPS-15-9560:~$ ls -a . gems .pyenv .. .gitconfig .pylint.d .azure .gksu.lock .python_history .azure-shell .gnome R .bash_history .gnupg .Rhistory .bash_logout .hplip .rstudio-desktop .bashrc .ICEauthority seaborn-data .bundle .ipython Slicer-4.10.2-linux-amd64 .cache .java snap check-config.sh .jupyter .sqlite_history .compiz .kde src .config .lesshst .ssh .DataGrip2019.1 .local .sudo_as_admin_successful Desktop .mozilla .systemtap .dmrc Music Templates .docker .nano Videos docker_practice .node_repl_history .viminfo Documents .pgadmin .vscode Downloads pgAdmin4 .wget-hsts .dropbox Pictures wget-log -la Using both -la : bbearce@bbearce-XPS-15-9560:~$ ls -la total 2564 drwxrwxrwx 50 bbearce bbearce 4096 Sep 19 22:01 . drwxr-xr-x 3 root root 4096 May 29 15:05 .. drwxrwxr-x 6 bbearce bbearce 4096 May 23 16:51 .azure drwxrwxr-x 3 bbearce bbearce 4096 May 23 16:51 .azure-shell -rw------- 1 bbearce bbearce 56352 Sep 19 18:05 .bash_history -rw-r--r-- 1 bbearce bbearce 220 May 16 16:13 .bash_logout -rw-r--r-- 1 bbearce bbearce 3865 Jun 20 16:55 .bashrc drwxrwxr-x 4 bbearce bbearce 4096 Jul 10 14:28 .bundle drwx------ 48 bbearce bbearce 4096 Sep 9 13:31 .cache -rw-rw-r-- 1 bbearce bbearce 10314 May 20 09:40 check-config.sh drwx------ 3 bbearce bbearce 4096 May 16 16:50 .compiz drwx------ 43 bbearce bbearce 4096 Sep 12 11:10 .config drwxrwxr-x 4 bbearce bbearce 4096 Jul 25 16:43 .DataGrip2019.1 drwxr-xr-x 2 bbearce bbearce 4096 Sep 18 11:15 Desktop -rw-r--r-- 1 bbearce bbearce 25 May 16 16:49 .dmrc drwxrwx--- 3 bbearce bbearce 4096 Sep 5 17:35 .docker drwxrwxr-x 2 bbearce bbearce 4096 Sep 5 18:11 docker_practice drwxr-xr-x 14 bbearce bbearce 4096 Sep 18 13:37 Documents drwxr-xr-x 4 bbearce bbearce 4096 Sep 19 14:44 Downloads drwx------ 7 bbearce bbearce 4096 Sep 19 22:01 .dropbox drwxrwxr-x 3 bbearce bbearce 4096 Sep 18 03:24 .dropbox-dist drwx------ 7 bbearce bbearce 4096 Sep 18 11:22 Dropbox (Partners HealthCare) -rw-r--r-- 1 bbearce bbearce 8980 May 16 16:13 examples.desktop drwx------ 2 bbearce bbearce 4096 Sep 17 06:50 .gconf drwxrwxr-x 4 bbearce bbearce 4096 Jun 20 16:33 .gem drwxrwxr-x 9 bbearce bbearce 4096 Jul 15 13:53 gems -rw-rw-r-- 1 bbearce bbearce 57 May 21 17:11 .gitconfig -rw-r----- 1 bbearce bbearce 0 May 31 12:23 .gksu.lock drwx------ 3 bbearce bbearce 4096 May 17 08:25 .gnome drwx------ 3 bbearce bbearce 4096 Sep 19 22:01 .gnupg drwxr-xr-x 2 bbearce bbearce 4096 Jun 28 10:21 .hplip -rw------- 1 bbearce bbearce 27762 Sep 19 22:01 .ICEauthority drwxr-xr-x 5 bbearce bbearce 4096 Jun 16 18:30 .ipython drwxrwxr-x 4 bbearce bbearce 4096 Jul 25 16:43 .java drwxrwxr-x 3 bbearce bbearce 4096 Jun 16 22:35 .jupyter drwx------ 3 bbearce bbearce 4096 Jun 9 21:43 .kde -rw------- 1 bbearce bbearce 99 Sep 5 19:01 .lesshst drwx------ 6 bbearce bbearce 4096 Jun 16 18:30 .local drwx------ 5 bbearce bbearce 4096 May 17 08:13 .mozilla drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Music drwxrwxr-x 2 bbearce bbearce 4096 May 22 15:23 .nano -rw-rw-r-- 1 bbearce bbearce 55 Aug 6 09:48 .node_repl_history --block_size=SIZE SIZE units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000). bbearce@bbearce-XPS-15-9560:~$ ls -la --block-size=M total 3M drwxrwxrwx 50 bbearce bbearce 1M Sep 20 09:32 . drwxr-xr-x 3 root root 1M May 29 15:05 .. drwxrwxr-x 6 bbearce bbearce 1M May 23 16:51 .azure drwxrwxr-x 3 bbearce bbearce 1M May 23 16:51 .azure-shell -rw------- 1 bbearce bbearce 1M Sep 19 18:05 .bash_history -rw-r--r-- 1 bbearce bbearce 1M May 16 16:13 .bash_logout ... Count files Use wc to count things and get their byte sizes. The -l flag stands for lines and can be used with ls and | to count files in a directory: Ex: user@server:~/$ ls data/rsna | wc -l 26684 sudo, su Notes courtesy of maketecheasier.com Using sudo you can execute code as superuser. It stands for super user do . $ man sudo NAME sudo, sudoedit \u2014 execute a command as another user . . . $ man su NAME su - change user ID or become superuser . . . $ su The su command substitutes the current user in use by the system in the shell. This will tell the system to switch (and essentially log out of) the current user to the one specified.. su is best used when a user wants direct access to the root account on the system. It doesn\u2019t go through sudo or anything like that. It is disabled by default on Ubuntu. It is recommended to use sudo -i in this case. As the note above mentions, this drops you at /root . $ sudo su This command is essentially the same as just running su in the shell. Instead of telling the system to \u201cswitch users\u201d directly, you\u2019re telling it to run the \u201csu\u201d command as root. When sudo su is run, \u201c.profile,\u201d \u201c.bashrc\u201d and \u201c/etc/profile\u201d will be started, much like running su (or su root ). This is because if any command is run with sudo in front of it, it\u2019s a command that is given root privileges. Though there isn\u2019t very much difference from \u201csu,\u201d sudo su is still a very useful command for one important reason: When a user is running \u201csu\u201d to gain root access on a system, they must know the root password. The way root is given with sudo su is by requesting the current user\u2019s password. This makes it possible to gain root without the root password which increases security. $ sudo -i Using sudo -i is virtually the same as the sudo su command. Users can gain root by \u201csudo\u201d and not by switching to the root user. Much like sudo su , the -i flag allows a user to get a root environment without having to know the root account password. sudo -i is also very similar to using sudo su in that it\u2019ll read all of the environmental files (.profile, etc.) and set the environment inside the shell with it. Where it differs from \u201csudo su\u201d is that sudo -i is a much cleaner way of gaining root and a root environment without directly interacting with the root user. How? With sudo su you\u2019re using more than one root setuid commands. This fact makes it much more challenging to figure out what environmental variables will be kept and which ones will be changed (when swamping to the root environment). This is not true with sudo -i , and it is because of this most people view it as the preferred method to gain root without logging in directly. $ sudo -s The -s switch for \u201csudo\u201d command reads the $SHELL variable of the current user executing commands. This command works as if the user is running sudo /bin/bash . sudo -s is a \u201cnon-login\u201d style shell. This means that unlike a command like sudo -i or sudo su , the system will not read any environmental files. This means that when a user tells the shell to run sudo -s , it gains root but will not change the user or the user environment. Your home will not be the root home, etc. This command is best used when the user doesn\u2019t want to touch root at all and just wants a root shell for easy command execution. Other commands talked about above gain root access, but touch root environmental files, and allow users more fuller access to root (which can be a security issue). Summary and demo of pwd when running these commands: bbearce@bbearce-XPS-15-9560:~$ sudo su root@bbearce-XPS-15-9560:/home/bbearce# pwd /home/bbearce root@bbearce-XPS-15-9560:/home/bbearce# exit exit bbearce@bbearce-XPS-15-9560:~$ sudo -i root@bbearce-XPS-15-9560:~# pwd /root root@bbearce-XPS-15-9560:~# exit logout bbearce@bbearce-XPS-15-9560:~$ sudo -s root@bbearce-XPS-15-9560:~# pwd /home/bbearce root@bbearce-XPS-15-9560:~# exit exit btw: exit is for leaving a current logged in session. bash or sh or dash? Notes courtesy of diffzi.com Bash ( bash ) is one of many available (yet the most commonly used) Unix shells. Bash stands for \" B ourne A gain SH ell\", and is a replacement/improvement of the original Bourne shell ( sh ). What is Bash? Bash is the Bourne-Again shell. Bash is an excellent full-featured shell appropriate for interactive use; indeed, it is still the default login shell. Bash is designed for human beings and provides a superset of POSIX functionality. What is Dash? Dash is the Debian Almquist Shell. Dash implements the Single Unix Spec, then it does not have to do more to meet that formal spec. Dash is for non-interactive script execution. Dash Only supports POSIX compliant features. Key Differences Bash is an excellent full-featured shell appropriate for interactive use; indeed, it is still the default login shell. However, it is rather large and slow to start up and operate by comparison with dash. Dash implements the Single Unix Spec, then it does not have to do more to meet that formal spec. But some of the \u201cbashisms\u201d are convenient, would add little to the size of dash, and would make it far easier to use dash as an alternative. A lot of shell scripts which contain the command set \u2013k are not supported by dash but supported by bash. Bash Supports the same scripting commands as Dash as well as its own additional commands, Dash Only supports POSIX compliant features. Bash is designed for human beings and provides a superset of POSIX functionality, Dash is for non-interactive script execution. Bash supports tab completion and Supports a command history. Dash is only 100K compared to Bash\u2019s 900K. Dash is for Faster start-up and script execution as compared to Bash. Moving and Copying Use the mv command to move things or rename them. Use the cp command to copy things. Use the rsync command to move things. Use the -a flag to archive and retain permissions and time stamps. scp Local to local: $ scp <from> <to> Local to Remote: $ scp [-i identity_file] <from_local> user@remote:<to_remote> Remote to Local: $ scp [-i identity_file] user@remote:<from_remote> <to_local> rsync Local to local: $ rsync <from> <to> Local to Remote: $ rsync [-e \"ssh -i <path to identity_file>\"] <from_local> user@remote:<to_remote> Remote to Local: $ rsync [-e \"ssh -i <path to identity_file>\"] user@remote:<from_remote> <to_local> Most important flags: -a -v -z rsync -avz foo:src/bar /data/tmp This would recursively transfer all files from the directory src/bar on the machine foo into the /data/tmp/bar directory on the local machine. The files are transferred in \"archive\" mode, which ensures that symbolic links, devices, attributes, permissions, ownerships, etc. are preserved in the transfer. Additionally, compression will be used to reduce the size of data portions of the transfer. -a : -a, --archive archive mode; equals -rlptgoD (no -H,-A,-X) -v : -v, --verbose increase verbosity -z : -z, --compress --progress : This option tells rsync to print information showing the progress of the transfer. This gives a bored user something to watch. Check OS Version $ cat /etc/os-release make files Courtesty of stackoverflow make is part of the build system commonly used in unix type systems - binutils . It looks at make files which hold configuration information and build targets. Specifically: ./configure - this is a script that sets up the environment for the build make - calls make with the default build target. Normally builds the app. make install - calls make with the install build target. Normally installs the app. sshfs Courtesy of github About SSHFS allows you to mount a remote filesystem using SFTP. Most SSH servers support and enable this SFTP access by default, so SSHFS is very simple to use - there's nothing to do on the server-side. Development Status SSHFS is shipped by all major Linux distributions and has been in production use across a wide range of systems for many years. However, at present SSHFS does not have any active, regular contributors, and there are a number of known issues (see the bugtracker). The current maintainer continues to apply pull requests and makes regular releases, but unfortunately has no capacity to do any development beyond addressing high-impact issues. When reporting bugs, please understand that unless you are including a pull request or are reporting a critical issue, you will probably not get a response. How to use Once sshfs is installed (see next section) running it is very simple: $ sshfs [user@]hostname:[directory] mountpoint It is recommended to run SSHFS as regular user (not as root). For this to work the mountpoint must be owned by the user. If username is omitted SSHFS will use the local username. If the directory is omitted, SSHFS will mount the (remote) home directory. If you need to enter a password sshfs will ask for it (actually it just runs ssh which ask for the password if needed). Also many ssh options can be specified (see the manual pages for sftp(1) and ssh_config(5)), including the remote port number (-oport=PORT) To unmount the filesystem: fusermount -u mountpoint On BSD and macOS, to unmount the filesystem: umount mountpoint If you get an error: fuse: mountpoint is not empty fuse: if you are sure this is safe, use the 'nonempty' mount option You might need to kill all sshfs processes and restart: ~$ killall sshfs Then restart your sshfs mounting procedure Mouse sensitivity Source: askubuntu Sometimes (happened to me) the mouse UI options in settings are not enough to control the mouse sensitivity. If that is the case then try this:","title":"Basics"},{"location":"notes/bash/basics/#basic-bash-commands","text":"","title":"Basic bash commands"},{"location":"notes/bash/basics/#help-from-the-man","text":"This small section is to show you that help is on the way with man . Type this prefacing any other command in bash and you can get help on it: $ man ls NAME ls - list directory contents . . .","title":"Help from the man ;)"},{"location":"notes/bash/basics/#where-are-you","text":"Find your location with pwd : bbearce@bbearce-XPS-15-9560:~$ pwd /home/bbearce Look around with ls : bbearce@bbearce-XPS-15-9560:~$ ls check-config.sh gems Slicer-4.10.2-linux-amd64 Desktop Music snap docker_practice pgAdmin4 src Documents Pictures Templates Downloads Public Videos Dropbox (Partners HealthCare) R wget-log examples.desktop seaborn-data","title":"Where are you?"},{"location":"notes/bash/basics/#-l","text":"With -l you can see things listed out vertically bbearce@bbearce-XPS-15-9560:~$ ls -l total 2268 -rw-rw-r-- 1 bbearce bbearce 10314 May 20 09:40 check-config.sh drwxr-xr-x 2 bbearce bbearce 4096 Sep 18 11:15 Desktop drwxrwxr-x 2 bbearce bbearce 4096 Sep 5 18:11 docker_practice drwxr-xr-x 14 bbearce bbearce 4096 Sep 18 13:37 Documents drwxr-xr-x 4 bbearce bbearce 4096 Sep 19 14:44 Downloads drwx------ 7 bbearce bbearce 4096 Sep 18 11:22 Dropbox (Partners HealthCare) -rw-r--r-- 1 bbearce bbearce 8980 May 16 16:13 examples.desktop drwxrwxr-x 9 bbearce bbearce 4096 Jul 15 13:53 gems drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Music drwxrwxr-x 3 bbearce bbearce 4096 Aug 8 09:10 pgAdmin4 drwxr-xr-x 2 bbearce bbearce 4096 Jun 17 15:07 Pictures drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Public drwxrwxr-x 3 bbearce bbearce 4096 May 17 09:02 R drwxrwxr-x 2 bbearce bbearce 4096 Jul 19 17:36 seaborn-data drwxrwxr-x 8 bbearce bbearce 4096 Jun 4 13:06 Slicer-4.10.2-linux-amd64 drwxr-xr-x 8 bbearce bbearce 4096 Jul 25 16:43 snap drwxrwxr-x 3 bbearce bbearce 4096 May 23 16:12 src drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Templates drwxr-xr-x 2 bbearce bbearce 4096 Aug 30 13:55 Videos -rw-rw-r-- 1 bbearce bbearce 2220414 Aug 16 17:05 wget-log The columns are using ls -lai : +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ | index number | file permissions | number of links | owner | group | size | month | day | time | filename | +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+ | 933442 | -rwxrw-r-- | 10 | root | root | 2048 | Jan | 13 | 07:11 | afile.exe | +--------------+------------------+-----------------+-------+-------+------+-------+-----+-------+-----------+","title":"-l"},{"location":"notes/bash/basics/#-a","text":"With -a you show hidden files: Hidden files are denoted with a .<file_name> bbearce@bbearce-XPS-15-9560:~$ ls -a . gems .pyenv .. .gitconfig .pylint.d .azure .gksu.lock .python_history .azure-shell .gnome R .bash_history .gnupg .Rhistory .bash_logout .hplip .rstudio-desktop .bashrc .ICEauthority seaborn-data .bundle .ipython Slicer-4.10.2-linux-amd64 .cache .java snap check-config.sh .jupyter .sqlite_history .compiz .kde src .config .lesshst .ssh .DataGrip2019.1 .local .sudo_as_admin_successful Desktop .mozilla .systemtap .dmrc Music Templates .docker .nano Videos docker_practice .node_repl_history .viminfo Documents .pgadmin .vscode Downloads pgAdmin4 .wget-hsts .dropbox Pictures wget-log","title":"-a"},{"location":"notes/bash/basics/#-la","text":"Using both -la : bbearce@bbearce-XPS-15-9560:~$ ls -la total 2564 drwxrwxrwx 50 bbearce bbearce 4096 Sep 19 22:01 . drwxr-xr-x 3 root root 4096 May 29 15:05 .. drwxrwxr-x 6 bbearce bbearce 4096 May 23 16:51 .azure drwxrwxr-x 3 bbearce bbearce 4096 May 23 16:51 .azure-shell -rw------- 1 bbearce bbearce 56352 Sep 19 18:05 .bash_history -rw-r--r-- 1 bbearce bbearce 220 May 16 16:13 .bash_logout -rw-r--r-- 1 bbearce bbearce 3865 Jun 20 16:55 .bashrc drwxrwxr-x 4 bbearce bbearce 4096 Jul 10 14:28 .bundle drwx------ 48 bbearce bbearce 4096 Sep 9 13:31 .cache -rw-rw-r-- 1 bbearce bbearce 10314 May 20 09:40 check-config.sh drwx------ 3 bbearce bbearce 4096 May 16 16:50 .compiz drwx------ 43 bbearce bbearce 4096 Sep 12 11:10 .config drwxrwxr-x 4 bbearce bbearce 4096 Jul 25 16:43 .DataGrip2019.1 drwxr-xr-x 2 bbearce bbearce 4096 Sep 18 11:15 Desktop -rw-r--r-- 1 bbearce bbearce 25 May 16 16:49 .dmrc drwxrwx--- 3 bbearce bbearce 4096 Sep 5 17:35 .docker drwxrwxr-x 2 bbearce bbearce 4096 Sep 5 18:11 docker_practice drwxr-xr-x 14 bbearce bbearce 4096 Sep 18 13:37 Documents drwxr-xr-x 4 bbearce bbearce 4096 Sep 19 14:44 Downloads drwx------ 7 bbearce bbearce 4096 Sep 19 22:01 .dropbox drwxrwxr-x 3 bbearce bbearce 4096 Sep 18 03:24 .dropbox-dist drwx------ 7 bbearce bbearce 4096 Sep 18 11:22 Dropbox (Partners HealthCare) -rw-r--r-- 1 bbearce bbearce 8980 May 16 16:13 examples.desktop drwx------ 2 bbearce bbearce 4096 Sep 17 06:50 .gconf drwxrwxr-x 4 bbearce bbearce 4096 Jun 20 16:33 .gem drwxrwxr-x 9 bbearce bbearce 4096 Jul 15 13:53 gems -rw-rw-r-- 1 bbearce bbearce 57 May 21 17:11 .gitconfig -rw-r----- 1 bbearce bbearce 0 May 31 12:23 .gksu.lock drwx------ 3 bbearce bbearce 4096 May 17 08:25 .gnome drwx------ 3 bbearce bbearce 4096 Sep 19 22:01 .gnupg drwxr-xr-x 2 bbearce bbearce 4096 Jun 28 10:21 .hplip -rw------- 1 bbearce bbearce 27762 Sep 19 22:01 .ICEauthority drwxr-xr-x 5 bbearce bbearce 4096 Jun 16 18:30 .ipython drwxrwxr-x 4 bbearce bbearce 4096 Jul 25 16:43 .java drwxrwxr-x 3 bbearce bbearce 4096 Jun 16 22:35 .jupyter drwx------ 3 bbearce bbearce 4096 Jun 9 21:43 .kde -rw------- 1 bbearce bbearce 99 Sep 5 19:01 .lesshst drwx------ 6 bbearce bbearce 4096 Jun 16 18:30 .local drwx------ 5 bbearce bbearce 4096 May 17 08:13 .mozilla drwxr-xr-x 2 bbearce bbearce 4096 May 16 16:49 Music drwxrwxr-x 2 bbearce bbearce 4096 May 22 15:23 .nano -rw-rw-r-- 1 bbearce bbearce 55 Aug 6 09:48 .node_repl_history","title":"-la"},{"location":"notes/bash/basics/#-block_sizesize","text":"SIZE units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000). bbearce@bbearce-XPS-15-9560:~$ ls -la --block-size=M total 3M drwxrwxrwx 50 bbearce bbearce 1M Sep 20 09:32 . drwxr-xr-x 3 root root 1M May 29 15:05 .. drwxrwxr-x 6 bbearce bbearce 1M May 23 16:51 .azure drwxrwxr-x 3 bbearce bbearce 1M May 23 16:51 .azure-shell -rw------- 1 bbearce bbearce 1M Sep 19 18:05 .bash_history -rw-r--r-- 1 bbearce bbearce 1M May 16 16:13 .bash_logout ...","title":"--block_size=SIZE"},{"location":"notes/bash/basics/#count-files","text":"Use wc to count things and get their byte sizes. The -l flag stands for lines and can be used with ls and | to count files in a directory: Ex: user@server:~/$ ls data/rsna | wc -l 26684","title":"Count files"},{"location":"notes/bash/basics/#sudo-su","text":"Notes courtesy of maketecheasier.com Using sudo you can execute code as superuser. It stands for super user do . $ man sudo NAME sudo, sudoedit \u2014 execute a command as another user . . . $ man su NAME su - change user ID or become superuser . . .","title":"sudo, su"},{"location":"notes/bash/basics/#su","text":"The su command substitutes the current user in use by the system in the shell. This will tell the system to switch (and essentially log out of) the current user to the one specified.. su is best used when a user wants direct access to the root account on the system. It doesn\u2019t go through sudo or anything like that. It is disabled by default on Ubuntu. It is recommended to use sudo -i in this case. As the note above mentions, this drops you at /root .","title":"$ su"},{"location":"notes/bash/basics/#sudo-su_1","text":"This command is essentially the same as just running su in the shell. Instead of telling the system to \u201cswitch users\u201d directly, you\u2019re telling it to run the \u201csu\u201d command as root. When sudo su is run, \u201c.profile,\u201d \u201c.bashrc\u201d and \u201c/etc/profile\u201d will be started, much like running su (or su root ). This is because if any command is run with sudo in front of it, it\u2019s a command that is given root privileges. Though there isn\u2019t very much difference from \u201csu,\u201d sudo su is still a very useful command for one important reason: When a user is running \u201csu\u201d to gain root access on a system, they must know the root password. The way root is given with sudo su is by requesting the current user\u2019s password. This makes it possible to gain root without the root password which increases security.","title":"$ sudo su"},{"location":"notes/bash/basics/#sudo-i","text":"Using sudo -i is virtually the same as the sudo su command. Users can gain root by \u201csudo\u201d and not by switching to the root user. Much like sudo su , the -i flag allows a user to get a root environment without having to know the root account password. sudo -i is also very similar to using sudo su in that it\u2019ll read all of the environmental files (.profile, etc.) and set the environment inside the shell with it. Where it differs from \u201csudo su\u201d is that sudo -i is a much cleaner way of gaining root and a root environment without directly interacting with the root user. How? With sudo su you\u2019re using more than one root setuid commands. This fact makes it much more challenging to figure out what environmental variables will be kept and which ones will be changed (when swamping to the root environment). This is not true with sudo -i , and it is because of this most people view it as the preferred method to gain root without logging in directly.","title":"$ sudo -i"},{"location":"notes/bash/basics/#sudo-s","text":"The -s switch for \u201csudo\u201d command reads the $SHELL variable of the current user executing commands. This command works as if the user is running sudo /bin/bash . sudo -s is a \u201cnon-login\u201d style shell. This means that unlike a command like sudo -i or sudo su , the system will not read any environmental files. This means that when a user tells the shell to run sudo -s , it gains root but will not change the user or the user environment. Your home will not be the root home, etc. This command is best used when the user doesn\u2019t want to touch root at all and just wants a root shell for easy command execution. Other commands talked about above gain root access, but touch root environmental files, and allow users more fuller access to root (which can be a security issue).","title":"$ sudo -s"},{"location":"notes/bash/basics/#summary-and-demo-of-pwd-when-running-these-commands","text":"bbearce@bbearce-XPS-15-9560:~$ sudo su root@bbearce-XPS-15-9560:/home/bbearce# pwd /home/bbearce root@bbearce-XPS-15-9560:/home/bbearce# exit exit bbearce@bbearce-XPS-15-9560:~$ sudo -i root@bbearce-XPS-15-9560:~# pwd /root root@bbearce-XPS-15-9560:~# exit logout bbearce@bbearce-XPS-15-9560:~$ sudo -s root@bbearce-XPS-15-9560:~# pwd /home/bbearce root@bbearce-XPS-15-9560:~# exit exit btw: exit is for leaving a current logged in session.","title":"Summary and demo of pwd when running these commands:"},{"location":"notes/bash/basics/#bash-or-sh-or-dash","text":"Notes courtesy of diffzi.com Bash ( bash ) is one of many available (yet the most commonly used) Unix shells. Bash stands for \" B ourne A gain SH ell\", and is a replacement/improvement of the original Bourne shell ( sh ).","title":"bash or sh or dash?"},{"location":"notes/bash/basics/#what-is-bash","text":"Bash is the Bourne-Again shell. Bash is an excellent full-featured shell appropriate for interactive use; indeed, it is still the default login shell. Bash is designed for human beings and provides a superset of POSIX functionality.","title":"What is Bash?"},{"location":"notes/bash/basics/#what-is-dash","text":"Dash is the Debian Almquist Shell. Dash implements the Single Unix Spec, then it does not have to do more to meet that formal spec. Dash is for non-interactive script execution. Dash Only supports POSIX compliant features.","title":"What is Dash?"},{"location":"notes/bash/basics/#key-differences","text":"Bash is an excellent full-featured shell appropriate for interactive use; indeed, it is still the default login shell. However, it is rather large and slow to start up and operate by comparison with dash. Dash implements the Single Unix Spec, then it does not have to do more to meet that formal spec. But some of the \u201cbashisms\u201d are convenient, would add little to the size of dash, and would make it far easier to use dash as an alternative. A lot of shell scripts which contain the command set \u2013k are not supported by dash but supported by bash. Bash Supports the same scripting commands as Dash as well as its own additional commands, Dash Only supports POSIX compliant features. Bash is designed for human beings and provides a superset of POSIX functionality, Dash is for non-interactive script execution. Bash supports tab completion and Supports a command history. Dash is only 100K compared to Bash\u2019s 900K. Dash is for Faster start-up and script execution as compared to Bash.","title":"Key Differences"},{"location":"notes/bash/basics/#moving-and-copying","text":"Use the mv command to move things or rename them. Use the cp command to copy things. Use the rsync command to move things. Use the -a flag to archive and retain permissions and time stamps.","title":"Moving and Copying"},{"location":"notes/bash/basics/#scp","text":"Local to local: $ scp <from> <to> Local to Remote: $ scp [-i identity_file] <from_local> user@remote:<to_remote> Remote to Local: $ scp [-i identity_file] user@remote:<from_remote> <to_local>","title":"scp"},{"location":"notes/bash/basics/#rsync","text":"Local to local: $ rsync <from> <to> Local to Remote: $ rsync [-e \"ssh -i <path to identity_file>\"] <from_local> user@remote:<to_remote> Remote to Local: $ rsync [-e \"ssh -i <path to identity_file>\"] user@remote:<from_remote> <to_local> Most important flags: -a -v -z rsync -avz foo:src/bar /data/tmp This would recursively transfer all files from the directory src/bar on the machine foo into the /data/tmp/bar directory on the local machine. The files are transferred in \"archive\" mode, which ensures that symbolic links, devices, attributes, permissions, ownerships, etc. are preserved in the transfer. Additionally, compression will be used to reduce the size of data portions of the transfer. -a : -a, --archive archive mode; equals -rlptgoD (no -H,-A,-X) -v : -v, --verbose increase verbosity -z : -z, --compress --progress : This option tells rsync to print information showing the progress of the transfer. This gives a bored user something to watch.","title":"rsync"},{"location":"notes/bash/basics/#check-os-version","text":"$ cat /etc/os-release","title":"Check OS Version"},{"location":"notes/bash/basics/#make-files","text":"Courtesty of stackoverflow make is part of the build system commonly used in unix type systems - binutils . It looks at make files which hold configuration information and build targets. Specifically: ./configure - this is a script that sets up the environment for the build make - calls make with the default build target. Normally builds the app. make install - calls make with the install build target. Normally installs the app.","title":"make files"},{"location":"notes/bash/basics/#sshfs","text":"Courtesy of github","title":"sshfs"},{"location":"notes/bash/basics/#about","text":"SSHFS allows you to mount a remote filesystem using SFTP. Most SSH servers support and enable this SFTP access by default, so SSHFS is very simple to use - there's nothing to do on the server-side.","title":"About"},{"location":"notes/bash/basics/#development-status","text":"SSHFS is shipped by all major Linux distributions and has been in production use across a wide range of systems for many years. However, at present SSHFS does not have any active, regular contributors, and there are a number of known issues (see the bugtracker). The current maintainer continues to apply pull requests and makes regular releases, but unfortunately has no capacity to do any development beyond addressing high-impact issues. When reporting bugs, please understand that unless you are including a pull request or are reporting a critical issue, you will probably not get a response.","title":"Development Status"},{"location":"notes/bash/basics/#how-to-use","text":"Once sshfs is installed (see next section) running it is very simple: $ sshfs [user@]hostname:[directory] mountpoint It is recommended to run SSHFS as regular user (not as root). For this to work the mountpoint must be owned by the user. If username is omitted SSHFS will use the local username. If the directory is omitted, SSHFS will mount the (remote) home directory. If you need to enter a password sshfs will ask for it (actually it just runs ssh which ask for the password if needed). Also many ssh options can be specified (see the manual pages for sftp(1) and ssh_config(5)), including the remote port number (-oport=PORT) To unmount the filesystem: fusermount -u mountpoint On BSD and macOS, to unmount the filesystem: umount mountpoint If you get an error: fuse: mountpoint is not empty fuse: if you are sure this is safe, use the 'nonempty' mount option You might need to kill all sshfs processes and restart: ~$ killall sshfs Then restart your sshfs mounting procedure","title":"How to use"},{"location":"notes/bash/basics/#mouse-sensitivity","text":"Source: askubuntu Sometimes (happened to me) the mouse UI options in settings are not enough to control the mouse sensitivity. If that is the case then try this:","title":"Mouse sensitivity"},{"location":"notes/bash/environment_variables/","text":"Environment Variables $PATH Courtesy of smallbusiness.chron.com Ubuntu Linux, as well as all other Linux distributions, uses the PATH variable to tell the operating system where to look for executable commands. Typically these commands are located in the /usr/sbin, usr/bin and /sbin, and /bin directories. Other command directories can be added to this list of directories by adding them to the PATH variable. You can choose to make a user-specified directory available for a single user or the entire system, depending on your company's requirements. Add to Path Add a path to the PATH variable: export PATH=$PATH:/my/custom/path This appends your new path to the old path and overwrites PATH. Remove from Path Execute echo $PATH to see full path variable and copy it to clipboard. Then remove a specific path between colons ...:<some path>:<path to be removed>:... and paste that into this command: export PATH=<new full path minus the path you do not want> Note thatwe aren't appending to $PATH this time, but overwriting it.","title":"Environment Variables"},{"location":"notes/bash/environment_variables/#environment-variables","text":"","title":"Environment Variables"},{"location":"notes/bash/environment_variables/#path","text":"Courtesy of smallbusiness.chron.com Ubuntu Linux, as well as all other Linux distributions, uses the PATH variable to tell the operating system where to look for executable commands. Typically these commands are located in the /usr/sbin, usr/bin and /sbin, and /bin directories. Other command directories can be added to this list of directories by adding them to the PATH variable. You can choose to make a user-specified directory available for a single user or the entire system, depending on your company's requirements.","title":"$PATH"},{"location":"notes/bash/environment_variables/#add-to-path","text":"Add a path to the PATH variable: export PATH=$PATH:/my/custom/path This appends your new path to the old path and overwrites PATH.","title":"Add to Path"},{"location":"notes/bash/environment_variables/#remove-from-path","text":"Execute echo $PATH to see full path variable and copy it to clipboard. Then remove a specific path between colons ...:<some path>:<path to be removed>:... and paste that into this command: export PATH=<new full path minus the path you do not want> Note thatwe aren't appending to $PATH this time, but overwriting it.","title":"Remove from Path"},{"location":"notes/bash/mount/","text":"Mount Courtesy of Linuxize Example, to mount the /dev/sdb1 file system to the /mnt/media directory you would use: sudo mount /dev/sdb1 /mnt/media","title":"Mount"},{"location":"notes/bash/mount/#mount","text":"Courtesy of Linuxize Example, to mount the /dev/sdb1 file system to the /mnt/media directory you would use: sudo mount /dev/sdb1 /mnt/media","title":"Mount"},{"location":"notes/bash/pdfs/","text":"PDFs Command line utility pdftk is used to create pdfs. Install: sudo apt instal pdftk Use: pdftk page1.pdf page2.pdf output mergedfile.pdf","title":"PDFs"},{"location":"notes/bash/pdfs/#pdfs","text":"Command line utility pdftk is used to create pdfs. Install: sudo apt instal pdftk Use: pdftk page1.pdf page2.pdf output mergedfile.pdf","title":"PDFs"},{"location":"notes/bash/processes/","text":"Processes Kill a process (TL;DR) $ kill <pid> Ex: $ kill 1234 By Port Source mr-khan.gitlab.io Sometimes I fed up with searching my program PID. As you know the port number so you can easily find the port PID and kill it. If you want to kill a process running on port number 8000 then first you need to find the PID and then kill it. Run the following command to find port number PID: sudo lsof -t -i:8000 then kill: sudo kill $(sudo lsof -t -i:8000) Courtesy of booleanworld.com ...more...below: Use top to see running processes. azureuser@miccai:~$ top top - 19:14:36 up 28 min, 1 user, load average: 0.00, 0.02, 0.05 Tasks: 115 total, 1 running, 114 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 3514568 total, 715812 used, 2798756 free, 32776 buffers KiB Swap: 0 total, 0 used, 0 free. 336792 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.10 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:00.19 kworker/u256:0 7 root 20 0 0 0 0 S 0.0 0.0 0:00.90 rcu_sched 8 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_bh 9 root 20 0 0 0 0 S 0.0 0.0 0:00.99 rcuos/0 10 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/0 11 root rt 0 0 0 0 S 0.0 0.0 0:00.07 migration/0 12 root rt 0 0 0 0 S 0.0 0.0 0:00.02 watchdog/0 13 root rt 0 0 0 0 S 0.0 0.0 0:00.02 watchdog/1 14 root rt 0 0 0 0 S 0.0 0.0 0:00.03 migration/1 15 root 20 0 0 0 0 S 0.0 0.0 0:00.11 ksoftirqd/1 17 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/1:0H 18 root 20 0 0 0 0 S 0.0 0.0 0:00.42 rcuos/1 19 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/1 20 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 khelper 21 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kdevtmpfs 22 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 netns 23 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 perf 24 root 20 0 0 0 0 S 0.0 0.0 0:00.00 khungtaskd 25 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 writeback 26 root 25 5 0 0 0 S 0.0 0.0 0:00.00 ksmd 27 root 39 19 0 0 0 S 0.0 0.0 0:00.03 khugepaged 28 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 crypto 29 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kintegrityd 30 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 bioset 31 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kblockd ... ... ... The other option is $ ps aux . This will give the command used to run the process (this can sometimes be more helpful). azureuser@miccai:~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.1 35080 5100 ? Ss 18:46 0:04 /sbin/init root 2 0.0 0.0 0 0 ? S 18:46 0:00 [kthreadd] root 3 0.0 0.0 0 0 ? S 18:46 0:00 [ksoftirqd/0] root 5 0.0 0.0 0 0 ? S< 18:46 0:00 [kworker/0:0H] root 6 0.0 0.0 0 0 ? S 18:46 0:00 [kworker/u256:0] root 7 0.0 0.0 0 0 ? S 18:46 0:00 [rcu_sched] root 8 0.0 0.0 0 0 ? S 18:46 0:00 [rcu_bh] root 9 0.0 0.0 0 0 ? S 18:46 0:01 [rcuos/0] root 10 0.0 0.0 0 0 ? S 18:46 0:00 [rcuob/0] root 11 0.0 0.0 0 0 ? S 18:46 0:00 [migration/0] root 12 0.0 0.0 0 0 ? S 18:46 0:00 [watchdog/0] root 13 0.0 0.0 0 0 ? S 18:46 0:00 [watchdog/1] root 14 0.0 0.0 0 0 ? S 18:46 0:00 [migration/1] ... ... ... The advantage of using ps is that you can easily filter this list with the grep command. For example, to find a process associated with the term \"SCREEN\", you can use: azureuser@miccai:~$ ps aux | grep -i SCREEN azureus+ 1813 0.0 0.0 26104 2756 ? Ss 19:02 0:00 SCREEN -S mysql azureus+ 2058 0.0 0.0 8212 2148 pts/0 S+ 19:17 0:00 grep --color=auto -i SCREEN Thus, even when there are no \u201cvnstat\u201d related processes running, we would get one entry showing the grep process: azureuser@miccai:~$ ps aux | grep -i \"vnstat\" azureus+ 2070 0.0 0.0 8212 2212 pts/0 S+ 19:18 0:00 grep --color=auto -i vnstat Killing a process: There are various commands you can use to kill a process \u2014 kill , killall , pkill and top . We will begin from the simplest one: the killall command. killall Killing processes with the killall command: The killall command is one of the easiest ways to kill a process. If you know the exact name of a process, and you know that it\u2019s not running as another user and it is not in the Z or D states, then you can use this command directly; there\u2019s no need to manually locate the process as we described above. By default, For example, to kill a process named \u201cfirefox\u201d, run: $ killall firefox To forcibly kill the process with SIGKILL , run: $ killall -9 firefox You can also use -SIGKILL instead of -9 . If you want to kill processes interactively, you can use -i like so: $ killall -i firefox If you want to kill a process running as a different user, you can use sudo : $ sudo killall firefox You can also kill a process that has been running for a certain period of time with the -o and -y flags. So, if you want to kill a process that has been running for more than 30 minutes, use: $ killall -o 30m <process-name> If you want to kill a process that has been running for less than 30 minutes, use: $ killall -y 30m <process-name> Similarly, use the following abbreviations for the respective units of time: s seconds m minutes h hours d days w weeks M months y years pkill Killing processes with the pkill command Sometimes, you only know part of a program\u2019s name. Just like pgrep , pkill allows you to kill processes based on partial matches. For example, if you want to kill all processes containing the name apache in the name, run: pkill apache If you want to use a SIGKILL instead of a SIGTERM , use: pkill -9 apache Again, you can also use -SIGKILL instead of -9 . kill Killing processes with the kill command: Using the kill command is straightforward. Once you have found out the PID of the process that you want to kill , you can terminate it using the kill command. For example, if you want to kill a process having a PID of 1234, then use the following command: kill 1234 As we mentioned previously, the default is to use a SIGTERM . To use a SIGKILL , use -9 or -SIGKILL as we have seen before: kill -9 1234 Using top Killing processes with the top command: It is very easy to kill processes using the top command. First, search for the process that you want to kill and note the PID. Then, press k while top is running (this is case sensitive). It will prompt you to enter the PID of the process that you want to kill.","title":"Processes"},{"location":"notes/bash/processes/#processes","text":"","title":"Processes"},{"location":"notes/bash/processes/#kill-a-process-tldr","text":"$ kill <pid> Ex: $ kill 1234","title":"Kill a process (TL;DR)"},{"location":"notes/bash/processes/#by-port","text":"Source mr-khan.gitlab.io Sometimes I fed up with searching my program PID. As you know the port number so you can easily find the port PID and kill it. If you want to kill a process running on port number 8000 then first you need to find the PID and then kill it. Run the following command to find port number PID: sudo lsof -t -i:8000 then kill: sudo kill $(sudo lsof -t -i:8000) Courtesy of booleanworld.com ...more...below: Use top to see running processes. azureuser@miccai:~$ top top - 19:14:36 up 28 min, 1 user, load average: 0.00, 0.02, 0.05 Tasks: 115 total, 1 running, 114 sleeping, 0 stopped, 0 zombie %Cpu(s): 0.0 us, 0.0 sy, 0.0 ni,100.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem: 3514568 total, 715812 used, 2798756 free, 32776 buffers KiB Swap: 0 total, 0 used, 0 free. 336792 cached Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.10 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 6 root 20 0 0 0 0 S 0.0 0.0 0:00.19 kworker/u256:0 7 root 20 0 0 0 0 S 0.0 0.0 0:00.90 rcu_sched 8 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcu_bh 9 root 20 0 0 0 0 S 0.0 0.0 0:00.99 rcuos/0 10 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/0 11 root rt 0 0 0 0 S 0.0 0.0 0:00.07 migration/0 12 root rt 0 0 0 0 S 0.0 0.0 0:00.02 watchdog/0 13 root rt 0 0 0 0 S 0.0 0.0 0:00.02 watchdog/1 14 root rt 0 0 0 0 S 0.0 0.0 0:00.03 migration/1 15 root 20 0 0 0 0 S 0.0 0.0 0:00.11 ksoftirqd/1 17 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/1:0H 18 root 20 0 0 0 0 S 0.0 0.0 0:00.42 rcuos/1 19 root 20 0 0 0 0 S 0.0 0.0 0:00.00 rcuob/1 20 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 khelper 21 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kdevtmpfs 22 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 netns 23 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 perf 24 root 20 0 0 0 0 S 0.0 0.0 0:00.00 khungtaskd 25 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 writeback 26 root 25 5 0 0 0 S 0.0 0.0 0:00.00 ksmd 27 root 39 19 0 0 0 S 0.0 0.0 0:00.03 khugepaged 28 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 crypto 29 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kintegrityd 30 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 bioset 31 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kblockd ... ... ... The other option is $ ps aux . This will give the command used to run the process (this can sometimes be more helpful). azureuser@miccai:~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.1 35080 5100 ? Ss 18:46 0:04 /sbin/init root 2 0.0 0.0 0 0 ? S 18:46 0:00 [kthreadd] root 3 0.0 0.0 0 0 ? S 18:46 0:00 [ksoftirqd/0] root 5 0.0 0.0 0 0 ? S< 18:46 0:00 [kworker/0:0H] root 6 0.0 0.0 0 0 ? S 18:46 0:00 [kworker/u256:0] root 7 0.0 0.0 0 0 ? S 18:46 0:00 [rcu_sched] root 8 0.0 0.0 0 0 ? S 18:46 0:00 [rcu_bh] root 9 0.0 0.0 0 0 ? S 18:46 0:01 [rcuos/0] root 10 0.0 0.0 0 0 ? S 18:46 0:00 [rcuob/0] root 11 0.0 0.0 0 0 ? S 18:46 0:00 [migration/0] root 12 0.0 0.0 0 0 ? S 18:46 0:00 [watchdog/0] root 13 0.0 0.0 0 0 ? S 18:46 0:00 [watchdog/1] root 14 0.0 0.0 0 0 ? S 18:46 0:00 [migration/1] ... ... ... The advantage of using ps is that you can easily filter this list with the grep command. For example, to find a process associated with the term \"SCREEN\", you can use: azureuser@miccai:~$ ps aux | grep -i SCREEN azureus+ 1813 0.0 0.0 26104 2756 ? Ss 19:02 0:00 SCREEN -S mysql azureus+ 2058 0.0 0.0 8212 2148 pts/0 S+ 19:17 0:00 grep --color=auto -i SCREEN Thus, even when there are no \u201cvnstat\u201d related processes running, we would get one entry showing the grep process: azureuser@miccai:~$ ps aux | grep -i \"vnstat\" azureus+ 2070 0.0 0.0 8212 2212 pts/0 S+ 19:18 0:00 grep --color=auto -i vnstat Killing a process: There are various commands you can use to kill a process \u2014 kill , killall , pkill and top . We will begin from the simplest one: the killall command.","title":"By Port"},{"location":"notes/bash/processes/#killall","text":"Killing processes with the killall command: The killall command is one of the easiest ways to kill a process. If you know the exact name of a process, and you know that it\u2019s not running as another user and it is not in the Z or D states, then you can use this command directly; there\u2019s no need to manually locate the process as we described above. By default, For example, to kill a process named \u201cfirefox\u201d, run: $ killall firefox To forcibly kill the process with SIGKILL , run: $ killall -9 firefox You can also use -SIGKILL instead of -9 . If you want to kill processes interactively, you can use -i like so: $ killall -i firefox If you want to kill a process running as a different user, you can use sudo : $ sudo killall firefox You can also kill a process that has been running for a certain period of time with the -o and -y flags. So, if you want to kill a process that has been running for more than 30 minutes, use: $ killall -o 30m <process-name> If you want to kill a process that has been running for less than 30 minutes, use: $ killall -y 30m <process-name> Similarly, use the following abbreviations for the respective units of time: s seconds m minutes h hours d days w weeks M months y years","title":"killall"},{"location":"notes/bash/processes/#pkill","text":"Killing processes with the pkill command Sometimes, you only know part of a program\u2019s name. Just like pgrep , pkill allows you to kill processes based on partial matches. For example, if you want to kill all processes containing the name apache in the name, run: pkill apache If you want to use a SIGKILL instead of a SIGTERM , use: pkill -9 apache Again, you can also use -SIGKILL instead of -9 .","title":"pkill"},{"location":"notes/bash/processes/#kill","text":"Killing processes with the kill command: Using the kill command is straightforward. Once you have found out the PID of the process that you want to kill , you can terminate it using the kill command. For example, if you want to kill a process having a PID of 1234, then use the following command: kill 1234 As we mentioned previously, the default is to use a SIGTERM . To use a SIGKILL , use -9 or -SIGKILL as we have seen before: kill -9 1234","title":"kill"},{"location":"notes/bash/processes/#using-top","text":"Killing processes with the top command: It is very easy to kill processes using the top command. First, search for the process that you want to kill and note the PID. Then, press k while top is running (this is case sensitive). It will prompt you to enter the PID of the process that you want to kill.","title":"Using top"},{"location":"notes/bash/screen/","text":"Screen Courtesy of linuxize.com and this video by Linux Leech . Basics: C-a means Control+a and seems to be the basis of most commands. C-a ? Means to press control+a and then the ? for help about other command for screen. Start a new screen with the word screen . Name a screen session with screen \u2013S secondscreen Rename Screen: To list running screen sessions, use screen -ls $screen -ls There is a screen on: 12129.testsession (Detached) 1 Socket in /var/run/screen/S-root. Yeah, here we go!! Renaming the screen session name testsession to something else. Here is the command to rename the existing session. Note: sessionname as used below is a command so it is always necessary $ screen -S 12129.testsession -X sessionname newname C-a d is to detach. Once you detach you can see all screens with screen \u2013ls Now connect to a screen... bbearce@bbearce-XPS-15-9560:~$ screen -ls There are screens on: 530.new_screen (05/28/2019 03:27:17 PM) (Detached) 370.pts-4.bbearce-XPS-15-9560 (05/28/2019 03:24:18 PM) (Detached) 2 Sockets in /var/run/screen/S-bbearce. Connect to either with the name of the screen or the PID (prefacing numbers {530, 370}) To get rid of a screen: bbearce@bbearce-XPS-15-9560:~$ screen -X -S 370 quit The \u2013X is for sending a command to a screen and \u2013S is to identify the name of the screen to send the command. The command is quit . Now use screen \u2013ls : bbearce@bbearce-XPS-15-9560:~$ screen -ls There is a screen on: 530.new_screen (05/28/2019 03:27:17 PM) (Detached) 1 Socket in /var/run/screen/S-bbearce. The other way to kill a screen is from within it. Keep in mind this is technically for windows and not screens, but will kill a screen if there is only 1 window C-a k This will prompt you for whether or not you are sure. (y/n) Now screen \u2013ls : bbearce@bbearce-XPS-15-9560:~$ screen -ls No Sockets found in /var/run/screen/S-bbearce. Windows: Once in a screen use C-a c to create a new window. C-a n is for next C-a p is for previous C-a w is for listing windows C-a \" is for showing a menu of windows Don't forget C-a k will kill a window and eventually the screen if there is only one window. If you make 4 screens and echo 0-3 in them, we can jump to each with these commands: C-a 0 will jump to the first window with echo 0 in in it C-a 1 will jump to the first window with echo 1 in in it C-a 2 will jump to the first window with echo 2 in in it C-a 3 will jump to the first window with echo 3 in in it C-a \" will show them all and notice they are all named bash. We can rename them to be more useful. Ex: Num Name 0 bash 1 bash 2 bash 3 bash If you press C-a A we can rename our windows. Notice what happens during C-a \" now after renaming: Ex: Num Name 0 bash 1 bash 2 window-2 3 bash Panes: C-a | will split the window vertically C-a S will split the window horizontally C-a tab to change panes C-a X to exit panes C-a x to lock the terminal\\screen - you will need a password to get back in. C-a t to to get the time and load on the system Tab over to a new pane that is empty and open a window with the general window commands. C-a X will close a pane as well as performing C-a : which will bring up a prompt starting with : . At the prompt type remove and press enter. This removes the pane as well. Run Commands with Screen: Use VI to make counter.py file as such: $ vi counter.py ...write the code below import time for i in range(5): print(time.ctime(time.time())) time.sleep(1) Now we can run this program in a screen but will kill the screen when complete $ screen -d -m counter.py You can see the screen momentarily before it quits by running screen \u2013r . Also we can run this in a screen and not have it automatically quit by connecting first. Problems There is no screen to be resumed matching <screen-name> azureuser@cbibop3:~$ screen -r codalab There is a screen on: 8967.codalab (10/18/2019 06:56:52 PM) (Attached) There is no screen to be resumed matching codalab. As screen -r says, there is one screen, but it is attached. To resume it on your current terminal, you have to detach it from the other one first: screen -d -r 8967 , see manpage -d . Edit: use -d instead of -x . Edit2: @alex78191: When using -x , screen attaches to the currently running session, resulting in a \"multi-display mode\": you see the session on both terminals simultaneously, i.e., when entering a command on one terminal, it also appears on the second. However, detaching from a multi-display mode just detaches the current terminal. You hence get the message that it is still attached (on the other terminal). Resize Region gnu Find section \"resize\" under \"9.5 - Regions\" The amount of lines to add or remove can be expressed a couple of different ways. By specifying a number n by itself will resize the region by that absolute amount. You can specify a relative amount by prefixing a plus \u2018+\u2019 or minus \u2018-\u2019 to the amount, such as adding +n lines or removing -n lines. Resizing can also be expressed as an absolute or relative percentage by postfixing a percent sign \u2018%\u2019. Using zero \u20180\u2019 is a synonym for min and using an underscore \u2018_\u2019 is a synonym for max. Some examples are: resize +N increase current region by N resize -N decrease current region by N resize N set current region to N resize 20% set current region to 20% of original size resize +20% increase current region by 20% resize -b = make all windows equally resize max maximize current region resize min minimize current region Without any arguments, screen will prompt for how you would like to resize the current region.","title":"Screen"},{"location":"notes/bash/screen/#screen","text":"Courtesy of linuxize.com and this video by Linux Leech .","title":"Screen"},{"location":"notes/bash/screen/#basics","text":"C-a means Control+a and seems to be the basis of most commands. C-a ? Means to press control+a and then the ? for help about other command for screen. Start a new screen with the word screen . Name a screen session with screen \u2013S secondscreen","title":"Basics:"},{"location":"notes/bash/screen/#rename-screen","text":"To list running screen sessions, use screen -ls $screen -ls There is a screen on: 12129.testsession (Detached) 1 Socket in /var/run/screen/S-root. Yeah, here we go!! Renaming the screen session name testsession to something else. Here is the command to rename the existing session. Note: sessionname as used below is a command so it is always necessary $ screen -S 12129.testsession -X sessionname newname C-a d is to detach. Once you detach you can see all screens with screen \u2013ls Now connect to a screen... bbearce@bbearce-XPS-15-9560:~$ screen -ls There are screens on: 530.new_screen (05/28/2019 03:27:17 PM) (Detached) 370.pts-4.bbearce-XPS-15-9560 (05/28/2019 03:24:18 PM) (Detached) 2 Sockets in /var/run/screen/S-bbearce. Connect to either with the name of the screen or the PID (prefacing numbers {530, 370})","title":"Rename Screen:"},{"location":"notes/bash/screen/#to-get-rid-of-a-screen","text":"bbearce@bbearce-XPS-15-9560:~$ screen -X -S 370 quit The \u2013X is for sending a command to a screen and \u2013S is to identify the name of the screen to send the command. The command is quit . Now use screen \u2013ls : bbearce@bbearce-XPS-15-9560:~$ screen -ls There is a screen on: 530.new_screen (05/28/2019 03:27:17 PM) (Detached) 1 Socket in /var/run/screen/S-bbearce. The other way to kill a screen is from within it. Keep in mind this is technically for windows and not screens, but will kill a screen if there is only 1 window C-a k This will prompt you for whether or not you are sure. (y/n) Now screen \u2013ls : bbearce@bbearce-XPS-15-9560:~$ screen -ls No Sockets found in /var/run/screen/S-bbearce.","title":"To get rid of a screen:"},{"location":"notes/bash/screen/#windows","text":"Once in a screen use C-a c to create a new window. C-a n is for next C-a p is for previous C-a w is for listing windows C-a \" is for showing a menu of windows Don't forget C-a k will kill a window and eventually the screen if there is only one window. If you make 4 screens and echo 0-3 in them, we can jump to each with these commands: C-a 0 will jump to the first window with echo 0 in in it C-a 1 will jump to the first window with echo 1 in in it C-a 2 will jump to the first window with echo 2 in in it C-a 3 will jump to the first window with echo 3 in in it C-a \" will show them all and notice they are all named bash. We can rename them to be more useful. Ex: Num Name 0 bash 1 bash 2 bash 3 bash If you press C-a A we can rename our windows. Notice what happens during C-a \" now after renaming: Ex: Num Name 0 bash 1 bash 2 window-2 3 bash","title":"Windows:"},{"location":"notes/bash/screen/#panes","text":"C-a | will split the window vertically C-a S will split the window horizontally C-a tab to change panes C-a X to exit panes C-a x to lock the terminal\\screen - you will need a password to get back in. C-a t to to get the time and load on the system Tab over to a new pane that is empty and open a window with the general window commands. C-a X will close a pane as well as performing C-a : which will bring up a prompt starting with : . At the prompt type remove and press enter. This removes the pane as well.","title":"Panes:"},{"location":"notes/bash/screen/#run-commands-with-screen","text":"Use VI to make counter.py file as such: $ vi counter.py ...write the code below import time for i in range(5): print(time.ctime(time.time())) time.sleep(1) Now we can run this program in a screen but will kill the screen when complete $ screen -d -m counter.py You can see the screen momentarily before it quits by running screen \u2013r . Also we can run this in a screen and not have it automatically quit by connecting first.","title":"Run Commands with Screen:"},{"location":"notes/bash/screen/#problems","text":"There is no screen to be resumed matching <screen-name> azureuser@cbibop3:~$ screen -r codalab There is a screen on: 8967.codalab (10/18/2019 06:56:52 PM) (Attached) There is no screen to be resumed matching codalab. As screen -r says, there is one screen, but it is attached. To resume it on your current terminal, you have to detach it from the other one first: screen -d -r 8967 , see manpage -d . Edit: use -d instead of -x . Edit2: @alex78191: When using -x , screen attaches to the currently running session, resulting in a \"multi-display mode\": you see the session on both terminals simultaneously, i.e., when entering a command on one terminal, it also appears on the second. However, detaching from a multi-display mode just detaches the current terminal. You hence get the message that it is still attached (on the other terminal).","title":"Problems"},{"location":"notes/bash/screen/#resize-region","text":"gnu Find section \"resize\" under \"9.5 - Regions\" The amount of lines to add or remove can be expressed a couple of different ways. By specifying a number n by itself will resize the region by that absolute amount. You can specify a relative amount by prefixing a plus \u2018+\u2019 or minus \u2018-\u2019 to the amount, such as adding +n lines or removing -n lines. Resizing can also be expressed as an absolute or relative percentage by postfixing a percent sign \u2018%\u2019. Using zero \u20180\u2019 is a synonym for min and using an underscore \u2018_\u2019 is a synonym for max. Some examples are: resize +N increase current region by N resize -N decrease current region by N resize N set current region to N resize 20% set current region to 20% of original size resize +20% increase current region by 20% resize -b = make all windows equally resize max maximize current region resize min minimize current region Without any arguments, screen will prompt for how you would like to resize the current region.","title":"Resize Region"},{"location":"notes/bash/ssh/","text":"SSH Useage $ ssh <username>@<host-ip> Keys Courtesty of stackexchange Generating a Key Pair the Proper way On Local server ssh-keygen -t rsa On remote Server ssh root@remote_servers_ip \"mkdir -p .ssh\" Uploading Generated Public Keys to the Remote Server cat ~/.ssh/id_rsa.pub | ssh root@remote_servers_ip \"cat >> ~/.ssh/authorized_keys\" Set Permissions on Remote server ssh root@remote_servers_ip \"chmod 700 ~/.ssh; chmod 640 ~/.ssh/authorized_keys\" Login ssh root@remote_servers_ip Enabling SSH Protocol v2 uncomment \"Protocol 2\" in /etc/ssh/sshd_config Enabling public key authorization in sshd uncomment \"PubkeyAuthentication yes\" in /etc/ssh/sshd_config If StrictModes is set to yes in /etc/ssh/sshd_config then restorecon -Rv ~/.ssh If you get \"The program 'restorecon' is currently not installed. You can install it by typing\" sudo apt install policycoreutils ...then run that Over Two ssh Hops larkinweb ssh from systemA trough systemB into systemC: $ ssh -t user1@systemB \"ssh user2@systemC\" Mount file system under similar circumstances:","title":"SSH"},{"location":"notes/bash/ssh/#ssh","text":"","title":"SSH"},{"location":"notes/bash/ssh/#useage","text":"$ ssh <username>@<host-ip>","title":"Useage"},{"location":"notes/bash/ssh/#keys","text":"Courtesty of stackexchange Generating a Key Pair the Proper way On Local server ssh-keygen -t rsa On remote Server ssh root@remote_servers_ip \"mkdir -p .ssh\" Uploading Generated Public Keys to the Remote Server cat ~/.ssh/id_rsa.pub | ssh root@remote_servers_ip \"cat >> ~/.ssh/authorized_keys\" Set Permissions on Remote server ssh root@remote_servers_ip \"chmod 700 ~/.ssh; chmod 640 ~/.ssh/authorized_keys\" Login ssh root@remote_servers_ip Enabling SSH Protocol v2 uncomment \"Protocol 2\" in /etc/ssh/sshd_config Enabling public key authorization in sshd uncomment \"PubkeyAuthentication yes\" in /etc/ssh/sshd_config If StrictModes is set to yes in /etc/ssh/sshd_config then restorecon -Rv ~/.ssh If you get \"The program 'restorecon' is currently not installed. You can install it by typing\" sudo apt install policycoreutils ...then run that","title":"Keys"},{"location":"notes/bash/ssh/#over-two-ssh-hops","text":"larkinweb ssh from systemA trough systemB into systemC: $ ssh -t user1@systemB \"ssh user2@systemC\" Mount file system under similar circumstances:","title":"Over Two ssh Hops"},{"location":"notes/bash/startup/","text":"Run On Startup Proper instructions aren't working so I made a run.sh script that I can run from /home/bbearce #!/bin/bash ### BB - 01/11/2020 - Add startup commands here ### # Mounting via sshfs if [ \"$1\" == mount ]; then sshfs bbearce@medici-codalab-master.eastus.cloudapp.azure.com:/home/bbearce/src/MedICI/codalab-competitions /home/bbearce/mounts/medici; sshfs azureuser@cbibop3.cloudapp.net:/home/azureuser/codalab-competitions /home/bbearce/mounts/cbibop3; # Postgres and pgAdmin elif [ \"$1\" == postgres ] || [ \"$1\" == pg ]; then cd /home/bbearce/pgAdmin4/pgAdmin4; . bin/activate; python lib/python2.7/site-packages/pgadmin4/pgAdmin4.py; elif [ \"$1\" == code-journal ]; then cd /home/bbearce/Documents/code-journal; . venv/bin/activate; mkdocs serve; elif [ \"$1\" == cj-commit ]; then cd /home/bbearce/Documents/code-journal/; git add .; git commit -m \"\"$2\"\"; echo \"$2\" git push origin master; cd /home/bbearce/Documents/code-journal; . venv/bin/activate; cd /home/bbearce/Documents/bbearce.github.io; mkdocs gh-deploy --config-file ../code-journal/mkdocs.yml --remote-branch master; elif [ \"$1\" == key_github ] || [ \"$1\" == gk ]; then # add github ssh key to agent eval \"$(ssh-agent -s)\"; ssh-add ~/.ssh/github; elif [ \"$1\" == qtim-site ] || [ \"$1\" == qs ]; then cd /home/bbearce/Documents/qtim-lab.github.io; bundle exec jekyll serve; elif [ \"$1\" == qs-commit ]; then cd /home/bbearce/Documents/qtim-lab.github.io; git add .; git commit -m \"\"$2\"\"; git push origin master; elif [ \"$1\" == thegratefulbrauer ] || [ \"$1\" == tgb ]; then cd /home/bbearce/Documents/thegratefulbrauer; . venv3.6/bin/activate; python app.py; else echo \"not sure what you want...\" fi ### BB - Add startup commands here ### Courtesy of ghacks Find file /etc/rc.local . You can place scripts that you want to run at startup. /etc/rc.local #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. ### BB - 01/11/2020 - Add startup commands here ### # Mounting via sshfs sshfs bbearce@medici-codalab-master.eastus.cloudapp.azure.com:/home/bbearce/src/MedICI/codalab-competitions /home/bbearce/mounts/medici; sshfs azureuser@cbibop3.cloudapp.net:/home/azureuser/codalab-competitions /home/bbearce/mounts/cbibop3; # add github ssh key to agent eval \"$(ssh-agent -s)\"; ssh-add ~/.ssh/github; ### BB - Add startup commands here ### exit 0","title":"On Startup"},{"location":"notes/bash/startup/#run-on-startup","text":"Proper instructions aren't working so I made a run.sh script that I can run from /home/bbearce #!/bin/bash ### BB - 01/11/2020 - Add startup commands here ### # Mounting via sshfs if [ \"$1\" == mount ]; then sshfs bbearce@medici-codalab-master.eastus.cloudapp.azure.com:/home/bbearce/src/MedICI/codalab-competitions /home/bbearce/mounts/medici; sshfs azureuser@cbibop3.cloudapp.net:/home/azureuser/codalab-competitions /home/bbearce/mounts/cbibop3; # Postgres and pgAdmin elif [ \"$1\" == postgres ] || [ \"$1\" == pg ]; then cd /home/bbearce/pgAdmin4/pgAdmin4; . bin/activate; python lib/python2.7/site-packages/pgadmin4/pgAdmin4.py; elif [ \"$1\" == code-journal ]; then cd /home/bbearce/Documents/code-journal; . venv/bin/activate; mkdocs serve; elif [ \"$1\" == cj-commit ]; then cd /home/bbearce/Documents/code-journal/; git add .; git commit -m \"\"$2\"\"; echo \"$2\" git push origin master; cd /home/bbearce/Documents/code-journal; . venv/bin/activate; cd /home/bbearce/Documents/bbearce.github.io; mkdocs gh-deploy --config-file ../code-journal/mkdocs.yml --remote-branch master; elif [ \"$1\" == key_github ] || [ \"$1\" == gk ]; then # add github ssh key to agent eval \"$(ssh-agent -s)\"; ssh-add ~/.ssh/github; elif [ \"$1\" == qtim-site ] || [ \"$1\" == qs ]; then cd /home/bbearce/Documents/qtim-lab.github.io; bundle exec jekyll serve; elif [ \"$1\" == qs-commit ]; then cd /home/bbearce/Documents/qtim-lab.github.io; git add .; git commit -m \"\"$2\"\"; git push origin master; elif [ \"$1\" == thegratefulbrauer ] || [ \"$1\" == tgb ]; then cd /home/bbearce/Documents/thegratefulbrauer; . venv3.6/bin/activate; python app.py; else echo \"not sure what you want...\" fi ### BB - Add startup commands here ### Courtesy of ghacks Find file /etc/rc.local . You can place scripts that you want to run at startup. /etc/rc.local #!/bin/sh -e # # rc.local # # This script is executed at the end of each multiuser runlevel. # Make sure that the script will \"exit 0\" on success or any other # value on error. # # In order to enable or disable this script just change the execution # bits. # # By default this script does nothing. ### BB - 01/11/2020 - Add startup commands here ### # Mounting via sshfs sshfs bbearce@medici-codalab-master.eastus.cloudapp.azure.com:/home/bbearce/src/MedICI/codalab-competitions /home/bbearce/mounts/medici; sshfs azureuser@cbibop3.cloudapp.net:/home/azureuser/codalab-competitions /home/bbearce/mounts/cbibop3; # add github ssh key to agent eval \"$(ssh-agent -s)\"; ssh-add ~/.ssh/github; ### BB - Add startup commands here ### exit 0","title":"Run On Startup"},{"location":"notes/bash/users/","text":"Users Courtesy of cyberciti.biz Introduction : By default, the cloud server comes with a user named ubuntu . You can use such primary user account for sysadmin tasks on Ubuntu. However, sometimes you need to add a user account on Ubuntu for additional sysadmin tasks. This page shows how to create a regular user account or sysadmin account on the Ubuntu server. Add User Steps to create a user account on Ubuntu Linux 1. Open the terminal application 2. Log in to remote box by running the ssh user@your-ubuntu-box-ip 3. To add a new user in Ubuntu run sudo adduser userNameHere 4. Enter password and other needed info to create a user account on Ubuntu server 5. New username would be added to /etc/passwd file, and encrypted password stored in the /etc/shadow file Example bbearce@bbearce-XPS-15-9560:~$ sudo adduser vivek [sudo] password for bbearce: Adding user `vivek' ... Adding new group `vivek' (1001) ... Adding new user `vivek' (1001) with group `vivek' ... Creating home directory `/home/vivek' ... Copying files from `/etc/skel' ... Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully Changing the user information for vivek Enter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] y Verification Use the grep command or cat command as follows: $ cat /etc/passwd . . . vivek:x:1001:1001:,,,:/home/vivek:/bin/bash or $ grep '^vivek' /etc/passwd vivek:x:1001:1001:,,,:/home/vivek:/bin/bash Check group, groupid and userid of user: Use id : $ id <user> Ex: bbearce@bbearce-XPS-15-9560:~$ id bbearce uid=1000(bbearce) gid=1000(bbearce) groups=1000(bbearce),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),113(lpadmin),128(sambashare),130(docker),999(microk8s) Delete User Remove the user Log in to your server via SSH. Switch to the root user: $ sudo su - Use the userdel command to remove the old user: $ userdel <user's username> Optional: You can also delete that user's home directory and mail spool by using the -r flag with the command: $ userdel -r user's username Warning: Only delete a user's home directory if you are certain you no longer need their files or mail. Change Password Source: cyberciti Type following passwd command to change your own password: $ passwd Sample Outputs: Changing password for vivek (current) UNIX password: Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully Add User to Sudo Source phoenixnap usermod \u2013aG sudo newuser","title":"Users"},{"location":"notes/bash/users/#users","text":"Courtesy of cyberciti.biz Introduction : By default, the cloud server comes with a user named ubuntu . You can use such primary user account for sysadmin tasks on Ubuntu. However, sometimes you need to add a user account on Ubuntu for additional sysadmin tasks. This page shows how to create a regular user account or sysadmin account on the Ubuntu server.","title":"Users"},{"location":"notes/bash/users/#add-user","text":"Steps to create a user account on Ubuntu Linux 1. Open the terminal application 2. Log in to remote box by running the ssh user@your-ubuntu-box-ip 3. To add a new user in Ubuntu run sudo adduser userNameHere 4. Enter password and other needed info to create a user account on Ubuntu server 5. New username would be added to /etc/passwd file, and encrypted password stored in the /etc/shadow file","title":"Add User"},{"location":"notes/bash/users/#example","text":"bbearce@bbearce-XPS-15-9560:~$ sudo adduser vivek [sudo] password for bbearce: Adding user `vivek' ... Adding new group `vivek' (1001) ... Adding new user `vivek' (1001) with group `vivek' ... Creating home directory `/home/vivek' ... Copying files from `/etc/skel' ... Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully Changing the user information for vivek Enter the new value, or press ENTER for the default Full Name []: Room Number []: Work Phone []: Home Phone []: Other []: Is the information correct? [Y/n] y","title":"Example"},{"location":"notes/bash/users/#verification","text":"Use the grep command or cat command as follows: $ cat /etc/passwd . . . vivek:x:1001:1001:,,,:/home/vivek:/bin/bash or $ grep '^vivek' /etc/passwd vivek:x:1001:1001:,,,:/home/vivek:/bin/bash","title":"Verification"},{"location":"notes/bash/users/#check-group-groupid-and-userid-of-user","text":"Use id : $ id <user> Ex: bbearce@bbearce-XPS-15-9560:~$ id bbearce uid=1000(bbearce) gid=1000(bbearce) groups=1000(bbearce),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),113(lpadmin),128(sambashare),130(docker),999(microk8s)","title":"Check group, groupid and userid of user:"},{"location":"notes/bash/users/#delete-user","text":"","title":"Delete User"},{"location":"notes/bash/users/#remove-the-user","text":"Log in to your server via SSH. Switch to the root user: $ sudo su - Use the userdel command to remove the old user: $ userdel <user's username> Optional: You can also delete that user's home directory and mail spool by using the -r flag with the command: $ userdel -r user's username Warning: Only delete a user's home directory if you are certain you no longer need their files or mail.","title":"Remove the user"},{"location":"notes/bash/users/#change-password","text":"Source: cyberciti Type following passwd command to change your own password: $ passwd Sample Outputs: Changing password for vivek (current) UNIX password: Enter new UNIX password: Retype new UNIX password: passwd: password updated successfully","title":"Change Password"},{"location":"notes/bash/users/#add-user-to-sudo","text":"Source phoenixnap usermod \u2013aG sudo newuser","title":"Add User to Sudo"},{"location":"notes/bash/vi/","text":"VI Find and Replace Source linfo vi also has powerful search and replace capabilities. To search the text of an open file for a specific string (combination of characters or words), in the command mode type a colon (:), \"s,\" forward slash (/) and the search string itself. What you type will appear on the bottom line of the display screen. Finally, press ENTER, and the matching area of the text will be highlighted, if it exists. If the matching string is on an area of text that is not currently displayed on the screen, the text will scroll to show that area. The formal syntax for searching is: :s/string For example, suppose you want to search some text for the string \"cherry.\" Type the following and press ENTER: :s/cherry The first match for \"cherry\" in your text will then be highlighted. To see if there are additional occurrences of the same string in the text, type n, and the highlight will switch to the next match, if one exists. The syntax for replacing one string with another string in the current line is :s/pattern/replace/ Here \"pattern\" represents the old string and \"replace\" represents the new string. For example, to replace each occurrence of the word \"lemon\" in a line with \"orange,\" type: :s/lemon/orange/ The syntax for replacing every occurrence of a string in the entire text is similar. The only difference is the addition of a \"%\" in front of the \"s\": :%s/pattern/replace/ Thus repeating the previous example for the entire text instead of just for a single line would be: :%s/lemon/orange/ Next: Working With Multiple Files","title":"VI"},{"location":"notes/bash/vi/#vi","text":"","title":"VI"},{"location":"notes/bash/vi/#find-and-replace","text":"Source linfo vi also has powerful search and replace capabilities. To search the text of an open file for a specific string (combination of characters or words), in the command mode type a colon (:), \"s,\" forward slash (/) and the search string itself. What you type will appear on the bottom line of the display screen. Finally, press ENTER, and the matching area of the text will be highlighted, if it exists. If the matching string is on an area of text that is not currently displayed on the screen, the text will scroll to show that area. The formal syntax for searching is: :s/string For example, suppose you want to search some text for the string \"cherry.\" Type the following and press ENTER: :s/cherry The first match for \"cherry\" in your text will then be highlighted. To see if there are additional occurrences of the same string in the text, type n, and the highlight will switch to the next match, if one exists. The syntax for replacing one string with another string in the current line is :s/pattern/replace/ Here \"pattern\" represents the old string and \"replace\" represents the new string. For example, to replace each occurrence of the word \"lemon\" in a line with \"orange,\" type: :s/lemon/orange/ The syntax for replacing every occurrence of a string in the entire text is similar. The only difference is the addition of a \"%\" in front of the \"s\": :%s/pattern/replace/ Thus repeating the previous example for the entire text instead of just for a single line would be: :%s/lemon/orange/ Next: Working With Multiple Files","title":"Find and Replace"},{"location":"notes/bash/zip_and_unzip/","text":"Zipping and Unzipping zip, unzip Courtesty of geeksforgeeks.org zip $ man zip NAME zip - package and compress (archive) files . . . zipping files To zip files into a .zip file run this command: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip wheel.py something.txt adding: wheel.py (deflated 79%) adding: something.txt (stored 0%) bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip something.txt wheel.py zipping folder contents To zip eveything in a directory into zip file use ./* This is useful for pointing to <some directory>/* bbearce@bbearce-XPS-15-9560:~/Desktop$ ls something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip ./* adding: something.txt (stored 0%) adding: wheel.py (deflated 79%) bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip something.txt wheel.py Folders will be picked up but not their contents without -r : bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory/ test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip ./* adding: directory/ (stored 0%) adding: something.txt (stored 0%) adding: wheel.py (deflated 79%) bbearce@bbearce-XPS-15-9560:~/Desktop$ zip -r example.zip ./* updating: directory/ (stored 0%) updating: something.txt (stored 0%) updating: wheel.py (deflated 79%) adding: directory/test.txt (stored 0%) Notice how zipping to example.zip again updates pre-existing files and adds new ones. Also flag -j can be used to make sure a folder's contents only get zipped. By default if you include a folder name, the folder is included in the zip file. Using -j will explicitly exclude the folder. unzip $ man unzip NAME unzip - list, test and extract compressed files in a ZIP archive . . . To unzip simply use the unzip command: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip Archive: example.zip creating: directory/ extracting: something.txt inflating: wheel.py extracting: directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory example.zip something.txt wheel.py We can unzip to a specific directory with the -d flag: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip -d test Archive: example.zip creating: test/directory/ extracting: test/something.txt inflating: test/wheel.py extracting: test/directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test directory something.txt wheel.py If the directory already exists, we can dump the contents into it: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ mkdir test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip -d test Archive: example.zip creating: test/directory/ extracting: test/something.txt inflating: test/wheel.py extracting: test/directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test directory something.txt wheel.py Tar Files .tar The tar command is used to create tar archives by converting a group of files into an archive. It supports a vast range of compression programs such as gzip , bzip2 , lzip , lzma , lzop , xz and compress . Tar was originally designed for creating archives to store files on magnetic tape which is why it has its name \u201c T ape AR chive\u201d. Create a .tar bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -cvf test.tar test.txt test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt Here -cvf means c: create; v: verbose; f: use archive file or device (we can and did specify a filename to use) . Keep in mind we didn't use the -z flag which would make a gzip . .tar.gz .tar.gz files are just files that have been zipped with -z to make a gzip. Let's redo the above example with -z : bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -cvzf test.tar test.txt test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt Note that the archive name test.tar is a .tar.gz but you have to specify that explicitly. So a canonical way of diong this is: tar -cvzf test.tar.gz test.txt . Also the [-] is optional, so tar cvzf test.tar.gz test.txt would work too. We can see that the .tar.gz is smaller with du bbearce@bbearce-XPS-15-9560:~/Desktop$ du -BK ./* 20K ./test.tar 4K ./test.tar.gz 12K ./test.txt Unzip a .tar We need the -x option: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -xvf test.tar test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt When downloading 3DSlicer you get a *.tar.gz file. This command installs it. tar zxvf Slicer-4.8.1-linux-amd64.tar.gz FYI: ( Link )","title":"Zip and Unzip"},{"location":"notes/bash/zip_and_unzip/#zipping-and-unzipping","text":"","title":"Zipping and Unzipping"},{"location":"notes/bash/zip_and_unzip/#zip-unzip","text":"Courtesty of geeksforgeeks.org","title":"zip, unzip"},{"location":"notes/bash/zip_and_unzip/#zip","text":"$ man zip NAME zip - package and compress (archive) files . . .","title":"zip"},{"location":"notes/bash/zip_and_unzip/#zipping-files","text":"To zip files into a .zip file run this command: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip wheel.py something.txt adding: wheel.py (deflated 79%) adding: something.txt (stored 0%) bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip something.txt wheel.py","title":"zipping files"},{"location":"notes/bash/zip_and_unzip/#zipping-folder-contents","text":"To zip eveything in a directory into zip file use ./* This is useful for pointing to <some directory>/* bbearce@bbearce-XPS-15-9560:~/Desktop$ ls something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip ./* adding: something.txt (stored 0%) adding: wheel.py (deflated 79%) bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip something.txt wheel.py Folders will be picked up but not their contents without -r : bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory something.txt wheel.py bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory/ test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ zip example.zip ./* adding: directory/ (stored 0%) adding: something.txt (stored 0%) adding: wheel.py (deflated 79%) bbearce@bbearce-XPS-15-9560:~/Desktop$ zip -r example.zip ./* updating: directory/ (stored 0%) updating: something.txt (stored 0%) updating: wheel.py (deflated 79%) adding: directory/test.txt (stored 0%) Notice how zipping to example.zip again updates pre-existing files and adds new ones. Also flag -j can be used to make sure a folder's contents only get zipped. By default if you include a folder name, the folder is included in the zip file. Using -j will explicitly exclude the folder.","title":"zipping folder contents"},{"location":"notes/bash/zip_and_unzip/#unzip","text":"$ man unzip NAME unzip - list, test and extract compressed files in a ZIP archive . . . To unzip simply use the unzip command: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip Archive: example.zip creating: directory/ extracting: something.txt inflating: wheel.py extracting: directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls directory example.zip something.txt wheel.py We can unzip to a specific directory with the -d flag: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip -d test Archive: example.zip creating: test/directory/ extracting: test/something.txt inflating: test/wheel.py extracting: test/directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test directory something.txt wheel.py If the directory already exists, we can dump the contents into it: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip bbearce@bbearce-XPS-15-9560:~/Desktop$ mkdir test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ unzip example.zip -d test Archive: example.zip creating: test/directory/ extracting: test/something.txt inflating: test/wheel.py extracting: test/directory/test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls example.zip test bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test directory something.txt wheel.py","title":"unzip"},{"location":"notes/bash/zip_and_unzip/#tar-files","text":"","title":"Tar Files"},{"location":"notes/bash/zip_and_unzip/#tar","text":"The tar command is used to create tar archives by converting a group of files into an archive. It supports a vast range of compression programs such as gzip , bzip2 , lzip , lzma , lzop , xz and compress . Tar was originally designed for creating archives to store files on magnetic tape which is why it has its name \u201c T ape AR chive\u201d.","title":".tar"},{"location":"notes/bash/zip_and_unzip/#create-a-tar","text":"bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -cvf test.tar test.txt test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt Here -cvf means c: create; v: verbose; f: use archive file or device (we can and did specify a filename to use) . Keep in mind we didn't use the -z flag which would make a gzip .","title":"Create a .tar"},{"location":"notes/bash/zip_and_unzip/#targz","text":".tar.gz files are just files that have been zipped with -z to make a gzip. Let's redo the above example with -z : bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -cvzf test.tar test.txt test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt Note that the archive name test.tar is a .tar.gz but you have to specify that explicitly. So a canonical way of diong this is: tar -cvzf test.tar.gz test.txt . Also the [-] is optional, so tar cvzf test.tar.gz test.txt would work too. We can see that the .tar.gz is smaller with du bbearce@bbearce-XPS-15-9560:~/Desktop$ du -BK ./* 20K ./test.tar 4K ./test.tar.gz 12K ./test.txt","title":".tar.gz"},{"location":"notes/bash/zip_and_unzip/#unzip-a-tar","text":"We need the -x option: bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar bbearce@bbearce-XPS-15-9560:~/Desktop$ tar -xvf test.tar test.txt bbearce@bbearce-XPS-15-9560:~/Desktop$ ls test.tar test.txt When downloading 3DSlicer you get a *.tar.gz file. This command installs it. tar zxvf Slicer-4.8.1-linux-amd64.tar.gz FYI: ( Link )","title":"Unzip a .tar"},{"location":"notes/codalab/docker_submission/","text":"Docker Submission This is a documentation for adding docker submissions to the open source project CodaLab . Introduction Codalab comes with essentially 2 types of submissions from participants: Results Submission: Participants submit the \"results\" of their algorithms. Code Submission: Participants submit actual code to be executed by the challenge organizer. The code can be executed directly as an executable ($ python sample.py...) or participants can define classes that the organizer calls themselves (hence the participants do not have to read the input data themselves). We will be adding a third option where participants can submit a docker image with their code and libraries baked in. Step 1: Install Codalab First we need a running instance installed. Setup the challenge to accept a results submission, as we will tweak this process to evaluate the output of a docker container. Step 2: Add Database Option for Docker Submission Add New Column in Competition Table Find file /codalab/apps/web/models.py . Approximately on line 233 is the Competition model definition. It has definitions for the columns of this table. It looks like this: class Competition(ChaHubSaveMixin, models.Model): ... Add enable_docker_submission as a BooleanField : Add this line: enable_docker_submission = models.BooleanField(default=False, verbose_name=\"Allow participant docker submission\") It doesn't matter what order the columns are in, so if you are familiar with setting up models in Django or Flask projects, just add a column that we can reference later. Migrate Database So We Can See Our Changes Codalab as of this writing (5/1/2020) is deployed with an orchestration of docker containers. One of these is the database (postgres). Using docker ps we can see our images: bbearce@MedICI-CodaLab-Master:~/src$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b8196a2e9fc5 nginx \"bash -x /app/docker\u2026\" 4 months ago Up 2 months 0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp nginx 528d2dc976a1 codalab-competitions_django \"bash /app/docker/ru\u2026\" 4 months ago Up 2 months 0.0.0.0:8000->8000/tcp django c56dcc7fb3e2 codalab-competitions_worker_site \"sh /app/docker/run_\u2026\" 4 months ago Up 2 months worker_site 23996bbcfbd0 codalab/competitions-v1-compute-worker:1.1.7 \"/bin/sh -c 'celery \u2026\" 4 months ago Up 2 months worker_compute 3f5a1df0f9ae codalab-competitions_flower \"/usr/bin/dumb-init \u2026\" 4 months ago Up 2 months 0.0.0.0:5555->5555/tcp flower b93b3f7e0a3b codalab-competitions_rabbit \"docker-entrypoint.s\u2026\" 4 months ago Up 2 months 4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 15671/tcp, 25672/tcp, 0.0.0.0:15672->15672/tcp rabbit 5e3492959db1 postgres:9.6.3 \"docker-entrypoint.s\u2026\" 4 months ago Up 2 months 5432/tcp postgres We need use manage.py from the django container to migrate the database. This will talk to the postgres container for us. Codalab has notes for how to do this so we will follow their lead : Note, in the link from codalab they have a placeholder for APPNAME . Substitute web in for this. root@django:/app/codalab# ./manage schemamigration web --auto ./manage: line 3: ./config/generated/startup_env.sh: No such file or directory ./manage: line 4: ../venv/bin/activate: No such file or directory /usr/local/lib/python2.7/site-packages/django_extensions/db/fields/__init__.py:425: DeprecationWarning: Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported. warnings.warn(\"Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported.\", DeprecationWarning) + Added field enable_docker_submission on web.Competition Created 0085_auto__add_field_competition_enable_docker_submission.py. You can now apply this migration with: ./manage.py migrate web Once you have that migrate the database. ./manage migrate This will apply the migration and look for all changes to all tables. Looking in the results we can see our migration in action: ~ ~ Running migrations for web: - Migrating forwards to 0085_auto__add_field_competition_enable_docker_submission. > web:0085_auto__add_field_competition_enable_docker_submission ~ ~ Now let's verify our change. First create a database superuser: docker exec -it django python manage.py createsuperuser This will prompt you to create a superuser: bbearce@miccai2019:~/src/codalab-competitions/codalab$ docker exec -it django python manage.py createsuperuser /usr/local/lib/python2.7/site-packages/django_extensions/db/fields/__init__.py:425: DeprecationWarning: Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported. warnings.warn(\"Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported.\", DeprecationWarning) Username: admin Email address: bbearce@mgh.harvard.edu Password: Password (again): Superuser created successfully. Now proceed to http://<your domain>.com/admin and log in with the credentials you made. Find the \"Web\" app section and click \"Competitions\". Next scroll till you see \"Allow participant docker submission\" or whatever you put as your new column. Add Our new Option to the CompetitionForm Class Django or CodaLab uses apps.web.forms.CompetitionForm to manage the edit competiton form. class CompetitionForm(forms.ModelForm): class Meta: model = models.Competition fields = ( 'title', 'description', 'queue', 'disallow_leaderboard_modifying', 'force_submission_to_leaderboard', 'image', 'end_date', 'published', 'enable_medical_image_viewer', 'enable_detailed_results', # change start 'enable_docker_submission', # change end 'admins', ... This will allow us to see this option in the edit competition view: This has the effect of allowing access to this option from multiple views so we can conditionally render a docker submission form or a regular result submission form. Step 3: Add Conditional Rendering for Docker\\Result Submission Add UI Elements Find template apps/web/templates/web/competitions/_submit_results_page.html . Roughly on line 70 there is a {% if USE_AWS %} block and in the {% else %} block we can add our code. Replace the inside of the else block with this code: {% if phase.competition.enable_docker_submission %} <h1>DOCKER SUBMISSION ENABLED </h1> <!-- Submit Docker Button --> <button id=submit-button type=\"button\" data-toggle=\"modal\" data-target=\"#submit-docker-dialog\" class=\"btn btn-secondary\">Submit Docker</button> <!-- Submit Docker Button --> {% else %} <h1>DOCKER SUBMISSION DISABLED</h1> <!-- original code from codalab --> <button id=\"fileUploadButton\" class=\"button btn btn-primary {% if not phase.reference_data %}disabled{% endif %}\" {% if not phase.reference_data %}disabled=\"disabled\"{% endif %}> Submit </button> <!-- original code from codalab --> {% endif %} Our If Else block has the original code in the else. At this point if you refresh the page and navigate to the \"Participate\" tab, you should see this. Now if we go to \"Edit\" and look at the competition options, we see that if we check the box \"Allow participant docker submission\" we can see the text change on the \"Participate\" tab. Next we need the \"Submit Docker\" button to do something. Let's add a modal that get's triggered when you click the button. Add this code just before the line that has {% include \"web/common/_submission_details_template.html\" %} on line ~167: <div class=\"modal fade\" id=\"submit-docker-dialog\"> <div class=\"modal-dialog\"> <div class=\"modal-content\"> <div class=\"modal-header\"> <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\">&times;</button> <h4 class=\"modal-title\">Submit Docker</h4> </div> <div class=\"modal-body\"> <h4>Submit Docker Run Command</h4> <textarea id=\"docker-run-command\" type=text style=\"width: 500px; height: 250px;\">docker run -v /<local-path>:/<docker-container-path-1> -v /<local-path>:/<docker-container-path-2> user/repo:tag</textarea> </div> <div class=\"modal-footer\"> <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button> <button id=\"submit-docker\" class=\"btn btn-primary\">Send</button> </div> </div> </div> </div> You should see this when you click the \"Submit Docker\" button: Now let's tie the \"Send\" button from the modal to some javascript so we can call the backend to process this for us. At the bottom of the same file we've been working in, add this code and create the file apps/web/static/js/submit_docker.js: <!-- Docker Submission Front End --> <script type=\"text/javascript\" src=\"{{STATIC_URL}}js/submit_docker.js\"></script> <!-- Docker Submission Front End --> In submit_docker.js add this: $('#submit-docker').click(function(){ //copy from codalab/apps/web/static/js/Competition.js file $('#details').html('Creating new submission...'); var competitionId = $(\"#competitionId\").val() var description = $('#submission_description_textarea').val() || ''; var method_name = $('#submission_method_name').val() || ''; var method_description = $('#submission_method_description').val() || ''; var project_url = $('#submission_project_url').val() || ''; var publication_url = $('#submission_publication_url').val() || ''; var bibtex = $('#submission_bibtex').val() || ''; var team_name = $('#submission_team_name').val() || ''; var organization_or_affiliation = $('#submission_organization_or_affiliation').val() || ''; var phase_id = $('#submission_phase_id').val(); // capture docker run command var docker_run_cmd = $('#docker-run-command').val(); console.log(docker_run_cmd); // capture docker run command $('#submission_description_textarea').val(''); // console.log('/api/competition/' + competitionId + '/submission?description=' + encodeURIComponent(description) + // '&method_name=' + encodeURIComponent(method_name) + // '&method_description=' + encodeURIComponent(method_description) + // '&project_url=' + encodeURIComponent(project_url) + // '&publication_url=' + encodeURIComponent(publication_url) + // '&bibtex=' + encodeURIComponent(bibtex) + // '&team_name=' + encodeURIComponent(team_name) + // '&organization_or_affiliation=' + encodeURIComponent(organization_or_affiliation) + // '&phase_id=' + encodeURIComponent(phase_id)+ // '&docker-run-command='+ encodeURIComponent(docker_run_cmd) // ) $.ajax({ url: '/api/competition/' + competitionId + '/submission?description=' + encodeURIComponent(description) + '&method_name=' + encodeURIComponent(method_name) + '&method_description=' + encodeURIComponent(method_description) + '&project_url=' + encodeURIComponent(project_url) + '&publication_url=' + encodeURIComponent(publication_url) + '&bibtex=' + encodeURIComponent(bibtex) + '&team_name=' + encodeURIComponent(team_name) + '&organization_or_affiliation=' + encodeURIComponent(organization_or_affiliation) + '&phase_id=' + encodeURIComponent(phase_id)+ '&docker-run-command='+ encodeURIComponent(docker_run_cmd), type: 'post', cache: false, async: false, data: { 'id': 'trackingid', 'name': '', 'type': '', 'size': '' } }).done(function(response) { $('#details').html(''); $('#user_results tr.noData').remove(); $('#user_results').append(Competition.displayNewSubmission(response, description, method_name, method_description, project_url, publication_url, bibtex, team_name, organization_or_affiliation)); $('#user_results #' + response.id + ' .glyphicon-plus').on('click', function() { Competition.showOrHideSubmissionDetails(this) }); //$('#fileUploadButton').removeClass('disabled'); //$('#fileUploadButton').text(\"Submit Results...\"); $('#user_results #' + response.id + ' .glyphicon-plus').click(); location.reload(true); }).fail(function(jqXHR) { var msg = 'An unexpected error occurred.'; if (jqXHR.status == 403) { msg = jqXHR.responseJSON.detail; } $('#details').html(msg); //$('#fileUploadButton').text(\"Submit Results...\"); $('#fileUploadButton').removeClass('disabled'); }); $('#submit-docker-dialog').modal('toggle'); }); From looking at the ajax call you can see we are hitting the route for an api that ultimately triggers a POST request in view \"CompetitionSubmissionViewSet\" under the api app. The part we need to edit is the post_save() method. Step 4: Edit Site-Worker Code Replace the post_save() with this: def post_save(self, obj, created): # Original Code #if created: # evaluate_submission.apply_async((obj.pk, obj.phase.is_scoring_only)) if escape(self.request.QUERY_PARAMS.get('docker-run-command')) == 'None': # submit_docker_command = None submit_docker_command = \"this is not a docker submission\" else: submit_docker_command = escape(self.request.QUERY_PARAMS.get('docker-run-command', \"\")).replace('&lt;','<').replace('&gt;','>') ## - pass extra info to submission if created: evaluate_submission.apply_async((obj.pk, obj.phase.is_scoring_only, submit_docker_command)) ## - pass extra info to submission At the bottom of post_save() is evaluate_submission . That is from apps/web/tasks.py. We are passing the submit_docker_command to evaluate submission . Change evaluate_submission to accept the submit_docker_command . Also edit it to pass the command to task_func() Add submit_docker_command to these two sections: def evaluate_submission(submission_id, is_scoring_only, submit_docker_command): and: task_func(submission, job_id, submit_docker_command) task_func is actually a user defined variable for score , a function in the same file above evaluate_submission . Add submit_docker_command to the input of score . def score(submission, job_id, submit_docker_command): Towards the bottom of score , _prepare_compute_worker_run needs submit_docker_command . Pass it in, and subsequently add it to it's function definition at towards the top of the page: _prepare_compute_worker_run(job_id, submission, submit_docker_command, is_prediction=False) and def _prepare_compute_worker_run(job_id, submission, submit_docker_command, is_prediction): Inside this function you will find a data variable we need to add the docker command to: data = { \"id\": job_id, \"task_type\": \"run\", \"task_args\": { \"submission_id\": submission.pk, \"docker_image\": docker_image, \"ingestion_program_docker_image\": docker_image, \"bundle_url\": _make_url_sassy(bundle_url), \"stdout_url\": _make_url_sassy(stdout, permission='w'), \"stderr_url\": _make_url_sassy(stderr, permission='w'), \"output_url\": _make_url_sassy(output, permission='w'), \"ingestion_program_output_url\": _make_url_sassy(submission.ingestion_program_stdout_file.name, permission='w'), \"ingestion_program_stderr_url\": _make_url_sassy(submission.ingestion_program_stderr_file.name, permission='w'), \"detailed_results_url\": _make_url_sassy(submission.detailed_results_file.name, permission='w'), \"private_output_url\": _make_url_sassy(submission.private_output_file.name, permission='w'), \"secret\": submission.secret, \"execution_time_limit\": submission.phase.execution_time_limit, \"predict\": is_prediction, 'submit_docker_command':submit_docker_command, ## - Added submit_docker_command here } At this point the code hands off the data to the Compute-Worker with compute_worker_run(data, soft_time_limit=time_limit, priority=2) . Step 5: The Compute-Worker We need to enter the compute-worker docker container and edit \"worker.py\". On the machine where the compute-worker is running execute: $ docker exec -it codalab/competitions-v1-compute-worker:latest ...or use the compute image that you need. Once inside you should be able to see \"worker.py\". Open it. Find function run(task_id, task_args) . This is where we will add our code on approx line 500. Add this: if task_args['submit_docker_command'] != 'this is not a docker submission': print('@CUSTOM DOCKER START@') participant_docker_cmd = task_args['submit_docker_command'].format(input_dir+\"/res\") logger.info(\"Invoking program %s\", participant_docker_cmd) participant_docker_process = Popen(participant_docker_cmd.split(\" \")) participant_docker_process.wait() # This halts other actions till this run isfinished. print('@CUSTOM DOCKER END@') This is the important line: participant_docker_cmd = task_args['submit_docker_command'].format(input_dir+\"/res\") This line substitutes the results directory into a mount placeholder so that the container can put the results from the run into it. Then the compute-worker can consume the results. Update (7/17/2020): Docker Image Submission only and resulting handling: participant_docker_submission_cmd = [ 'docker', 'run', # Ask all participants to add this user '-u', 'participant', # Cut internet '--net', 'none', # Remove it after run '--rm', # Add support for GPUs and nvidia '--gpus', 'all', # Give it a name associated to task_id '--name={}'.format(\"participant_docker_submission_taskid_\"+str(task_id)), # Try the new timeout feature '--stop-timeout={}'.format(execution_time_limit), # Don't allow subprocesses to raise privileges '--security-opt=no-new-privileges', # Set the right volume '-v', '{0}:/mnt/in:ro'.format('/home/bbearce/Documents/docker_submissions/directory_of_files'), # :ro for read-only file system '-v', '{0}:/mnt/out'.format(input_dir+\"/res\"), # Set aside 512m memory for the host #'--memory', '{}MB'.format(available_memory_mib - 512), # Don't buffer python output, so we don't lose any #'-e', 'PYTHONUNBUFFERED=1', # Set current working directory #'-w', run_dir, # Note that hidden data dir is excluded here! # Set the right image task_args['submit_docker_command'], ] if task_args['submit_docker_command'] != 'this is not a docker submission': print('@CUSTOM DOCKER START@') logger.info(\"Invoking participant docker submission: %s\", participant_docker_submission_cmd) participant_docker_process = Popen(participant_docker_submission_cmd) participant_docker_process.wait() # This halts other actions till this run isfinished. print('@CUSTOM DOCKER END@') Below I will discuss some of the important options... Note, we need to mount two folders ( datain:/mnt/in , dataout:/mnt/out ). We don't need to change anything about datain because we can use :ro to make it read-only inside the docker container like so /mnt/in:ro . This keeps users from tampering with input data. dataout needs to be owned by user \"participant\". We do this like so... $ chown participant:participant <path to>/dataout and make it writable as well: $ chmod 0777 <path to>/dataout If participant is the user that will be using these mounts then we need the container to run as participant . Notice we run the docker with -u participant , but how do we get that user in the docker ahead of time. In the Dockerfile for a submission add this to the top: ARG USER=participant ARG UID=1000 ARG GID=1000 # default password for user ARG PW=participant # Option1: Using unencrypted password/ specifying password RUN useradd -m ${USER} --uid=${UID} && echo \"${USER}:${PW}\" | \\ chpasswd This should add user participant with password participant . Finally we need to make sure the container has no internet. We add flag --net none to cut internet to the docker rendering it unable to transfer data. Lastly if this is an nvidia compute worker codalab/competitions-v1-nvidia-worker:latest then we have a few extra things ot take care of. On lines 274-275 there are two variables that don't exist being referenced. max_execution_time_limit = task_args['max_execution_time_limit'] previous_execution_time = task_args['previous_execution_time'] to #max_execution_time_limit = task_args['max_execution_time_limit'] # -BB- #previous_execution_time = task_args['previous_execution_time'] # -BB- and at the bottom of the file (~ line 700): signal.alarm(int(math.fabs(math.ceil(max_execution_time_limit - time_difference - previous_execution_time)))) # Total Execution to #signal.alarm(int(math.fabs(math.ceil(max_execution_time_limit - time_difference - previous_execution_time)))) # Total Execution -BB-","title":"Docker Submission"},{"location":"notes/codalab/docker_submission/#docker-submission","text":"This is a documentation for adding docker submissions to the open source project CodaLab .","title":"Docker Submission"},{"location":"notes/codalab/docker_submission/#introduction","text":"Codalab comes with essentially 2 types of submissions from participants: Results Submission: Participants submit the \"results\" of their algorithms. Code Submission: Participants submit actual code to be executed by the challenge organizer. The code can be executed directly as an executable ($ python sample.py...) or participants can define classes that the organizer calls themselves (hence the participants do not have to read the input data themselves). We will be adding a third option where participants can submit a docker image with their code and libraries baked in.","title":"Introduction"},{"location":"notes/codalab/docker_submission/#step-1-install-codalab","text":"First we need a running instance installed. Setup the challenge to accept a results submission, as we will tweak this process to evaluate the output of a docker container.","title":"Step 1: Install Codalab"},{"location":"notes/codalab/docker_submission/#step-2-add-database-option-for-docker-submission","text":"","title":"Step 2: Add Database Option for Docker Submission"},{"location":"notes/codalab/docker_submission/#add-new-column-in-competition-table","text":"Find file /codalab/apps/web/models.py . Approximately on line 233 is the Competition model definition. It has definitions for the columns of this table. It looks like this: class Competition(ChaHubSaveMixin, models.Model): ... Add enable_docker_submission as a BooleanField : Add this line: enable_docker_submission = models.BooleanField(default=False, verbose_name=\"Allow participant docker submission\") It doesn't matter what order the columns are in, so if you are familiar with setting up models in Django or Flask projects, just add a column that we can reference later.","title":"Add New Column in Competition Table"},{"location":"notes/codalab/docker_submission/#migrate-database-so-we-can-see-our-changes","text":"Codalab as of this writing (5/1/2020) is deployed with an orchestration of docker containers. One of these is the database (postgres). Using docker ps we can see our images: bbearce@MedICI-CodaLab-Master:~/src$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES b8196a2e9fc5 nginx \"bash -x /app/docker\u2026\" 4 months ago Up 2 months 0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp nginx 528d2dc976a1 codalab-competitions_django \"bash /app/docker/ru\u2026\" 4 months ago Up 2 months 0.0.0.0:8000->8000/tcp django c56dcc7fb3e2 codalab-competitions_worker_site \"sh /app/docker/run_\u2026\" 4 months ago Up 2 months worker_site 23996bbcfbd0 codalab/competitions-v1-compute-worker:1.1.7 \"/bin/sh -c 'celery \u2026\" 4 months ago Up 2 months worker_compute 3f5a1df0f9ae codalab-competitions_flower \"/usr/bin/dumb-init \u2026\" 4 months ago Up 2 months 0.0.0.0:5555->5555/tcp flower b93b3f7e0a3b codalab-competitions_rabbit \"docker-entrypoint.s\u2026\" 4 months ago Up 2 months 4369/tcp, 5671/tcp, 0.0.0.0:5672->5672/tcp, 15671/tcp, 25672/tcp, 0.0.0.0:15672->15672/tcp rabbit 5e3492959db1 postgres:9.6.3 \"docker-entrypoint.s\u2026\" 4 months ago Up 2 months 5432/tcp postgres We need use manage.py from the django container to migrate the database. This will talk to the postgres container for us. Codalab has notes for how to do this so we will follow their lead : Note, in the link from codalab they have a placeholder for APPNAME . Substitute web in for this. root@django:/app/codalab# ./manage schemamigration web --auto ./manage: line 3: ./config/generated/startup_env.sh: No such file or directory ./manage: line 4: ../venv/bin/activate: No such file or directory /usr/local/lib/python2.7/site-packages/django_extensions/db/fields/__init__.py:425: DeprecationWarning: Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported. warnings.warn(\"Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported.\", DeprecationWarning) + Added field enable_docker_submission on web.Competition Created 0085_auto__add_field_competition_enable_docker_submission.py. You can now apply this migration with: ./manage.py migrate web Once you have that migrate the database. ./manage migrate This will apply the migration and look for all changes to all tables. Looking in the results we can see our migration in action: ~ ~ Running migrations for web: - Migrating forwards to 0085_auto__add_field_competition_enable_docker_submission. > web:0085_auto__add_field_competition_enable_docker_submission ~ ~ Now let's verify our change. First create a database superuser: docker exec -it django python manage.py createsuperuser This will prompt you to create a superuser: bbearce@miccai2019:~/src/codalab-competitions/codalab$ docker exec -it django python manage.py createsuperuser /usr/local/lib/python2.7/site-packages/django_extensions/db/fields/__init__.py:425: DeprecationWarning: Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported. warnings.warn(\"Django 1.8 features a native UUIDField, this UUIDField will be removed after Django 1.7 becomes unsupported.\", DeprecationWarning) Username: admin Email address: bbearce@mgh.harvard.edu Password: Password (again): Superuser created successfully. Now proceed to http://<your domain>.com/admin and log in with the credentials you made. Find the \"Web\" app section and click \"Competitions\". Next scroll till you see \"Allow participant docker submission\" or whatever you put as your new column.","title":"Migrate Database So We Can See Our Changes"},{"location":"notes/codalab/docker_submission/#add-our-new-option-to-the-competitionform-class","text":"Django or CodaLab uses apps.web.forms.CompetitionForm to manage the edit competiton form. class CompetitionForm(forms.ModelForm): class Meta: model = models.Competition fields = ( 'title', 'description', 'queue', 'disallow_leaderboard_modifying', 'force_submission_to_leaderboard', 'image', 'end_date', 'published', 'enable_medical_image_viewer', 'enable_detailed_results', # change start 'enable_docker_submission', # change end 'admins', ... This will allow us to see this option in the edit competition view: This has the effect of allowing access to this option from multiple views so we can conditionally render a docker submission form or a regular result submission form.","title":"Add Our new Option to the CompetitionForm Class"},{"location":"notes/codalab/docker_submission/#step-3-add-conditional-rendering-for-dockerresult-submission","text":"","title":"Step 3: Add Conditional Rendering for Docker\\Result Submission"},{"location":"notes/codalab/docker_submission/#add-ui-elements","text":"Find template apps/web/templates/web/competitions/_submit_results_page.html . Roughly on line 70 there is a {% if USE_AWS %} block and in the {% else %} block we can add our code. Replace the inside of the else block with this code: {% if phase.competition.enable_docker_submission %} <h1>DOCKER SUBMISSION ENABLED </h1> <!-- Submit Docker Button --> <button id=submit-button type=\"button\" data-toggle=\"modal\" data-target=\"#submit-docker-dialog\" class=\"btn btn-secondary\">Submit Docker</button> <!-- Submit Docker Button --> {% else %} <h1>DOCKER SUBMISSION DISABLED</h1> <!-- original code from codalab --> <button id=\"fileUploadButton\" class=\"button btn btn-primary {% if not phase.reference_data %}disabled{% endif %}\" {% if not phase.reference_data %}disabled=\"disabled\"{% endif %}> Submit </button> <!-- original code from codalab --> {% endif %} Our If Else block has the original code in the else. At this point if you refresh the page and navigate to the \"Participate\" tab, you should see this. Now if we go to \"Edit\" and look at the competition options, we see that if we check the box \"Allow participant docker submission\" we can see the text change on the \"Participate\" tab. Next we need the \"Submit Docker\" button to do something. Let's add a modal that get's triggered when you click the button. Add this code just before the line that has {% include \"web/common/_submission_details_template.html\" %} on line ~167: <div class=\"modal fade\" id=\"submit-docker-dialog\"> <div class=\"modal-dialog\"> <div class=\"modal-content\"> <div class=\"modal-header\"> <button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-hidden=\"true\">&times;</button> <h4 class=\"modal-title\">Submit Docker</h4> </div> <div class=\"modal-body\"> <h4>Submit Docker Run Command</h4> <textarea id=\"docker-run-command\" type=text style=\"width: 500px; height: 250px;\">docker run -v /<local-path>:/<docker-container-path-1> -v /<local-path>:/<docker-container-path-2> user/repo:tag</textarea> </div> <div class=\"modal-footer\"> <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button> <button id=\"submit-docker\" class=\"btn btn-primary\">Send</button> </div> </div> </div> </div> You should see this when you click the \"Submit Docker\" button: Now let's tie the \"Send\" button from the modal to some javascript so we can call the backend to process this for us. At the bottom of the same file we've been working in, add this code and create the file apps/web/static/js/submit_docker.js: <!-- Docker Submission Front End --> <script type=\"text/javascript\" src=\"{{STATIC_URL}}js/submit_docker.js\"></script> <!-- Docker Submission Front End --> In submit_docker.js add this: $('#submit-docker').click(function(){ //copy from codalab/apps/web/static/js/Competition.js file $('#details').html('Creating new submission...'); var competitionId = $(\"#competitionId\").val() var description = $('#submission_description_textarea').val() || ''; var method_name = $('#submission_method_name').val() || ''; var method_description = $('#submission_method_description').val() || ''; var project_url = $('#submission_project_url').val() || ''; var publication_url = $('#submission_publication_url').val() || ''; var bibtex = $('#submission_bibtex').val() || ''; var team_name = $('#submission_team_name').val() || ''; var organization_or_affiliation = $('#submission_organization_or_affiliation').val() || ''; var phase_id = $('#submission_phase_id').val(); // capture docker run command var docker_run_cmd = $('#docker-run-command').val(); console.log(docker_run_cmd); // capture docker run command $('#submission_description_textarea').val(''); // console.log('/api/competition/' + competitionId + '/submission?description=' + encodeURIComponent(description) + // '&method_name=' + encodeURIComponent(method_name) + // '&method_description=' + encodeURIComponent(method_description) + // '&project_url=' + encodeURIComponent(project_url) + // '&publication_url=' + encodeURIComponent(publication_url) + // '&bibtex=' + encodeURIComponent(bibtex) + // '&team_name=' + encodeURIComponent(team_name) + // '&organization_or_affiliation=' + encodeURIComponent(organization_or_affiliation) + // '&phase_id=' + encodeURIComponent(phase_id)+ // '&docker-run-command='+ encodeURIComponent(docker_run_cmd) // ) $.ajax({ url: '/api/competition/' + competitionId + '/submission?description=' + encodeURIComponent(description) + '&method_name=' + encodeURIComponent(method_name) + '&method_description=' + encodeURIComponent(method_description) + '&project_url=' + encodeURIComponent(project_url) + '&publication_url=' + encodeURIComponent(publication_url) + '&bibtex=' + encodeURIComponent(bibtex) + '&team_name=' + encodeURIComponent(team_name) + '&organization_or_affiliation=' + encodeURIComponent(organization_or_affiliation) + '&phase_id=' + encodeURIComponent(phase_id)+ '&docker-run-command='+ encodeURIComponent(docker_run_cmd), type: 'post', cache: false, async: false, data: { 'id': 'trackingid', 'name': '', 'type': '', 'size': '' } }).done(function(response) { $('#details').html(''); $('#user_results tr.noData').remove(); $('#user_results').append(Competition.displayNewSubmission(response, description, method_name, method_description, project_url, publication_url, bibtex, team_name, organization_or_affiliation)); $('#user_results #' + response.id + ' .glyphicon-plus').on('click', function() { Competition.showOrHideSubmissionDetails(this) }); //$('#fileUploadButton').removeClass('disabled'); //$('#fileUploadButton').text(\"Submit Results...\"); $('#user_results #' + response.id + ' .glyphicon-plus').click(); location.reload(true); }).fail(function(jqXHR) { var msg = 'An unexpected error occurred.'; if (jqXHR.status == 403) { msg = jqXHR.responseJSON.detail; } $('#details').html(msg); //$('#fileUploadButton').text(\"Submit Results...\"); $('#fileUploadButton').removeClass('disabled'); }); $('#submit-docker-dialog').modal('toggle'); }); From looking at the ajax call you can see we are hitting the route for an api that ultimately triggers a POST request in view \"CompetitionSubmissionViewSet\" under the api app. The part we need to edit is the post_save() method.","title":"Add UI Elements"},{"location":"notes/codalab/docker_submission/#step-4-edit-site-worker-code","text":"Replace the post_save() with this: def post_save(self, obj, created): # Original Code #if created: # evaluate_submission.apply_async((obj.pk, obj.phase.is_scoring_only)) if escape(self.request.QUERY_PARAMS.get('docker-run-command')) == 'None': # submit_docker_command = None submit_docker_command = \"this is not a docker submission\" else: submit_docker_command = escape(self.request.QUERY_PARAMS.get('docker-run-command', \"\")).replace('&lt;','<').replace('&gt;','>') ## - pass extra info to submission if created: evaluate_submission.apply_async((obj.pk, obj.phase.is_scoring_only, submit_docker_command)) ## - pass extra info to submission At the bottom of post_save() is evaluate_submission . That is from apps/web/tasks.py. We are passing the submit_docker_command to evaluate submission . Change evaluate_submission to accept the submit_docker_command . Also edit it to pass the command to task_func() Add submit_docker_command to these two sections: def evaluate_submission(submission_id, is_scoring_only, submit_docker_command): and: task_func(submission, job_id, submit_docker_command) task_func is actually a user defined variable for score , a function in the same file above evaluate_submission . Add submit_docker_command to the input of score . def score(submission, job_id, submit_docker_command): Towards the bottom of score , _prepare_compute_worker_run needs submit_docker_command . Pass it in, and subsequently add it to it's function definition at towards the top of the page: _prepare_compute_worker_run(job_id, submission, submit_docker_command, is_prediction=False) and def _prepare_compute_worker_run(job_id, submission, submit_docker_command, is_prediction): Inside this function you will find a data variable we need to add the docker command to: data = { \"id\": job_id, \"task_type\": \"run\", \"task_args\": { \"submission_id\": submission.pk, \"docker_image\": docker_image, \"ingestion_program_docker_image\": docker_image, \"bundle_url\": _make_url_sassy(bundle_url), \"stdout_url\": _make_url_sassy(stdout, permission='w'), \"stderr_url\": _make_url_sassy(stderr, permission='w'), \"output_url\": _make_url_sassy(output, permission='w'), \"ingestion_program_output_url\": _make_url_sassy(submission.ingestion_program_stdout_file.name, permission='w'), \"ingestion_program_stderr_url\": _make_url_sassy(submission.ingestion_program_stderr_file.name, permission='w'), \"detailed_results_url\": _make_url_sassy(submission.detailed_results_file.name, permission='w'), \"private_output_url\": _make_url_sassy(submission.private_output_file.name, permission='w'), \"secret\": submission.secret, \"execution_time_limit\": submission.phase.execution_time_limit, \"predict\": is_prediction, 'submit_docker_command':submit_docker_command, ## - Added submit_docker_command here } At this point the code hands off the data to the Compute-Worker with compute_worker_run(data, soft_time_limit=time_limit, priority=2) .","title":"Step 4: Edit Site-Worker Code"},{"location":"notes/codalab/docker_submission/#step-5-the-compute-worker","text":"We need to enter the compute-worker docker container and edit \"worker.py\". On the machine where the compute-worker is running execute: $ docker exec -it codalab/competitions-v1-compute-worker:latest ...or use the compute image that you need. Once inside you should be able to see \"worker.py\". Open it. Find function run(task_id, task_args) . This is where we will add our code on approx line 500. Add this: if task_args['submit_docker_command'] != 'this is not a docker submission': print('@CUSTOM DOCKER START@') participant_docker_cmd = task_args['submit_docker_command'].format(input_dir+\"/res\") logger.info(\"Invoking program %s\", participant_docker_cmd) participant_docker_process = Popen(participant_docker_cmd.split(\" \")) participant_docker_process.wait() # This halts other actions till this run isfinished. print('@CUSTOM DOCKER END@') This is the important line: participant_docker_cmd = task_args['submit_docker_command'].format(input_dir+\"/res\") This line substitutes the results directory into a mount placeholder so that the container can put the results from the run into it. Then the compute-worker can consume the results. Update (7/17/2020): Docker Image Submission only and resulting handling: participant_docker_submission_cmd = [ 'docker', 'run', # Ask all participants to add this user '-u', 'participant', # Cut internet '--net', 'none', # Remove it after run '--rm', # Add support for GPUs and nvidia '--gpus', 'all', # Give it a name associated to task_id '--name={}'.format(\"participant_docker_submission_taskid_\"+str(task_id)), # Try the new timeout feature '--stop-timeout={}'.format(execution_time_limit), # Don't allow subprocesses to raise privileges '--security-opt=no-new-privileges', # Set the right volume '-v', '{0}:/mnt/in:ro'.format('/home/bbearce/Documents/docker_submissions/directory_of_files'), # :ro for read-only file system '-v', '{0}:/mnt/out'.format(input_dir+\"/res\"), # Set aside 512m memory for the host #'--memory', '{}MB'.format(available_memory_mib - 512), # Don't buffer python output, so we don't lose any #'-e', 'PYTHONUNBUFFERED=1', # Set current working directory #'-w', run_dir, # Note that hidden data dir is excluded here! # Set the right image task_args['submit_docker_command'], ] if task_args['submit_docker_command'] != 'this is not a docker submission': print('@CUSTOM DOCKER START@') logger.info(\"Invoking participant docker submission: %s\", participant_docker_submission_cmd) participant_docker_process = Popen(participant_docker_submission_cmd) participant_docker_process.wait() # This halts other actions till this run isfinished. print('@CUSTOM DOCKER END@') Below I will discuss some of the important options... Note, we need to mount two folders ( datain:/mnt/in , dataout:/mnt/out ). We don't need to change anything about datain because we can use :ro to make it read-only inside the docker container like so /mnt/in:ro . This keeps users from tampering with input data. dataout needs to be owned by user \"participant\". We do this like so... $ chown participant:participant <path to>/dataout and make it writable as well: $ chmod 0777 <path to>/dataout If participant is the user that will be using these mounts then we need the container to run as participant . Notice we run the docker with -u participant , but how do we get that user in the docker ahead of time. In the Dockerfile for a submission add this to the top: ARG USER=participant ARG UID=1000 ARG GID=1000 # default password for user ARG PW=participant # Option1: Using unencrypted password/ specifying password RUN useradd -m ${USER} --uid=${UID} && echo \"${USER}:${PW}\" | \\ chpasswd This should add user participant with password participant . Finally we need to make sure the container has no internet. We add flag --net none to cut internet to the docker rendering it unable to transfer data. Lastly if this is an nvidia compute worker codalab/competitions-v1-nvidia-worker:latest then we have a few extra things ot take care of. On lines 274-275 there are two variables that don't exist being referenced. max_execution_time_limit = task_args['max_execution_time_limit'] previous_execution_time = task_args['previous_execution_time'] to #max_execution_time_limit = task_args['max_execution_time_limit'] # -BB- #previous_execution_time = task_args['previous_execution_time'] # -BB- and at the bottom of the file (~ line 700): signal.alarm(int(math.fabs(math.ceil(max_execution_time_limit - time_difference - previous_execution_time)))) # Total Execution to #signal.alarm(int(math.fabs(math.ceil(max_execution_time_limit - time_difference - previous_execution_time)))) # Total Execution -BB-","title":"Step 5: The Compute-Worker"},{"location":"notes/couchdb/basics/","text":"CouchDB Good delete script Source moduliertersingvogel Supplementary Material tutorialspoint Keep in mind that your view determines how you structure the object in todelete.append( object ) #!/usr/bin/env python3 # coding: utf-8 import json import requests import sys database=sys.argv[1] if len(database)==0: sys.exit(1) # You can use views r=requests.get(\"http://localhost:5984/{}/_all_docs\".format(database)) rows=json.loads(r.text)['rows'] print(len(rows)) todelete=[] for doc in rows: # original # todelete.append({\"_deleted\": True, \"_id\": doc[\"id\"], \"_rev\": doc[\"value\"][\"rev\"]}) todelete.append({\"_deleted\": True, \"_id\": doc[\"id\"], \"_rev\": doc[\"value\"][0]}) r=requests.post(\"http://localhost:5984/{}/_bulk_docs\".format(database), json={\"docs\": todelete}) print(r.status_code)","title":"Basics"},{"location":"notes/couchdb/basics/#couchdb","text":"","title":"CouchDB"},{"location":"notes/couchdb/basics/#good-delete-script","text":"Source moduliertersingvogel Supplementary Material tutorialspoint Keep in mind that your view determines how you structure the object in todelete.append( object ) #!/usr/bin/env python3 # coding: utf-8 import json import requests import sys database=sys.argv[1] if len(database)==0: sys.exit(1) # You can use views r=requests.get(\"http://localhost:5984/{}/_all_docs\".format(database)) rows=json.loads(r.text)['rows'] print(len(rows)) todelete=[] for doc in rows: # original # todelete.append({\"_deleted\": True, \"_id\": doc[\"id\"], \"_rev\": doc[\"value\"][\"rev\"]}) todelete.append({\"_deleted\": True, \"_id\": doc[\"id\"], \"_rev\": doc[\"value\"][0]}) r=requests.post(\"http://localhost:5984/{}/_bulk_docs\".format(database), json={\"docs\": todelete}) print(r.status_code)","title":"Good delete script"},{"location":"notes/couchdb/container/","text":"From Container Source docker $ docker pull couchdb This will get you latest. Then as per their instructoins run this command: $ docker run -d --name my-couchdb couchdb:tag If you want to expose the port to the outside world, run $ docker run -p 5984:5984 -d couchdb Start not in Admin Party mode: $ docker run -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=password -d couchdb Example I used for a simple project: $ docker run -d --name ben-couchdb2 -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=password -p 5985:5984 couchdb:latest","title":"As container"},{"location":"notes/couchdb/container/#from-container","text":"Source docker $ docker pull couchdb This will get you latest. Then as per their instructoins run this command: $ docker run -d --name my-couchdb couchdb:tag If you want to expose the port to the outside world, run $ docker run -p 5984:5984 -d couchdb Start not in Admin Party mode: $ docker run -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=password -d couchdb Example I used for a simple project: $ docker run -d --name ben-couchdb2 -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=password -p 5985:5984 couchdb:latest","title":"From Container"},{"location":"notes/css/css/","text":"CSS Style something with: p { color:red; }","title":"CSS"},{"location":"notes/css/css/#css","text":"Style something with: p { color:red; }","title":"CSS"},{"location":"notes/docker/basics/","text":"Docker Recap and cheat sheet: List Docker CLI commands docker docker container --help Display Docker version and info docker --version docker version docker info Execute Docker image docker run hello-world List Docker images docker image ls docker image ls --all docker image ls -aq List Docker containers (running, all, all in quiet mode) docker container ls docker container ls --all docker container ls \u2013aq Location of Docker logs Source: stackoverflow Use this command to locate the logs: docker inspect --format='{{.LogPath}}' containername and this to view live: tail -f `docker inspect --format='{{.LogPath}}' containername` PS: Don't know why but docker logs containername > docker.log just creates a blank file. List Docker services docker service ls docker stack services getstartedlab Tasks Note the addition of '_web' to the service name docker service ps getstartedlab_web Note: container ls will show you the tasks running as well docker container ls Run without sudo To create the docker group and add your user: Create the docker group: $ sudo groupadd docker Add your user to the docker group. $ sudo usermod -aG docker $USER Log out and log back in so that your group membership is re-evaluated. If testing on a virtual machine, it may be necessary to restart the virtual machine for changes to take effect. On a desktop Linux environment such as X Windows, log out of your session completely and then log back in. On Linux, you can also run the following command to activate the changes to groups: $ newgrp docker Verify that you can run docker commands without sudo. $ docker run hello-world Useful Commands Use docker container inspect 4ca8ce46f817 to inspect a container. You will get a json dump of characteristics that are super useful. Example: [ { \"Id\": \"4ca8ce46f8170fc5c5eeb93bc27e8f84c2e8b32ddafc8e748a124e24fc8ff455\", \"Created\": \"2019-09-26T19:10:42.524604801Z\", \"Path\": \"python\", \"Args\": [ \"predict.py\" ], \"State\": { \"Status\": \"exited\", \"Running\": false, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 0, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2019-09-26T19:10:43.536396229Z\", \"FinishedAt\": \"2019-09-27T01:41:30.393256193Z\" . . .there is a lot more. Useful Flags -v|--volume[=[[HOST-DIR:]CONTAINER-DIR[:OPTIONS]]] Create a bind mount. If you specify, -v /HOST-DIR:/CONTAINER-DIR, Docker bind mounts /HOST-DIR in the host to /CONTAINER-DIR in the Docker container. If 'HOST-DIR' is omitted, Docker automatically creates the new volume on the host. The OPTIONS are a comma delimited list and can be: \u00b7 [rw|ro] \u00b7 [z|Z] \u00b7 [[r]shared|[r]slave|[r]private] \u00b7 [delegated|cached|consistent] \u00b7 [nocopy] -d, --detach=true|false Detached mode: run the container in the background and print the new container ID. The default is false. -i, --interactive=true|false Keep STDIN open even if not attached. The default is false. When set to true, keep stdin open even if not attached. -t, --tty=true|false Allocate a pseudo-TTY. The default is false. When set to true Docker can allocate a pseudo-tty and attach to the standard input of any container. This can be used, for example, to run a throwaway interactive shell. The default is false. The -t option is incompatible with a redirection of the docker client standard input. --name=\" \" Assign a name to the container The operator can identify a container in three ways: \u2502Identifier type \u2502 Example value \u2502 \u2502UUID long identifier \u2502 \"f78375b1c487e03c9438c729345e54db9d20cfa2ac1fc3494b6eb60872e74778\" \u2502 \u2502UUID short identifier \u2502 \"f78375b1c487\"\u2502 \u2502Name \u2502 \"evil_ptolemy\"| --restart=\" \" To configure the restart policy for a container, use the --restart flag when using the docker run command. The value of the --restart flag can be any of the following: Flag Description no Do not automatically restart the container. (the default) on-failure Restart the container if it exits due to an error, which manifests as a non-zero exit code. always Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details ) unless-stopped Similar to always , except that when the container is stopped (manually or otherwise), it is not restarted even after Docker daemon restarts. Managing and Removing Good discussion Summary docker build -t friendlyhello . # Create image using this directory's Dockerfile docker run -d -p 4000:80 friendlyhello # Same thing, but in detached mode docker container ls # List all running containers docker container ls -a # List all containers, even those not running docker container stop <hash> # Gracefully stop the specified container docker container kill <hash> # Force shutdown of the specified container docker container rm <hash> # Remove specified container from this machine docker container rm $(docker container ls -a -q) # Remove all containers docker image ls -a # List all images on this machine docker image rm <image id> # Remove specified image from this machine docker image rm $(docker image ls -a -q) # Remove all images from this machine docker login # Log in this CLI session using your Docker credentials docker tag <image> username/repository:tag # Tag <image> for upload to registry docker push username/repository:tag # Upload tagged image to registry docker run username/repository:tag # Run image from a registry Investigating and playing around DockerHub: Image specifications: Account: bbearce Repo: Nomenclature: bbearce/ : To start with a clean slate: Stop all containers: docker container rm $(docker container ls -a -q) # Remove all containers docker image rm $(docker image ls -a -q) # Remove all images from this machine If you want to run an instance of an image issue this command(this will download it if you don't have the image): docker run -it -d ashok/pyashokproj bin/bash Keep in mind this is different than getting inside a running container. For this to work the container needs to be already running: docker exec -it <container-name/ID> bash Some dockers can be started and stopped indefinitely either because they are a web server or an OS image(Ubuntu). Others based on things like pythonX.X can't be started up once stopped. A lot of customization and investigation can only really be done inside the running docker. So in order to get in we need to one of two things depending on the docker: OS or web server type docker: Use docker container \u2013ls a to find the stopped container id. Next use docker start <container id> to start the container again. Now use docker ps to see that it is running Python or language image: If you attempt to start the stopped container it will run for a brief second like it already did upon instantiation and then stop. We have to explicitly use docker run -it -d <image> bin/bash . This needs all of those flags (itd) in order to start a container in an interactive session, pipe the terminal from the docker to your local terminal and to run in detached mode. Now that it is running indefinitely we can run docker exec -it <container-name/ID> bash and actually connect to it. Start a docker container: docker start <container-name/ID> Stop a docker container: docker stop <container-name/ID> Take a container and make a new image from it (Example): docker run -itd codalab/codalab-legacy bash Note: Don't forget the -d This downloads a new image and starts an instance. It doesn't have the python package 'seaborn' installed in the python so we are going to add it. We started a container and it is running in detached mode. Now let's connect to it. Use docker ps to get the container's id: docker exec -it d7ef724c4309 bash You should be looking at the prompt: root@f907b9e5d9a6:/# Follow these instructions root@f907b9e5d9a6:/# pip install seaborn ...python is importing stuff... root@f907b9e5d9a6:/# python >>> import seaborn >>> You can see we installed seaborn. Now exit the docker root@f907b9e5d9a6:/# exit . We need to build it to an image: docker commit d7ef724c4309 bbearce/codalab:legacy We just made an image and tagged it with this identifier bbearce/codalab:legacy and we can see it if we execute: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE bbearce/codalab legacy c2d04ddb825a 3 seconds ago 2.65GB codalab/codalab-legacy latest 432ce2829707 20 months ago 2.64GB Now let's push it online (Docker Hub) as an optional last step: docker login docker push bbearce/codalab:legacy Another way to share is with: docker save docker load save/load docs Internet issues Source medium.com I had issues while connected to vpn. It was solved once I disconnected...","title":"Basics"},{"location":"notes/docker/basics/#docker","text":"Recap and cheat sheet: List Docker CLI commands docker docker container --help Display Docker version and info docker --version docker version docker info Execute Docker image docker run hello-world List Docker images docker image ls docker image ls --all docker image ls -aq List Docker containers (running, all, all in quiet mode) docker container ls docker container ls --all docker container ls \u2013aq Location of Docker logs Source: stackoverflow Use this command to locate the logs: docker inspect --format='{{.LogPath}}' containername and this to view live: tail -f `docker inspect --format='{{.LogPath}}' containername` PS: Don't know why but docker logs containername > docker.log just creates a blank file. List Docker services docker service ls docker stack services getstartedlab Tasks Note the addition of '_web' to the service name docker service ps getstartedlab_web Note: container ls will show you the tasks running as well docker container ls","title":"Docker"},{"location":"notes/docker/basics/#run-without-sudo","text":"To create the docker group and add your user: Create the docker group: $ sudo groupadd docker Add your user to the docker group. $ sudo usermod -aG docker $USER Log out and log back in so that your group membership is re-evaluated. If testing on a virtual machine, it may be necessary to restart the virtual machine for changes to take effect. On a desktop Linux environment such as X Windows, log out of your session completely and then log back in. On Linux, you can also run the following command to activate the changes to groups: $ newgrp docker Verify that you can run docker commands without sudo. $ docker run hello-world","title":"Run without sudo"},{"location":"notes/docker/basics/#useful-commands","text":"Use docker container inspect 4ca8ce46f817 to inspect a container. You will get a json dump of characteristics that are super useful. Example: [ { \"Id\": \"4ca8ce46f8170fc5c5eeb93bc27e8f84c2e8b32ddafc8e748a124e24fc8ff455\", \"Created\": \"2019-09-26T19:10:42.524604801Z\", \"Path\": \"python\", \"Args\": [ \"predict.py\" ], \"State\": { \"Status\": \"exited\", \"Running\": false, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 0, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2019-09-26T19:10:43.536396229Z\", \"FinishedAt\": \"2019-09-27T01:41:30.393256193Z\" . . .there is a lot more.","title":"Useful Commands"},{"location":"notes/docker/basics/#useful-flags","text":"-v|--volume[=[[HOST-DIR:]CONTAINER-DIR[:OPTIONS]]] Create a bind mount. If you specify, -v /HOST-DIR:/CONTAINER-DIR, Docker bind mounts /HOST-DIR in the host to /CONTAINER-DIR in the Docker container. If 'HOST-DIR' is omitted, Docker automatically creates the new volume on the host. The OPTIONS are a comma delimited list and can be: \u00b7 [rw|ro] \u00b7 [z|Z] \u00b7 [[r]shared|[r]slave|[r]private] \u00b7 [delegated|cached|consistent] \u00b7 [nocopy] -d, --detach=true|false Detached mode: run the container in the background and print the new container ID. The default is false. -i, --interactive=true|false Keep STDIN open even if not attached. The default is false. When set to true, keep stdin open even if not attached. -t, --tty=true|false Allocate a pseudo-TTY. The default is false. When set to true Docker can allocate a pseudo-tty and attach to the standard input of any container. This can be used, for example, to run a throwaway interactive shell. The default is false. The -t option is incompatible with a redirection of the docker client standard input. --name=\" \" Assign a name to the container The operator can identify a container in three ways: \u2502Identifier type \u2502 Example value \u2502 \u2502UUID long identifier \u2502 \"f78375b1c487e03c9438c729345e54db9d20cfa2ac1fc3494b6eb60872e74778\" \u2502 \u2502UUID short identifier \u2502 \"f78375b1c487\"\u2502 \u2502Name \u2502 \"evil_ptolemy\"| --restart=\" \" To configure the restart policy for a container, use the --restart flag when using the docker run command. The value of the --restart flag can be any of the following: Flag Description no Do not automatically restart the container. (the default) on-failure Restart the container if it exits due to an error, which manifests as a non-zero exit code. always Always restart the container if it stops. If it is manually stopped, it is restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details ) unless-stopped Similar to always , except that when the container is stopped (manually or otherwise), it is not restarted even after Docker daemon restarts.","title":"Useful Flags"},{"location":"notes/docker/basics/#managing-and-removing","text":"Good discussion Summary docker build -t friendlyhello . # Create image using this directory's Dockerfile docker run -d -p 4000:80 friendlyhello # Same thing, but in detached mode docker container ls # List all running containers docker container ls -a # List all containers, even those not running docker container stop <hash> # Gracefully stop the specified container docker container kill <hash> # Force shutdown of the specified container docker container rm <hash> # Remove specified container from this machine docker container rm $(docker container ls -a -q) # Remove all containers docker image ls -a # List all images on this machine docker image rm <image id> # Remove specified image from this machine docker image rm $(docker image ls -a -q) # Remove all images from this machine docker login # Log in this CLI session using your Docker credentials docker tag <image> username/repository:tag # Tag <image> for upload to registry docker push username/repository:tag # Upload tagged image to registry docker run username/repository:tag # Run image from a registry","title":"Managing and Removing"},{"location":"notes/docker/basics/#investigating-and-playing-around","text":"DockerHub: Image specifications: Account: bbearce Repo: Nomenclature: bbearce/ : To start with a clean slate: Stop all containers: docker container rm $(docker container ls -a -q) # Remove all containers docker image rm $(docker image ls -a -q) # Remove all images from this machine If you want to run an instance of an image issue this command(this will download it if you don't have the image): docker run -it -d ashok/pyashokproj bin/bash Keep in mind this is different than getting inside a running container. For this to work the container needs to be already running: docker exec -it <container-name/ID> bash Some dockers can be started and stopped indefinitely either because they are a web server or an OS image(Ubuntu). Others based on things like pythonX.X can't be started up once stopped. A lot of customization and investigation can only really be done inside the running docker. So in order to get in we need to one of two things depending on the docker: OS or web server type docker: Use docker container \u2013ls a to find the stopped container id. Next use docker start <container id> to start the container again. Now use docker ps to see that it is running Python or language image: If you attempt to start the stopped container it will run for a brief second like it already did upon instantiation and then stop. We have to explicitly use docker run -it -d <image> bin/bash . This needs all of those flags (itd) in order to start a container in an interactive session, pipe the terminal from the docker to your local terminal and to run in detached mode. Now that it is running indefinitely we can run docker exec -it <container-name/ID> bash and actually connect to it. Start a docker container: docker start <container-name/ID> Stop a docker container: docker stop <container-name/ID> Take a container and make a new image from it (Example): docker run -itd codalab/codalab-legacy bash Note: Don't forget the -d This downloads a new image and starts an instance. It doesn't have the python package 'seaborn' installed in the python so we are going to add it. We started a container and it is running in detached mode. Now let's connect to it. Use docker ps to get the container's id: docker exec -it d7ef724c4309 bash You should be looking at the prompt: root@f907b9e5d9a6:/# Follow these instructions root@f907b9e5d9a6:/# pip install seaborn ...python is importing stuff... root@f907b9e5d9a6:/# python >>> import seaborn >>> You can see we installed seaborn. Now exit the docker root@f907b9e5d9a6:/# exit . We need to build it to an image: docker commit d7ef724c4309 bbearce/codalab:legacy We just made an image and tagged it with this identifier bbearce/codalab:legacy and we can see it if we execute: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE bbearce/codalab legacy c2d04ddb825a 3 seconds ago 2.65GB codalab/codalab-legacy latest 432ce2829707 20 months ago 2.64GB Now let's push it online (Docker Hub) as an optional last step: docker login docker push bbearce/codalab:legacy Another way to share is with: docker save docker load save/load docs","title":"Investigating and playing around"},{"location":"notes/docker/basics/#internet-issues","text":"Source medium.com I had issues while connected to vpn. It was solved once I disconnected...","title":"Internet issues"},{"location":"notes/docker/install/","text":"Install Docker has good documentation here . Uninstall old versions $ sudo apt-get remove docker docker-engine docker.io containerd runc Install using the repository [1] Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update and $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common [2] Add Docker\u2019s official GPG key: $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Verify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 , by searching for the last 8 characters of the fingerprint. $ sudo apt-key fingerprint 0EBFCD88 pub rsa4096 2017-02-22 [SCEA] 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid [ unknown] Docker Release (CE deb) <docker@docker.com> sub rsa4096 2017-02-22 [S] [3] Use the following command to set up the stable repository. $ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" Install Docker Engine [1] Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io [2] Verify that Docker Engine is installed correctly by running the hello-world image. $ sudo docker run hello-world [3] If you would like to use Docker as a non-root user, you should now consider adding your user to the \u201cdocker\u201d group with something like: $ sudo usermod -aG docker <your-user>","title":"Install"},{"location":"notes/docker/install/#install","text":"Docker has good documentation here .","title":"Install"},{"location":"notes/docker/install/#uninstall-old-versions","text":"$ sudo apt-get remove docker docker-engine docker.io containerd runc","title":"Uninstall old versions"},{"location":"notes/docker/install/#install-using-the-repository","text":"","title":"Install using the repository"},{"location":"notes/docker/install/#1-update-the-apt-package-index-and-install-packages-to-allow-apt-to-use-a-repository-over-https","text":"sudo apt-get update and $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common","title":"[1] Update the apt package index and install packages to allow apt to use a repository over HTTPS:"},{"location":"notes/docker/install/#2-add-dockers-official-gpg-key","text":"$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Verify that you now have the key with the fingerprint 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 , by searching for the last 8 characters of the fingerprint. $ sudo apt-key fingerprint 0EBFCD88 pub rsa4096 2017-02-22 [SCEA] 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 uid [ unknown] Docker Release (CE deb) <docker@docker.com> sub rsa4096 2017-02-22 [S]","title":"[2] Add Docker\u2019s official GPG key:"},{"location":"notes/docker/install/#3-use-the-following-command-to-set-up-the-stable-repository","text":"$ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"","title":"[3] Use the following command to set up the stable repository."},{"location":"notes/docker/install/#install-docker-engine","text":"[1] Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io [2] Verify that Docker Engine is installed correctly by running the hello-world image. $ sudo docker run hello-world [3] If you would like to use Docker as a non-root user, you should now consider adding your user to the \u201cdocker\u201d group with something like: $ sudo usermod -aG docker <your-user>","title":"Install Docker Engine"},{"location":"notes/docker/remote_registry/","text":"Remote Registry Courtesy of docs.docker.com Deploy a registry server Before you can deploy a registry, you need to install Docker on the host. A registry is an instance of the registry image, and runs within Docker. Run a local registry Use a command like the following to start the registry container: $ docker run -d -p 5000:5000 --restart=always --name registry registry:2 The registry is now ready to use. Warning: These first few examples show registry configurations that are only appropriate for testing. A production-ready registry must be protected by TLS and should ideally use an access-control mechanism. Keep reading and then continue to the configuration guide to deploy a production-ready registry. Copy an image from Docker Hub to your registry You can pull an image from Docker Hub and push it to your registry. The following example pulls the ubuntu:16.04 image from Docker Hub and re-tags it as my-ubuntu, then pushes it to the local registry. Finally, the ubuntu:16.04 and my-ubuntu images are deleted locally and the my-ubuntu image is pulled from the local registry. Pull the ubuntu:16.04 image from Docker Hub. $ docker pull ubuntu:16.04 Tag the image as localhost:5000/my-ubuntu. This creates an additional tag for the existing image. When the first part of the tag is a hostname and port, Docker interprets this as the location of a registry, when pushing. $ docker tag ubuntu:16.04 localhost:5000/my-ubuntu Push the image to the local registry running at localhost:5000: $ docker push localhost:5000/my-ubuntu Remove the locally-cached ubuntu:16.04 and localhost:5000/my-ubuntu images, so that you can test pulling the image from your registry. This does not remove the localhost:5000/my-ubuntu image from your registry. $ docker image remove ubuntu:16.04 $ docker image remove localhost:5000/my-ubuntu Pull the localhost:5000/my-ubuntu image from your local registry. $ docker pull localhost:5000/my-ubuntu Stop a local registry To stop the registry, use the same docker container stop command as with any other container. $ docker container stop registry To remove the container, use docker container rm. $ docker container stop registry && docker container rm -v registry SSL and HTTPS","title":"Remote Registry"},{"location":"notes/docker/remote_registry/#remote-registry","text":"Courtesy of docs.docker.com","title":"Remote Registry"},{"location":"notes/docker/remote_registry/#deploy-a-registry-server","text":"Before you can deploy a registry, you need to install Docker on the host. A registry is an instance of the registry image, and runs within Docker.","title":"Deploy a registry server"},{"location":"notes/docker/remote_registry/#run-a-local-registry","text":"Use a command like the following to start the registry container: $ docker run -d -p 5000:5000 --restart=always --name registry registry:2 The registry is now ready to use. Warning: These first few examples show registry configurations that are only appropriate for testing. A production-ready registry must be protected by TLS and should ideally use an access-control mechanism. Keep reading and then continue to the configuration guide to deploy a production-ready registry.","title":"Run a local registry"},{"location":"notes/docker/remote_registry/#copy-an-image-from-docker-hub-to-your-registry","text":"You can pull an image from Docker Hub and push it to your registry. The following example pulls the ubuntu:16.04 image from Docker Hub and re-tags it as my-ubuntu, then pushes it to the local registry. Finally, the ubuntu:16.04 and my-ubuntu images are deleted locally and the my-ubuntu image is pulled from the local registry. Pull the ubuntu:16.04 image from Docker Hub. $ docker pull ubuntu:16.04 Tag the image as localhost:5000/my-ubuntu. This creates an additional tag for the existing image. When the first part of the tag is a hostname and port, Docker interprets this as the location of a registry, when pushing. $ docker tag ubuntu:16.04 localhost:5000/my-ubuntu Push the image to the local registry running at localhost:5000: $ docker push localhost:5000/my-ubuntu Remove the locally-cached ubuntu:16.04 and localhost:5000/my-ubuntu images, so that you can test pulling the image from your registry. This does not remove the localhost:5000/my-ubuntu image from your registry. $ docker image remove ubuntu:16.04 $ docker image remove localhost:5000/my-ubuntu Pull the localhost:5000/my-ubuntu image from your local registry. $ docker pull localhost:5000/my-ubuntu","title":"Copy an image from Docker Hub to your registry"},{"location":"notes/docker/remote_registry/#stop-a-local-registry","text":"To stop the registry, use the same docker container stop command as with any other container. $ docker container stop registry To remove the container, use docker container rm. $ docker container stop registry && docker container rm -v registry","title":"Stop a local registry"},{"location":"notes/docker/remote_registry/#ssl-and-https","text":"","title":"SSL and HTTPS"},{"location":"notes/docker/tutorial/","text":"Docker Command Sumamry We will go over these fundamental commands: docker build -t friendlyhello . # Create image using this directory's Dockerfile docker run -d -p 4000:80 friendlyhello # Same thing, but in detached mode docker container ls # List all running containers docker container ls -a # List all containers, even those not running docker container stop <hash> # Gracefully stop the specified container docker container kill <hash> # Force shutdown of the specified container docker container rm <hash> # Remove specified container from this machine docker container rm $(docker container ls -a -q) # Remove all containers docker image ls -a # List all images on this machine docker image rm <image id> # Remove specified image from this machine docker image rm $(docker image ls -a -q) # Remove all images from this machine docker login # Log in this CLI session using your Docker credentials docker tag <image> username/repository:tag # Tag <image> for upload to registry docker push username/repository:tag # Upload tagged image to registry docker run username/repository:tag # Run image from a registry Lists images on machine docker images Lists running containers docker ps Lists all containers stopped or otherwise docker ps -a Start a docker docker run bbearce/docker_demo:from_docker_file Delete a container docker container rm 3e64bfa61b7e Removes an image docker image rm 7a59e09539e4 Run an interactive session docker run -it bbearce/docker_demo:from_docker_file bash -i - interactive sessoin -t - join docker terminal to yours bash - run bash inside the session instead of default program(s) Detach from container and leave running <cntrl p+q> To attach back to a docker container docker attach silly_wright or docker attach <container_id> Stop a running docker like this (you'll need to to delete it) docker stop <container_id or name> To name a container --name=<name> docker run -it --name=auto_remove bbearce/docker_demo:from_docker_file bash Tag existing image as a different name (repo/image:tag) docker tag bbearce/docker_demo:from_docker_file bbearce/docker_demo:delete_me Login to your dockerhub account docker login --username=bbearce To push image online docker push bbearce/docker_demo:from_docker_file To automatically remove container when exited docker run -it --rm --name=auto_remove bbearce/docker_demo:from_docker_file bash Use bash flag -v : to mount the host folder inside the docker. docker run -it --rm --name=auto_remove -v /home/bbearce/Desktop/Docker_Demo/mount:/app bbearce/docker_demo:from_docker_file bash from a Dockerfile docker build -t bbearce/docker_demo:from_docker_file .","title":"Tutorial"},{"location":"notes/docker/tutorial/#docker-command-sumamry","text":"","title":"Docker Command Sumamry"},{"location":"notes/docker/tutorial/#we-will-go-over-these-fundamental-commands","text":"docker build -t friendlyhello . # Create image using this directory's Dockerfile docker run -d -p 4000:80 friendlyhello # Same thing, but in detached mode docker container ls # List all running containers docker container ls -a # List all containers, even those not running docker container stop <hash> # Gracefully stop the specified container docker container kill <hash> # Force shutdown of the specified container docker container rm <hash> # Remove specified container from this machine docker container rm $(docker container ls -a -q) # Remove all containers docker image ls -a # List all images on this machine docker image rm <image id> # Remove specified image from this machine docker image rm $(docker image ls -a -q) # Remove all images from this machine docker login # Log in this CLI session using your Docker credentials docker tag <image> username/repository:tag # Tag <image> for upload to registry docker push username/repository:tag # Upload tagged image to registry docker run username/repository:tag # Run image from a registry","title":"We will go over these fundamental commands:"},{"location":"notes/docker/tutorial/#lists-images-on-machine","text":"docker images","title":"Lists images on machine"},{"location":"notes/docker/tutorial/#lists-running-containers","text":"docker ps","title":"Lists running containers"},{"location":"notes/docker/tutorial/#lists-all-containers-stopped-or-otherwise","text":"docker ps -a","title":"Lists all containers stopped or otherwise"},{"location":"notes/docker/tutorial/#start-a-docker","text":"docker run bbearce/docker_demo:from_docker_file","title":"Start a docker"},{"location":"notes/docker/tutorial/#delete-a-container","text":"docker container rm 3e64bfa61b7e","title":"Delete a container"},{"location":"notes/docker/tutorial/#removes-an-image","text":"docker image rm 7a59e09539e4","title":"Removes an image"},{"location":"notes/docker/tutorial/#run-an-interactive-session","text":"docker run -it bbearce/docker_demo:from_docker_file bash -i - interactive sessoin -t - join docker terminal to yours bash - run bash inside the session instead of default program(s)","title":"Run an interactive session"},{"location":"notes/docker/tutorial/#detach-from-container-and-leave-running","text":"<cntrl p+q>","title":"Detach from container and leave running"},{"location":"notes/docker/tutorial/#to-attach-back-to-a-docker-container","text":"docker attach silly_wright or docker attach <container_id>","title":"To attach back to a docker container"},{"location":"notes/docker/tutorial/#stop-a-running-docker-like-this-youll-need-to-to-delete-it","text":"docker stop <container_id or name>","title":"Stop a running docker like this (you'll need to to delete it)"},{"location":"notes/docker/tutorial/#to-name-a-container","text":"--name=<name> docker run -it --name=auto_remove bbearce/docker_demo:from_docker_file bash","title":"To name a container"},{"location":"notes/docker/tutorial/#tag-existing-image-as-a-different-name-repoimagetag","text":"docker tag bbearce/docker_demo:from_docker_file bbearce/docker_demo:delete_me","title":"Tag existing image as a different name (repo/image:tag)"},{"location":"notes/docker/tutorial/#login-to-your-dockerhub-account","text":"docker login --username=bbearce","title":"Login to your dockerhub account"},{"location":"notes/docker/tutorial/#to-push-image-online","text":"docker push bbearce/docker_demo:from_docker_file","title":"To push image online"},{"location":"notes/docker/tutorial/#to-automatically-remove-container-when-exited","text":"docker run -it --rm --name=auto_remove bbearce/docker_demo:from_docker_file bash","title":"To automatically remove container when exited"},{"location":"notes/docker/tutorial/#use-bash-flag-v-to-mount-the-host-folder-inside-the-docker","text":"docker run -it --rm --name=auto_remove -v /home/bbearce/Desktop/Docker_Demo/mount:/app bbearce/docker_demo:from_docker_file bash","title":"Use bash flag -v : to mount the host folder inside the docker."},{"location":"notes/docker/tutorial/#from-a-dockerfile","text":"docker build -t bbearce/docker_demo:from_docker_file .","title":"from a Dockerfile"},{"location":"notes/fast.ai/introduction/","text":"Introduction Source: fast.ai Fastai is based on pyTorch and if Keras simplifies Tensorflow, Fastai simplifies pyTorch. Getting Started What is a GPU? GPUs (Graphics Processing Units) are specialized computer hardware originally created to render images at high frame rates (most commonly images in video games). Since graphics texturing and shading require more matrix and vector operations executed in parallel than a CPU (Central Processing Unit) can reasonably handle, GPUs were made to perform these calculations more efficiently. Why a GPU? It so happens that Deep Learning also requires super fast matrix computations. So researchers put two and two together and started training models in GPU\u2019s and the rest is history. Deep Learning really only cares about the number of Floating Point Operations (FLOPs) per second. GPUs are highly optimized for that. Try running this inside a Jupyter Notebook: Cell [1]: import torch t_cpu = torch.rand(500,500,500) %timeit t_cpu @ t_cpu 1 loop, best of 3: 1.78 s per loop Cell [2]: t_gpu = torch.rand(500,500,500).cuda() %timeit t_gpu @ t_gpu 1000 loops, best of 3: 15.4 ms per loop If you would like to train anything meaningful in deep learning, a GPU is what you need - specifically an NVIDIA GPU. Why NVIDIA? We recommend you to use an NVIDIA GPU since they are currently the best out there for a few reasons: Currently the fastest Native Pytorch support for CUDA Highly optimized for deep learning with cuDNN","title":"Introduction"},{"location":"notes/fast.ai/introduction/#introduction","text":"Source: fast.ai Fastai is based on pyTorch and if Keras simplifies Tensorflow, Fastai simplifies pyTorch.","title":"Introduction"},{"location":"notes/fast.ai/introduction/#getting-started","text":"","title":"Getting Started"},{"location":"notes/fast.ai/introduction/#what-is-a-gpu","text":"GPUs (Graphics Processing Units) are specialized computer hardware originally created to render images at high frame rates (most commonly images in video games). Since graphics texturing and shading require more matrix and vector operations executed in parallel than a CPU (Central Processing Unit) can reasonably handle, GPUs were made to perform these calculations more efficiently.","title":"What is a GPU?"},{"location":"notes/fast.ai/introduction/#why-a-gpu","text":"It so happens that Deep Learning also requires super fast matrix computations. So researchers put two and two together and started training models in GPU\u2019s and the rest is history. Deep Learning really only cares about the number of Floating Point Operations (FLOPs) per second. GPUs are highly optimized for that. Try running this inside a Jupyter Notebook: Cell [1]: import torch t_cpu = torch.rand(500,500,500) %timeit t_cpu @ t_cpu 1 loop, best of 3: 1.78 s per loop Cell [2]: t_gpu = torch.rand(500,500,500).cuda() %timeit t_gpu @ t_gpu 1000 loops, best of 3: 15.4 ms per loop If you would like to train anything meaningful in deep learning, a GPU is what you need - specifically an NVIDIA GPU. Why NVIDIA? We recommend you to use an NVIDIA GPU since they are currently the best out there for a few reasons: Currently the fastest Native Pytorch support for CUDA Highly optimized for deep learning with cuDNN","title":"Why a GPU?"},{"location":"notes/fast.ai/lessons_part_1/","text":"Lessons (Part 1) Source Notebook bbearce_colab Lesson 1 - Pets Keyboard shortcuts: Ctrl+m b makes a new code block Shift+Enter to run a cell and proceed to next Cntl+Enter runs a cell and does not proceed to next Key function to remember: untar_data is an unzip utility Transfer Learning Take a pre-trained model and then further learn for your specific task. In the course we downloaded \"resnet34-333f7ec4.pth\", which is a re-trained model and then further train it on cats and dogs. The point of this is that this model was previously trained on \"image net\" which is a giant repository of general images like planes, houses, animals and other objects so that the model is already pretty good at general classification. We will then use the fact that it already knows about \"animals\" to further train it to detect species of pet or other more specifc things. Key function to remember: get_image_files will get PosixPath file path-to/image-names.jpg... Key function to remember: get_image_files will get PosixPath file path-to/image-names.jpg... Train it After they setup the resnet model we atually kick offf the learn step: learn = cnn_learner(data, models.resnet34, metrics=error_rate) learn.model # prints the architecture learn.fit_one_cycle(4) When looking at the cycles 4 is a good start. Cycles and Epochs are the same thing. They are passes throught the data. Each pass the model gets more and more accurate. Here is an example: epoch train_loss valid_loss error_rate time 0 1.372660 0.296747 0.096076 01:33 1 0.605791 0.287795 0.092016 01:32 2 0.393741 0.237194 0.078484 01:32 3 0.277990 0.229392 0.066306 01:32 Save it Now we want to save the model, and this means save the weights or in a classical sense, the coefficients. learn.save('stage-1') Interpret it We need to look at the loss functions, which tell you how well did you predict what you tried to. Incorrect predictions have high losses and good predictions have low losses","title":"Lessons (Part 1)"},{"location":"notes/fast.ai/lessons_part_1/#lessons-part-1","text":"Source Notebook bbearce_colab","title":"Lessons (Part 1)"},{"location":"notes/fast.ai/lessons_part_1/#lesson-1-pets","text":"Keyboard shortcuts: Ctrl+m b makes a new code block Shift+Enter to run a cell and proceed to next Cntl+Enter runs a cell and does not proceed to next Key function to remember: untar_data is an unzip utility","title":"Lesson 1 - Pets"},{"location":"notes/fast.ai/lessons_part_1/#transfer-learning","text":"Take a pre-trained model and then further learn for your specific task. In the course we downloaded \"resnet34-333f7ec4.pth\", which is a re-trained model and then further train it on cats and dogs. The point of this is that this model was previously trained on \"image net\" which is a giant repository of general images like planes, houses, animals and other objects so that the model is already pretty good at general classification. We will then use the fact that it already knows about \"animals\" to further train it to detect species of pet or other more specifc things. Key function to remember: get_image_files will get PosixPath file path-to/image-names.jpg... Key function to remember: get_image_files will get PosixPath file path-to/image-names.jpg...","title":"Transfer Learning"},{"location":"notes/fast.ai/lessons_part_1/#train-it","text":"After they setup the resnet model we atually kick offf the learn step: learn = cnn_learner(data, models.resnet34, metrics=error_rate) learn.model # prints the architecture learn.fit_one_cycle(4) When looking at the cycles 4 is a good start. Cycles and Epochs are the same thing. They are passes throught the data. Each pass the model gets more and more accurate. Here is an example: epoch train_loss valid_loss error_rate time 0 1.372660 0.296747 0.096076 01:33 1 0.605791 0.287795 0.092016 01:32 2 0.393741 0.237194 0.078484 01:32 3 0.277990 0.229392 0.066306 01:32","title":"Train it"},{"location":"notes/fast.ai/lessons_part_1/#save-it","text":"Now we want to save the model, and this means save the weights or in a classical sense, the coefficients. learn.save('stage-1')","title":"Save it"},{"location":"notes/fast.ai/lessons_part_1/#interpret-it","text":"We need to look at the loss functions, which tell you how well did you predict what you tried to. Incorrect predictions have high losses and good predictions have low losses","title":"Interpret it"},{"location":"notes/git/git_basics/","text":"Git Basics Courtesy of rogerdudler.github.io Create a Repo Create a new repo with git init : bbearce@bbearce-XPS-15-9560:~/Desktop/git_practice$ git init Initialized empty Git repository in /home/bbearce/Desktop/git_practice/.git/ Checkout a Repo Checkout a repo with git clone : $ git clone /path/to/repository When using a remote server, your command will be: git clone username@host:/path/to/repository Workflow Your local repository consists of three \"trees\" maintained by git. the first one is your Working Directory which holds the actual files. The second one is the Index which acts as a staging area and finally the HEAD which points to the last commit you've made. Add and Commit You can propose changes (add it to the Index) using: git add <filename> or git add * This is the first step in the basic git workflow. To actually commit these changes use: git commit -m \"Commit message\" Now the file is committed to the HEAD , but not in your remote repository yet. git remote -v will tell you which remote you are connected to. SYNOPSIS git remote [-v | --verbose] ... OPTIONS -v, --verbose Be a little more verbose and show remote url after name. NOTE: This must be placed between remote and subcommand. Adding a New Remote Courtesy of articles.assembla.com To add a new remote, use the git remote add command on the terminal, in the directory your repository is stored at. The git remote add command takes two arguments: A remote name, for example, \u201corigin\u201d A remote URL, which you can find on the Source sub-tab of your Git repo #set a new remote git remote add origin git@git.assembla.com:portfolio/space.space_name.git #Verify new remote git remote -v origin git@git.assembla.com:portfolio/space.space_name.git (fetch) origin git@git.assembla.com:portfolio/space.space_name.git (push) Pushing Changes Your changes are now in the HEAD of your local working copy. To send those changes to your remote repository, execute: git push origin master Change master to whatever branch you want to push your changes to. If you have not cloned an existing repository and want to connect your repository to a remote server, you need to add it with: git remote add origin <server> Now you are able to push your changes to the selected remote server. Push Without User:Pass Courtesy of medium.com A way to skip typing my username/password when using https://github, is by changing the HTTPs origin remote which pointing to an HTTP url into an SSH url. For example: https url: https://github.com/<Username>/<Project>.git ssh url: git@github.com:<Username>/<Project>.git You can do: git remote set-url origin git@github.com:<Username>/<Project>.git to change the url. You need to have an ssh key pair generated and added to github Steps: [1]. Generate Key Pair Open Terminal. Paste the text below, substituting in your GitHub email address. $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Generating public/private rsa key pair. This creates a new ssh key, using the provided email as a label. When you're prompted to \"Enter a file in which to save the key,\" press Enter. This accepts the default file location. Enter a file in which to save the key (/home/you/.ssh/id_rsa): [Press enter] At the prompt, type a secure passphrase. For more information, see Working with SSH key passphrases . Enter passphrase (empty for no passphrase): [Type a passphrase]> Enter same passphrase again: [Type passphrase again] [2]. Add public key to git account or Add public key to git repo Possible Issues: If you get this error: ERROR: Permission to bbearce/code-journal.git denied to deploy key fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. You need to add your ssh key to the ssh-agent. Before adding a new SSH key to the ssh-agent to manage your keys, you should have checked for existing SSH keys and generated a new SSH key . If you have checked for existing SSH keys and find one you want to use, follow the below instructions. Start the ssh-agent in the background. $ eval \"$(ssh-agent -s)\" > Agent pid 59566 Add your SSH private key to the ssh-agent. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file. $ ssh-add ~/.ssh/<your_ssh_key> not the .pub file but the file with no extension. Add the SSH key to your GitHub account. Branching Branches are used to develop features isolated from each other. The master branch is the \"default\" branch when you create a repository. Use other branches for development and merge them back to the master branch upon completion. Create a new branch named \"feature_x\" and switch to it using: git checkout -b feature_x or git branch feature_x When switching to a new branch commit changes to master before switching. Also same for the new branch when switching to back to master. Switch back to master: git checkout master Delete the branch again: git branch -d feature_x A branch is not available to others unless you push the branch to your remote repository: git push origin <branch> Update and Merge To update your local repository to the newest commit, execute: git pull In your working directory to fetch and merge remote changes. To merge another branch into your active branch (e.g. master), use: git merge <branch> In both cases git tries to auto-merge changes. Unfortunately, this is not always possible and results in conflicts. You are responsible to merge those conflicts manually by editing the files shown by git. After changing, you need to mark them as merged with: git add <filename> Before merging changes, you can also preview them by using: git diff <source_branch> <target_branch> Tagging It's recommended to create tags for software releases. This is a known concept, which also exists in SVN. You can create a new tag named 1.0.0 by executing: git tag 1.0.0 1b2e1d63ff The 1b2e1d63ff stands for the first 10 characters of the commit id you want to reference with your tag. You can get the commit id by looking at the...log. Log In its simplest form, you can study repository history using: git log You can add a lot of parameters to make the log look like what you want. To see only the commits of a certain author: git log --author=bob To see a very compressed log where each commit is one line: git log --pretty=oneline Or maybe you want to see an ASCII art tree of all the branches, decorated with the names of tags and branches: git log --graph --oneline --decorate --all See only which files have changed: git log --name-status These are just a few of the possible parameters you can use. For more, see git log --help Replace Local Changes In case you did something wrong, which for sure never happens ;), you can replace local changes using the command: git checkout -- <filename> This replaces the changes in your working tree with the last content in HEAD . Changes already added to the index, as well as new files, will be kept. If you instead want to drop all your local changes and commits, fetch the latest history from the server and point your local master branch at it like this: git fetch origin git reset --hard origin/master Useful Hints If you want a built-in git GUI, use: gitk Use colorful git output: git config color.ui true To show log on just one line per commit, use: git config format.pretty oneline Use interactive adding: git add -i Clone Single Branch Courtesy of stackoverflow git clone -b <mybranch> --single-branch <git-url>","title":"Basics"},{"location":"notes/git/git_basics/#git-basics","text":"Courtesy of rogerdudler.github.io","title":"Git Basics"},{"location":"notes/git/git_basics/#create-a-repo","text":"Create a new repo with git init : bbearce@bbearce-XPS-15-9560:~/Desktop/git_practice$ git init Initialized empty Git repository in /home/bbearce/Desktop/git_practice/.git/","title":"Create a Repo"},{"location":"notes/git/git_basics/#checkout-a-repo","text":"Checkout a repo with git clone : $ git clone /path/to/repository When using a remote server, your command will be: git clone username@host:/path/to/repository","title":"Checkout a Repo"},{"location":"notes/git/git_basics/#workflow","text":"Your local repository consists of three \"trees\" maintained by git. the first one is your Working Directory which holds the actual files. The second one is the Index which acts as a staging area and finally the HEAD which points to the last commit you've made.","title":"Workflow"},{"location":"notes/git/git_basics/#add-and-commit","text":"You can propose changes (add it to the Index) using: git add <filename> or git add * This is the first step in the basic git workflow. To actually commit these changes use: git commit -m \"Commit message\" Now the file is committed to the HEAD , but not in your remote repository yet. git remote -v will tell you which remote you are connected to. SYNOPSIS git remote [-v | --verbose] ... OPTIONS -v, --verbose Be a little more verbose and show remote url after name. NOTE: This must be placed between remote and subcommand.","title":"Add and Commit"},{"location":"notes/git/git_basics/#adding-a-new-remote","text":"Courtesy of articles.assembla.com To add a new remote, use the git remote add command on the terminal, in the directory your repository is stored at. The git remote add command takes two arguments: A remote name, for example, \u201corigin\u201d A remote URL, which you can find on the Source sub-tab of your Git repo #set a new remote git remote add origin git@git.assembla.com:portfolio/space.space_name.git #Verify new remote git remote -v origin git@git.assembla.com:portfolio/space.space_name.git (fetch) origin git@git.assembla.com:portfolio/space.space_name.git (push)","title":"Adding a New Remote"},{"location":"notes/git/git_basics/#pushing-changes","text":"Your changes are now in the HEAD of your local working copy. To send those changes to your remote repository, execute: git push origin master Change master to whatever branch you want to push your changes to. If you have not cloned an existing repository and want to connect your repository to a remote server, you need to add it with: git remote add origin <server> Now you are able to push your changes to the selected remote server.","title":"Pushing Changes"},{"location":"notes/git/git_basics/#push-without-userpass","text":"Courtesy of medium.com A way to skip typing my username/password when using https://github, is by changing the HTTPs origin remote which pointing to an HTTP url into an SSH url. For example: https url: https://github.com/<Username>/<Project>.git ssh url: git@github.com:<Username>/<Project>.git You can do: git remote set-url origin git@github.com:<Username>/<Project>.git to change the url. You need to have an ssh key pair generated and added to github Steps: [1]. Generate Key Pair Open Terminal. Paste the text below, substituting in your GitHub email address. $ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" Generating public/private rsa key pair. This creates a new ssh key, using the provided email as a label. When you're prompted to \"Enter a file in which to save the key,\" press Enter. This accepts the default file location. Enter a file in which to save the key (/home/you/.ssh/id_rsa): [Press enter] At the prompt, type a secure passphrase. For more information, see Working with SSH key passphrases . Enter passphrase (empty for no passphrase): [Type a passphrase]> Enter same passphrase again: [Type passphrase again] [2]. Add public key to git account or Add public key to git repo","title":"Push Without User:Pass"},{"location":"notes/git/git_basics/#possible-issues","text":"If you get this error: ERROR: Permission to bbearce/code-journal.git denied to deploy key fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists. You need to add your ssh key to the ssh-agent. Before adding a new SSH key to the ssh-agent to manage your keys, you should have checked for existing SSH keys and generated a new SSH key . If you have checked for existing SSH keys and find one you want to use, follow the below instructions. Start the ssh-agent in the background. $ eval \"$(ssh-agent -s)\" > Agent pid 59566 Add your SSH private key to the ssh-agent. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file. $ ssh-add ~/.ssh/<your_ssh_key> not the .pub file but the file with no extension. Add the SSH key to your GitHub account.","title":"Possible Issues:"},{"location":"notes/git/git_basics/#branching","text":"Branches are used to develop features isolated from each other. The master branch is the \"default\" branch when you create a repository. Use other branches for development and merge them back to the master branch upon completion. Create a new branch named \"feature_x\" and switch to it using: git checkout -b feature_x or git branch feature_x When switching to a new branch commit changes to master before switching. Also same for the new branch when switching to back to master. Switch back to master: git checkout master Delete the branch again: git branch -d feature_x A branch is not available to others unless you push the branch to your remote repository: git push origin <branch>","title":"Branching"},{"location":"notes/git/git_basics/#update-and-merge","text":"To update your local repository to the newest commit, execute: git pull In your working directory to fetch and merge remote changes. To merge another branch into your active branch (e.g. master), use: git merge <branch> In both cases git tries to auto-merge changes. Unfortunately, this is not always possible and results in conflicts. You are responsible to merge those conflicts manually by editing the files shown by git. After changing, you need to mark them as merged with: git add <filename> Before merging changes, you can also preview them by using: git diff <source_branch> <target_branch>","title":"Update and Merge"},{"location":"notes/git/git_basics/#tagging","text":"It's recommended to create tags for software releases. This is a known concept, which also exists in SVN. You can create a new tag named 1.0.0 by executing: git tag 1.0.0 1b2e1d63ff The 1b2e1d63ff stands for the first 10 characters of the commit id you want to reference with your tag. You can get the commit id by looking at the...log.","title":"Tagging"},{"location":"notes/git/git_basics/#log","text":"In its simplest form, you can study repository history using: git log You can add a lot of parameters to make the log look like what you want. To see only the commits of a certain author: git log --author=bob To see a very compressed log where each commit is one line: git log --pretty=oneline Or maybe you want to see an ASCII art tree of all the branches, decorated with the names of tags and branches: git log --graph --oneline --decorate --all See only which files have changed: git log --name-status These are just a few of the possible parameters you can use. For more, see git log --help","title":"Log"},{"location":"notes/git/git_basics/#replace-local-changes","text":"In case you did something wrong, which for sure never happens ;), you can replace local changes using the command: git checkout -- <filename> This replaces the changes in your working tree with the last content in HEAD . Changes already added to the index, as well as new files, will be kept. If you instead want to drop all your local changes and commits, fetch the latest history from the server and point your local master branch at it like this: git fetch origin git reset --hard origin/master","title":"Replace Local Changes"},{"location":"notes/git/git_basics/#useful-hints","text":"If you want a built-in git GUI, use: gitk Use colorful git output: git config color.ui true To show log on just one line per commit, use: git config format.pretty oneline Use interactive adding: git add -i","title":"Useful Hints"},{"location":"notes/git/git_basics/#clone-single-branch","text":"Courtesy of stackoverflow git clone -b <mybranch> --single-branch <git-url>","title":"Clone Single Branch"},{"location":"notes/git/git_sub_trees/","text":"Subtrees Courtesty of github Typically, a subtree merge is used to contain a repository within a repository. The \"subrepository\" is stored in a folder of the main repository. The best way to explain subtree merges is to show by example. We will: Make an empty repository called test that represents our project Merge another repository into it as a subtree called Spoon-Knife . The test project will use that subproject as if it were part of the same repository. Fetch updates from Spoon-Knife into our test project. Setting up the empty repository for a subtree merge Open Terminal. Create a new directory and navigate to it. $ mkdir test $ cd test Initialize a new Git repository. $ git init > Initialized empty Git repository in /Users/octocat/tmp/test/.git/ Create and commit a new file. $ touch .gitignore $ git add .gitignore $ git commit -m \"initial commit\" > [master (root-commit) 3146c2a] initial commit > 0 files changed, 0 insertions(+), 0 deletions(-) > create mode 100644 .gitignore Adding a new repository as a subtree Add a new remote URL pointing to the separate project that we're interested in. $ git remote add -f spoon-knife git@github.com:octocat/Spoon-Knife.git > Updating spoon-knife > warning: no common commits > remote: Counting objects: 1732, done. > remote: Compressing objects: 100% (750/750), done. > remote: Total 1732 (delta 1086), reused 1558 (delta 967) > Receiving objects: 100% (1732/1732), 528.19 KiB | 621 KiB/s, done. > Resolving deltas: 100% (1086/1086), done. > From git://github.com/octocat/Spoon-Knife > * [new branch] master -> Spoon-Knife/master Merge the Spoon-Knife project into the local Git project. This doesn't change any of your files locally, but it does prepare Git for the next step. If you're using Git 2.9 or above: $ git merge -s ours --no-commit --allow-unrelated-histories spoon-knife/master > Automatic merge went well; stopped before committing as requested If you're using Git 2.8 or below: $ git merge -s ours --no-commit spoon-knife/master > Automatic merge went well; stopped before committing as requested Create a new directory called spoon-knife , and copy the Git history of the Spoon-Knife project into it. $ git read-tree --prefix=spoon-knife/ -u spoon-knife/master Commit the changes to keep them safe. $ git commit -m \"Subtree merged in spoon-knife\" > [master fe0ca25] Subtree merged in spoon-knife Although we've only added one subproject, any number of subprojects can be incorporated into a Git repository. Tip: If you create a fresh clone of the repository in the future, the remotes you've added will not be created for you. You will have to add them again using the git remote add command. Synchronizing with updates and changes When a subproject is added, it is not automatically kept in sync with the upstream changes. You will need to update the subproject with the following command: $ git pull -s subtree remotename branchname For the example above, this would be: $ git pull -s subtree spoon-knife master","title":"Subtrees"},{"location":"notes/git/git_sub_trees/#subtrees","text":"Courtesty of github Typically, a subtree merge is used to contain a repository within a repository. The \"subrepository\" is stored in a folder of the main repository. The best way to explain subtree merges is to show by example. We will: Make an empty repository called test that represents our project Merge another repository into it as a subtree called Spoon-Knife . The test project will use that subproject as if it were part of the same repository. Fetch updates from Spoon-Knife into our test project.","title":"Subtrees"},{"location":"notes/git/git_sub_trees/#setting-up-the-empty-repository-for-a-subtree-merge","text":"Open Terminal. Create a new directory and navigate to it. $ mkdir test $ cd test Initialize a new Git repository. $ git init > Initialized empty Git repository in /Users/octocat/tmp/test/.git/ Create and commit a new file. $ touch .gitignore $ git add .gitignore $ git commit -m \"initial commit\" > [master (root-commit) 3146c2a] initial commit > 0 files changed, 0 insertions(+), 0 deletions(-) > create mode 100644 .gitignore","title":"Setting up the empty repository for a subtree merge"},{"location":"notes/git/git_sub_trees/#adding-a-new-repository-as-a-subtree","text":"Add a new remote URL pointing to the separate project that we're interested in. $ git remote add -f spoon-knife git@github.com:octocat/Spoon-Knife.git > Updating spoon-knife > warning: no common commits > remote: Counting objects: 1732, done. > remote: Compressing objects: 100% (750/750), done. > remote: Total 1732 (delta 1086), reused 1558 (delta 967) > Receiving objects: 100% (1732/1732), 528.19 KiB | 621 KiB/s, done. > Resolving deltas: 100% (1086/1086), done. > From git://github.com/octocat/Spoon-Knife > * [new branch] master -> Spoon-Knife/master Merge the Spoon-Knife project into the local Git project. This doesn't change any of your files locally, but it does prepare Git for the next step. If you're using Git 2.9 or above: $ git merge -s ours --no-commit --allow-unrelated-histories spoon-knife/master > Automatic merge went well; stopped before committing as requested If you're using Git 2.8 or below: $ git merge -s ours --no-commit spoon-knife/master > Automatic merge went well; stopped before committing as requested Create a new directory called spoon-knife , and copy the Git history of the Spoon-Knife project into it. $ git read-tree --prefix=spoon-knife/ -u spoon-knife/master Commit the changes to keep them safe. $ git commit -m \"Subtree merged in spoon-knife\" > [master fe0ca25] Subtree merged in spoon-knife Although we've only added one subproject, any number of subprojects can be incorporated into a Git repository. Tip: If you create a fresh clone of the repository in the future, the remotes you've added will not be created for you. You will have to add them again using the git remote add command.","title":"Adding a new repository as a subtree"},{"location":"notes/git/git_sub_trees/#synchronizing-with-updates-and-changes","text":"When a subproject is added, it is not automatically kept in sync with the upstream changes. You will need to update the subproject with the following command: $ git pull -s subtree remotename branchname For the example above, this would be: $ git pull -s subtree spoon-knife master","title":"Synchronizing with updates and changes"},{"location":"notes/git/git_submodules/","text":"Submodules Courtesy of github Git Submodules basic explanation Why submodules? In Git you can add a submodule to a repository. This is basically a repository embedded in your main repository. This can be very useful. A couple of advantages of using submodules: You can separate the code into different repositories. Useful if you have a codebase with big components, you could make a component a submodule. This way you'll have a cleaner Git log (commits are specific to a certain component). You can add the submodule to multiple repositories. Useful if you have multiple repositories that share the same components. With this approach you can easily update those components in all the repositories that added them as a submodule. This is a lot more convienient than copy-pasting the code into the repositories. Basics When you add a submodule in Git, you don't add the code of the submodule to the main repository, you only add information about the submodule that is added to the main repository. This information describes which commit the submodule is pointing at. This way, the submodule's code won't automatically be updated if the submodule's repository is updated. This is good, because your code might not work with the latest commit of the submodule, it prevents unexpected behaviour. Adding a submodule You can add a submodule to a repository like this: git submodule add git@github.com:url_to/awesome_submodule.git path_to_awesome_submodule With default configuration, this will check out the code of the awesome_submodule.git repository to the path_to_awesome_submodule directory, and will add information to the main repository about this submodule, which contains the commit the submodule points to , which will be the current commit of the default branch (usually the master branch) at the time this command is executed. After this operation, if you do a git status you'll see two files in the Changes to be committed list: the .gitmodules file and the path to the submodule. When you commit and push these files you commit/push the submodule to the origin. Getting the submodule's code If a new submodule is created by one person, the other people in the team need to initiate this submodule. First you have to get the information about the submodule, this is retrieved by a normal git pull . If there are new submodules you'll see it in the output of git pull . Then you'll have to initiate them with: git submodule init This will pull all the code from the submodule and place it in the directory that it's configured to. If you've cloned a repository that makes use of submodules, you should also run this command to get the submodule's code. This is not automatically done by git clone . Pushing updates in the submodule The submodule is just a separate resository. If you want to make changes to it, you should make the changes in this repository and push them like in a regular Git repository (just execute the git commands in the submodule's directory). However, you should also let the main repository know that you've updated the submodule's repository, and make it use the latest commit of the repository of the submodule. Because if you make new commits inside a submodule, the main repository will still point to the old commit . So, if you want to have these changes in your main repository too, you should tell the main repository to use the latest commit of the submodule. Now how do you do this? So you've made changes in the submodule's repository and committed them in its repository. If you now do a git status in the main repository, you'll see that the submodule is in the list Changes not staged for commit and it has the text (modified content) behind it. This means that the code of the submodule is checked out on a different commit than the main repository is pointing to . To make the main repository point to this new commit, you just add this change with git add and then commit and push it. Keeping your submodules up-to-date If someone updated a submodule, the other team-members should update the code of their submodules. This is not automatically done by git pull , because with git pull it only retrieves the information that the submodule is pointing to another commit , but doesn't update the submodule's code . To update the code of your submodules, you should run: git submodule update What happens if you don't run this command? If you don't run this command, the code of your submodule is checked out to an old commit. When you do git status you will see the submodule in the Changes not staged for commit list with the text (modified content) behind it. This is not because you changed the submodule's code, but because its code is checked out to a different commit. So Git sees this as a change, but actually you just didn't update the submodule's code. So if you're working with submodules, don't forget to keep your submodules up-to-date. Making it easier for everyone It is sometimes annoying if you forget to initiate and update your submodules. Fortunately, there are some tricks to make it easier: git submodule update --init This will update the submodules, and if they're not initiated yet, will initiate them. You can also have submodules inside of submodules. In this case you'll want to update/initiate the submodules recursively: git submodule update --init --recursive This is a lot to type, so you can make an alias: git config --global alias.update '!git pull && git submodule update --init --recursive' Now whenever you execute git update , it will execute a git pull and a git submodule update --init --recursive , thus updating all the code in your project. Courtesy of github To remove a submodule you need to: Delete the relevant section from the .gitmodules file. Stage the .gitmodules changes git add .gitmodules Delete the relevant section from .git/config . Run git rm --cached path_to_submodule (no trailing slash). Run rm -rf .git/modules/path_to_submodule (no trailing slash). Commit git commit -m \"Removed submodule \" Delete the now untracked submodule files rm -rf path_to_submodule","title":"Submodules"},{"location":"notes/git/git_submodules/#submodules","text":"Courtesy of github Git Submodules basic explanation","title":"Submodules"},{"location":"notes/git/git_submodules/#why-submodules","text":"In Git you can add a submodule to a repository. This is basically a repository embedded in your main repository. This can be very useful. A couple of advantages of using submodules: You can separate the code into different repositories. Useful if you have a codebase with big components, you could make a component a submodule. This way you'll have a cleaner Git log (commits are specific to a certain component). You can add the submodule to multiple repositories. Useful if you have multiple repositories that share the same components. With this approach you can easily update those components in all the repositories that added them as a submodule. This is a lot more convienient than copy-pasting the code into the repositories.","title":"Why submodules?"},{"location":"notes/git/git_submodules/#basics","text":"When you add a submodule in Git, you don't add the code of the submodule to the main repository, you only add information about the submodule that is added to the main repository. This information describes which commit the submodule is pointing at. This way, the submodule's code won't automatically be updated if the submodule's repository is updated. This is good, because your code might not work with the latest commit of the submodule, it prevents unexpected behaviour.","title":"Basics"},{"location":"notes/git/git_submodules/#adding-a-submodule","text":"You can add a submodule to a repository like this: git submodule add git@github.com:url_to/awesome_submodule.git path_to_awesome_submodule With default configuration, this will check out the code of the awesome_submodule.git repository to the path_to_awesome_submodule directory, and will add information to the main repository about this submodule, which contains the commit the submodule points to , which will be the current commit of the default branch (usually the master branch) at the time this command is executed. After this operation, if you do a git status you'll see two files in the Changes to be committed list: the .gitmodules file and the path to the submodule. When you commit and push these files you commit/push the submodule to the origin.","title":"Adding a submodule"},{"location":"notes/git/git_submodules/#getting-the-submodules-code","text":"If a new submodule is created by one person, the other people in the team need to initiate this submodule. First you have to get the information about the submodule, this is retrieved by a normal git pull . If there are new submodules you'll see it in the output of git pull . Then you'll have to initiate them with: git submodule init This will pull all the code from the submodule and place it in the directory that it's configured to. If you've cloned a repository that makes use of submodules, you should also run this command to get the submodule's code. This is not automatically done by git clone .","title":"Getting the submodule's code"},{"location":"notes/git/git_submodules/#pushing-updates-in-the-submodule","text":"The submodule is just a separate resository. If you want to make changes to it, you should make the changes in this repository and push them like in a regular Git repository (just execute the git commands in the submodule's directory). However, you should also let the main repository know that you've updated the submodule's repository, and make it use the latest commit of the repository of the submodule. Because if you make new commits inside a submodule, the main repository will still point to the old commit . So, if you want to have these changes in your main repository too, you should tell the main repository to use the latest commit of the submodule. Now how do you do this? So you've made changes in the submodule's repository and committed them in its repository. If you now do a git status in the main repository, you'll see that the submodule is in the list Changes not staged for commit and it has the text (modified content) behind it. This means that the code of the submodule is checked out on a different commit than the main repository is pointing to . To make the main repository point to this new commit, you just add this change with git add and then commit and push it.","title":"Pushing updates in the submodule"},{"location":"notes/git/git_submodules/#keeping-your-submodules-up-to-date","text":"If someone updated a submodule, the other team-members should update the code of their submodules. This is not automatically done by git pull , because with git pull it only retrieves the information that the submodule is pointing to another commit , but doesn't update the submodule's code . To update the code of your submodules, you should run: git submodule update","title":"Keeping your submodules up-to-date"},{"location":"notes/git/git_submodules/#what-happens-if-you-dont-run-this-command","text":"If you don't run this command, the code of your submodule is checked out to an old commit. When you do git status you will see the submodule in the Changes not staged for commit list with the text (modified content) behind it. This is not because you changed the submodule's code, but because its code is checked out to a different commit. So Git sees this as a change, but actually you just didn't update the submodule's code. So if you're working with submodules, don't forget to keep your submodules up-to-date.","title":"What happens if you don't run this command?"},{"location":"notes/git/git_submodules/#making-it-easier-for-everyone","text":"It is sometimes annoying if you forget to initiate and update your submodules. Fortunately, there are some tricks to make it easier: git submodule update --init This will update the submodules, and if they're not initiated yet, will initiate them. You can also have submodules inside of submodules. In this case you'll want to update/initiate the submodules recursively: git submodule update --init --recursive This is a lot to type, so you can make an alias: git config --global alias.update '!git pull && git submodule update --init --recursive' Now whenever you execute git update , it will execute a git pull and a git submodule update --init --recursive , thus updating all the code in your project. Courtesy of github To remove a submodule you need to: Delete the relevant section from the .gitmodules file. Stage the .gitmodules changes git add .gitmodules Delete the relevant section from .git/config . Run git rm --cached path_to_submodule (no trailing slash). Run rm -rf .git/modules/path_to_submodule (no trailing slash). Commit git commit -m \"Removed submodule \" Delete the now untracked submodule files rm -rf path_to_submodule","title":"Making it easier for everyone"},{"location":"notes/html/html_template/","text":"HTML The basic html tree is as follows: <!DOCTYPE html> <html> <head> <title></title> </head> <body> </body> </html>","title":"HTML"},{"location":"notes/html/html_template/#html","text":"The basic html tree is as follows: <!DOCTYPE html> <html> <head> <title></title> </head> <body> </body> </html>","title":"HTML"},{"location":"notes/internet_of_things/General%20Notes/","text":"Notes About the Internet IP Address Types IPv4 IPv4 (Internet Protocol Version 4) is the fourth revision of the Internet Protocol (IP) used to to identify devices on a network through an addressing system. The Internet Protocol is designed for use in interconnected systems of packet-switched computer communication networks IPv4 uses a 32-bit address scheme allowing for a total of 2^32 addresses (just over 4 billion addresses). IPv6 A new Internet addressing system Internet Protocol version 6 (IPv6) is being deployed to fulfill the need for more Internet addresses. IPv6 (Internet Protocol Version 6) is also called IPng (Internet Protocol next generation) and it is the newest version of the Internet Protocol (IP) reviewed in the IETF standards committees to replace the current version of IPv4 (Internet Protocol Version 4). IPv6 addresses are 128-bit IP address written in hexadecimal and separated by colons. An example IPv6 address could be written like this: 3ffe:1900:4545:3:200:f8ff:fe21:67cf.","title":"General Notes"},{"location":"notes/internet_of_things/General%20Notes/#notes-about-the-internet","text":"","title":"Notes About the Internet"},{"location":"notes/internet_of_things/General%20Notes/#ip-address-types","text":"","title":"IP Address Types"},{"location":"notes/internet_of_things/General%20Notes/#ipv4","text":"IPv4 (Internet Protocol Version 4) is the fourth revision of the Internet Protocol (IP) used to to identify devices on a network through an addressing system. The Internet Protocol is designed for use in interconnected systems of packet-switched computer communication networks IPv4 uses a 32-bit address scheme allowing for a total of 2^32 addresses (just over 4 billion addresses).","title":"IPv4"},{"location":"notes/internet_of_things/General%20Notes/#ipv6","text":"A new Internet addressing system Internet Protocol version 6 (IPv6) is being deployed to fulfill the need for more Internet addresses. IPv6 (Internet Protocol Version 6) is also called IPng (Internet Protocol next generation) and it is the newest version of the Internet Protocol (IP) reviewed in the IETF standards committees to replace the current version of IPv4 (Internet Protocol Version 4). IPv6 addresses are 128-bit IP address written in hexadecimal and separated by colons. An example IPv6 address could be written like this: 3ffe:1900:4545:3:200:f8ff:fe21:67cf.","title":"IPv6"},{"location":"notes/internet_of_things/AWS/storage/","text":"Storage In my quest to get started with AWS, I noticed lots of different storage types. I'll make some quick notes about them here. EC2 \u2013 Elastic Compute Cloud \u2013 The VMs While not storage, EC2 instances are the basis for anything in AWS EBS \u2013 Elastic Block Storage - $$ - Can be scaled but upgrading costs money S3 \u2013 Simple Storage Service \u2013 files and objects - $$ EFS \u2013 Elastic File System - - $$$ Glacier \u2013 Low cost option for long term storage - $ AWS Snowball - Snowball is designed to make such transfers easy without incurring astronomical network usage fees. FSx for Lustre - High performance computing for fast processing of workloads. Integrates with S3 and you pay as you go. AWS Storage Gateway - Seamlessly links your on-premises environment to Amazon cloud storage Cloud Data Migration Services - A portfolio of services to help simplify and accelerate moving data of all types and sizes into and out of the AWS cloud AWS Backup - Backups","title":"AWS"},{"location":"notes/internet_of_things/AWS/storage/#storage","text":"In my quest to get started with AWS, I noticed lots of different storage types. I'll make some quick notes about them here. EC2 \u2013 Elastic Compute Cloud \u2013 The VMs While not storage, EC2 instances are the basis for anything in AWS EBS \u2013 Elastic Block Storage - $$ - Can be scaled but upgrading costs money S3 \u2013 Simple Storage Service \u2013 files and objects - $$ EFS \u2013 Elastic File System - - $$$ Glacier \u2013 Low cost option for long term storage - $ AWS Snowball - Snowball is designed to make such transfers easy without incurring astronomical network usage fees. FSx for Lustre - High performance computing for fast processing of workloads. Integrates with S3 and you pay as you go. AWS Storage Gateway - Seamlessly links your on-premises environment to Amazon cloud storage Cloud Data Migration Services - A portfolio of services to help simplify and accelerate moving data of all types and sizes into and out of the AWS cloud AWS Backup - Backups","title":"Storage"},{"location":"notes/internet_of_things/Azure/placeholder/","text":"Placeholder This will be filled in eventually: $ echo awesome stuff coming","title":"Azure"},{"location":"notes/internet_of_things/Azure/placeholder/#placeholder","text":"This will be filled in eventually: $ echo awesome stuff coming","title":"Placeholder"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/","text":"TLS Security Courtesy of cloudfare Summary What is Transport Layer Security (TLS)? Transport Layer Security, or TLS, is a widely adopted security protocol designed to facilitate privacy and data security for communications over the Internet. A primary use case of TLS is encrypting the communication between web applications and servers, such as web browsers loading a website. TLS can also be used to encrypt other communications such as email, messaging, and voice over IP (VOIP). In this article we will focus on the role of TLS in web application security. TLS was proposed by the Internet Engineering Task Force (IETF), an international standards organization, and the first version of the protocol was published in 1999. The most recent version is TLS 1.3, which was published in 2018. What\u2019s the difference between TLS and SSL? TLS evolved from a previous encryption protocol called Secure Socket Layer (SSL), which was developed by Netscape. TLS version 1.0 actually began development as SSL version 3.1, but the name of the protocol was changed before publication in order to indicate that it was no longer associated with Netscape. Because of this history, the terms TLS and SSL are sometimes used interchangeably. What\u2019s the difference between TLS and HTTPS? HTTPS is an implementation of TLS encryption on top of the HTTP protocol, which is used by all websites as well as some other web services. Any website that uses HTTPS is therefore employing TLS encryption. Why should you use TLS? TLS encryption can help protect web applications from attacks such as data breaches, and DDoS attacks. Additionally, TLS-protected HTTPS is quickly becoming a standard practice for websites. For example, the Google Chrome browser is cracking down on non-HTTPS sites, and everyday Internet users are starting to become more wary of websites that don\u2019t feature the HTTPS padlock icon. How does TLS work? TLS can be used on top of a transport-layer security protocol like TCP. There are three main components to TLS: Encryption, Authentication, and Integrity. Encryption: hides the data being transferred from third parties. Authentication: ensures that the parties exchanging information are who they claim to be. Integrity: verifies that the data has not been forged or tampered with. A TLS connection is initiated using a sequence known as the TLS handshake. The TLS handshake establishes a cypher suite for each communication session. The cypher suite is a set of algorithms that specifies details such as which shared encryption keys, or session keys, will be used for that particular session. TLS is able to set the matching session keys over an unencrypted channel thanks to a technology known as public key cryptography. The handshake also handles authentication, which usually consists of the server proving its identity to the client. This is done using public keys. Public keys are encryption keys that use one-way encryption, meaning that anyone can unscramble data encrypted with the private key to ensure its authenticity, but only the original sender can encrypt data with the private key. Once data is encrypted and authenticated, it is then signed with a message authentication code (MAC). The recipient can then verify the MAC to ensure the integrity of the data. This is kind of like the tamper-proof foil found on a bottle of aspirin; the consumer knows no one has tampered with their medicine because the foil is intact when they purchase it. How does TLS affect web application performance? Because of the complex process involved in setting up a TLS connection, some load time and computational power must be expended. The client and server must communicate back and forth several times before any data is transmitted, and that eats up precious milliseconds of load times for web applications, as well as some memory for both the client and the server. Thankfully there are technologies in place that help to mitigate the lag created by the TLS handshake. One is TLS False Start, which lets the server and client start transmitting data before the TLS handshake is complete. Another technology to speed up TLS is TLS Session Resumption, which allows clients and servers that have previously communicated to use an abbreviated handshake. These improvements have helped to make TLS a very fast protocol that shouldn\u2019t noticeably affect load times. As for the computational costs associated with TLS, they are mostly negligible by today\u2019s standards. For example, when Google moved their entire Gmail platform to HTTPS in 2010, there was no need for them to enable any additional hardware. The extra load on their servers as a result of TLS encryption was less than 1%. How to start implementing TLS on a website All Cloudflare users automatically have HTTPS protection from Cloudflare. Via Universal SSL, Cloudflare offers free TLS/SSL certificates to all users. Anyone who doesn't use Cloudflare will have to acquire an SSL certificate from a certificate authority, often for a fee, and install the certificate on their origin servers. For more on how TLS/SSL certificates work, see What is an SSL certificate? Get a CA Certificate Notes to self sign for testing, but in practice I still see the \"Not Secure\" when useing this method, though https will work: Source Useful Stackoverflow article discussing file extensions and protocols\\file types: Helpful! - from serverfault In summary, there are four different ways to present certificates and their components: PEM - Governed by RFCs, its used preferentially by open-source software. It can have a variety of extensions (.pem, .key, .cer, .cert, more) PKCS7 - An open standard used by Java and supported by Windows. Does not contain private key material. PKCS12 - A Microsoft private standard that was later defined in an RFC that provides enhanced security versus the plain-text PEM format. This can contain private key material. Its used preferentially by Windows systems, and can be freely converted to PEM format through use of openssl. DER - The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. Not routinely used very much outside of Windows. Also Helpful! Above link is from certbot, a CA, and has useful notes regarding locations of certificates once generated and definitions of each file type. The follows notes are for generating certificates from them. Generate a Self-Signed Certificate Use this method if you want to use HTTPS (HTTP over TLS) to secure your Apache HTTP or Nginx web server, and you do not require that your certificate is signed by a CA. This command creates a 2048-bit private key ( domain.key ) and a self-signed certificate ( domain.crt ) from scratch: openssl req \\ -newkey rsa:2048 -nodes -keyout domain.key \\ -x509 -days 365 -out domain.crt The -x509 option tells req to create a self-signed cerificate. The -days 365 option specifies that the certificate will be valid for 365 days. A temporary CSR is generated to gather information to associate with the certificate. Talks about the CSR and getting a real CA verified key pair. source a2hosting CA Authority: certbot Source SSH into the server SSH into the server running your HTTP website as a user with sudo privileges. Add Certbot PPA You'll need to add the Certbot PPA to your list of repositories. To do so, run the following commands on the command line on the machine: sudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository universe sudo add-apt-repository ppa:certbot/certbot sudo apt-get update Install Certbot Run this command on the command line on the machine to install Certbot. sudo apt-get install certbot Choose how you'd like to run Certbot Are you ok with temporarily stopping your website? Yes, my web server is not currently running on this machine. Stop your webserver, then run this command to get a certificate. Certbot will temporarily spin up a webserver on your machine. sudo certbot certonly --standalone This gives this output: bbearce@miccai2019:~$ sudo certbot certonly --standalone Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator standalone, Installer None Starting new HTTPS connection (1): acme-v02.api.letsencrypt.org Please enter in your domain name(s) (comma and/or space separated) (Enter 'c' to cancel): miccai2020.eastus.cloudapp.azure.com Cert not yet due for renewal You have an existing certificate that has exactly the same domains or certificate name you requested and isn't close to expiry. (ref: /etc/letsencrypt/renewal/miccai2020.eastus.cloudapp.azure.com.conf) What would you like to do? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: Keep the existing certificate for now 2: Renew & replace the cert (limit ~5 per 7 days) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 2 Renewing an existing certificate IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/privkey.pem Your cert will expire on 2020-08-02. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \"certbot renew\" - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le Notice where it put the cert: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/ bbearce@miccai2019:~$ sudo ls /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/ cert.pem chain.pem fullchain.pem privkey.pem README The README has this info: bbearce@miccai2019:~$ sudo cat /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/README This directory contains your keys and certificates. `privkey.pem` : the private key for your certificate. `fullchain.pem`: the certificate file used in most server software. `chain.pem` : used for OCSP stapling in Nginx >=1.3.7. `cert.pem` : will break many server configurations, and should not be used without reading further documentation (see link below). WARNING: DO NOT MOVE OR RENAME THESE FILES! Certbot expects these files to remain in this location in order to function properly! We recommend not moving these files. For more information, see the Certbot User Guide at https://certbot.eff.org/docs/using.html#where-are-my-certificates. Let's look inside just to get a feel for what is going on: fullchain cert and chain Note how the fullchain is really just the cert and chain together in one file. privkey If you need to keep my web server running. If you have a webserver that's already using port 80 and don't want to stop it while Certbot runs, run this command and follow the instructions in the terminal. sudo certbot certonly --webroot Install your certificate You'll need to install your new certificate in the configuration file for your webserver.","title":"TLS Security"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#tls-security","text":"Courtesy of cloudfare","title":"TLS Security"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#summary","text":"","title":"Summary"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#what-is-transport-layer-security-tls","text":"Transport Layer Security, or TLS, is a widely adopted security protocol designed to facilitate privacy and data security for communications over the Internet. A primary use case of TLS is encrypting the communication between web applications and servers, such as web browsers loading a website. TLS can also be used to encrypt other communications such as email, messaging, and voice over IP (VOIP). In this article we will focus on the role of TLS in web application security. TLS was proposed by the Internet Engineering Task Force (IETF), an international standards organization, and the first version of the protocol was published in 1999. The most recent version is TLS 1.3, which was published in 2018.","title":"What is Transport Layer Security (TLS)?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#whats-the-difference-between-tls-and-ssl","text":"TLS evolved from a previous encryption protocol called Secure Socket Layer (SSL), which was developed by Netscape. TLS version 1.0 actually began development as SSL version 3.1, but the name of the protocol was changed before publication in order to indicate that it was no longer associated with Netscape. Because of this history, the terms TLS and SSL are sometimes used interchangeably.","title":"What\u2019s the difference between TLS and SSL?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#whats-the-difference-between-tls-and-https","text":"HTTPS is an implementation of TLS encryption on top of the HTTP protocol, which is used by all websites as well as some other web services. Any website that uses HTTPS is therefore employing TLS encryption.","title":"What\u2019s the difference between TLS and HTTPS?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#why-should-you-use-tls","text":"TLS encryption can help protect web applications from attacks such as data breaches, and DDoS attacks. Additionally, TLS-protected HTTPS is quickly becoming a standard practice for websites. For example, the Google Chrome browser is cracking down on non-HTTPS sites, and everyday Internet users are starting to become more wary of websites that don\u2019t feature the HTTPS padlock icon.","title":"Why should you use TLS?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#how-does-tls-work","text":"TLS can be used on top of a transport-layer security protocol like TCP. There are three main components to TLS: Encryption, Authentication, and Integrity. Encryption: hides the data being transferred from third parties. Authentication: ensures that the parties exchanging information are who they claim to be. Integrity: verifies that the data has not been forged or tampered with. A TLS connection is initiated using a sequence known as the TLS handshake. The TLS handshake establishes a cypher suite for each communication session. The cypher suite is a set of algorithms that specifies details such as which shared encryption keys, or session keys, will be used for that particular session. TLS is able to set the matching session keys over an unencrypted channel thanks to a technology known as public key cryptography. The handshake also handles authentication, which usually consists of the server proving its identity to the client. This is done using public keys. Public keys are encryption keys that use one-way encryption, meaning that anyone can unscramble data encrypted with the private key to ensure its authenticity, but only the original sender can encrypt data with the private key. Once data is encrypted and authenticated, it is then signed with a message authentication code (MAC). The recipient can then verify the MAC to ensure the integrity of the data. This is kind of like the tamper-proof foil found on a bottle of aspirin; the consumer knows no one has tampered with their medicine because the foil is intact when they purchase it.","title":"How does TLS work?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#how-does-tls-affect-web-application-performance","text":"Because of the complex process involved in setting up a TLS connection, some load time and computational power must be expended. The client and server must communicate back and forth several times before any data is transmitted, and that eats up precious milliseconds of load times for web applications, as well as some memory for both the client and the server. Thankfully there are technologies in place that help to mitigate the lag created by the TLS handshake. One is TLS False Start, which lets the server and client start transmitting data before the TLS handshake is complete. Another technology to speed up TLS is TLS Session Resumption, which allows clients and servers that have previously communicated to use an abbreviated handshake. These improvements have helped to make TLS a very fast protocol that shouldn\u2019t noticeably affect load times. As for the computational costs associated with TLS, they are mostly negligible by today\u2019s standards. For example, when Google moved their entire Gmail platform to HTTPS in 2010, there was no need for them to enable any additional hardware. The extra load on their servers as a result of TLS encryption was less than 1%.","title":"How does TLS affect web application performance?"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#how-to-start-implementing-tls-on-a-website","text":"All Cloudflare users automatically have HTTPS protection from Cloudflare. Via Universal SSL, Cloudflare offers free TLS/SSL certificates to all users. Anyone who doesn't use Cloudflare will have to acquire an SSL certificate from a certificate authority, often for a fee, and install the certificate on their origin servers. For more on how TLS/SSL certificates work, see What is an SSL certificate?","title":"How to start implementing TLS on a website"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#get-a-ca-certificate","text":"Notes to self sign for testing, but in practice I still see the \"Not Secure\" when useing this method, though https will work: Source Useful Stackoverflow article discussing file extensions and protocols\\file types: Helpful! - from serverfault In summary, there are four different ways to present certificates and their components: PEM - Governed by RFCs, its used preferentially by open-source software. It can have a variety of extensions (.pem, .key, .cer, .cert, more) PKCS7 - An open standard used by Java and supported by Windows. Does not contain private key material. PKCS12 - A Microsoft private standard that was later defined in an RFC that provides enhanced security versus the plain-text PEM format. This can contain private key material. Its used preferentially by Windows systems, and can be freely converted to PEM format through use of openssl. DER - The parent format of PEM. It's useful to think of it as a binary version of the base64-encoded PEM file. Not routinely used very much outside of Windows. Also Helpful! Above link is from certbot, a CA, and has useful notes regarding locations of certificates once generated and definitions of each file type. The follows notes are for generating certificates from them.","title":"Get a CA Certificate"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#generate-a-self-signed-certificate","text":"Use this method if you want to use HTTPS (HTTP over TLS) to secure your Apache HTTP or Nginx web server, and you do not require that your certificate is signed by a CA. This command creates a 2048-bit private key ( domain.key ) and a self-signed certificate ( domain.crt ) from scratch: openssl req \\ -newkey rsa:2048 -nodes -keyout domain.key \\ -x509 -days 365 -out domain.crt The -x509 option tells req to create a self-signed cerificate. The -days 365 option specifies that the certificate will be valid for 365 days. A temporary CSR is generated to gather information to associate with the certificate.","title":"Generate a Self-Signed Certificate"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#talks-about-the-csr-and-getting-a-real-ca-verified-key-pair","text":"source a2hosting","title":"Talks about the CSR and getting a real CA verified key pair."},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#ca-authority-certbot","text":"Source","title":"CA Authority: certbot"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#ssh-into-the-server","text":"SSH into the server running your HTTP website as a user with sudo privileges.","title":"SSH into the server"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#add-certbot-ppa","text":"You'll need to add the Certbot PPA to your list of repositories. To do so, run the following commands on the command line on the machine: sudo apt-get update sudo apt-get install software-properties-common sudo add-apt-repository universe sudo add-apt-repository ppa:certbot/certbot sudo apt-get update","title":"Add Certbot PPA"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#install-certbot","text":"Run this command on the command line on the machine to install Certbot. sudo apt-get install certbot","title":"Install Certbot"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#choose-how-youd-like-to-run-certbot","text":"Are you ok with temporarily stopping your website? Yes, my web server is not currently running on this machine. Stop your webserver, then run this command to get a certificate. Certbot will temporarily spin up a webserver on your machine. sudo certbot certonly --standalone This gives this output: bbearce@miccai2019:~$ sudo certbot certonly --standalone Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator standalone, Installer None Starting new HTTPS connection (1): acme-v02.api.letsencrypt.org Please enter in your domain name(s) (comma and/or space separated) (Enter 'c' to cancel): miccai2020.eastus.cloudapp.azure.com Cert not yet due for renewal You have an existing certificate that has exactly the same domains or certificate name you requested and isn't close to expiry. (ref: /etc/letsencrypt/renewal/miccai2020.eastus.cloudapp.azure.com.conf) What would you like to do? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 1: Keep the existing certificate for now 2: Renew & replace the cert (limit ~5 per 7 days) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Select the appropriate number [1-2] then [enter] (press 'c' to cancel): 2 Renewing an existing certificate IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/privkey.pem Your cert will expire on 2020-08-02. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \"certbot renew\" - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let's Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le Notice where it put the cert: /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/ bbearce@miccai2019:~$ sudo ls /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/ cert.pem chain.pem fullchain.pem privkey.pem README The README has this info: bbearce@miccai2019:~$ sudo cat /etc/letsencrypt/live/miccai2020.eastus.cloudapp.azure.com/README This directory contains your keys and certificates. `privkey.pem` : the private key for your certificate. `fullchain.pem`: the certificate file used in most server software. `chain.pem` : used for OCSP stapling in Nginx >=1.3.7. `cert.pem` : will break many server configurations, and should not be used without reading further documentation (see link below). WARNING: DO NOT MOVE OR RENAME THESE FILES! Certbot expects these files to remain in this location in order to function properly! We recommend not moving these files. For more information, see the Certbot User Guide at https://certbot.eff.org/docs/using.html#where-are-my-certificates. Let's look inside just to get a feel for what is going on:","title":"Choose how you'd like to run Certbot"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#fullchain","text":"","title":"fullchain"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#cert-and-chain","text":"Note how the fullchain is really just the cert and chain together in one file.","title":"cert and chain"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#privkey","text":"If you need to keep my web server running. If you have a webserver that's already using port 80 and don't want to stop it while Certbot runs, run this command and follow the instructions in the terminal. sudo certbot certonly --webroot","title":"privkey"},{"location":"notes/internet_of_things/TLS%20Security/TLS%20Security/#install-your-certificate","text":"You'll need to install your new certificate in the configuration file for your webserver.","title":"Install your certificate"},{"location":"notes/javascript/basics/","text":"Basics All JavaScript examples use ES2015 syntax, which include: const and let instead of var Arrow functions ( d => d instead of function(d) { return d; } ) where appropriate Spread operators [... iterable] , chained expressions, maps, sets and promises Template strings literals, defined using backticks Iterable collections, such as maps and sets Arrays const colors = [\"red\", \"blue\", \"green\"]; const geocoords = [27.2345, 34.9937]; const numbers = [1,2,3,4,5,6]; const empty = []; Each array has a length property that returns the number of elements. It's very useful to iterate using the array index: for(let i = 0; i < colors.length; i++) { console.log(colors[i]); } You can also loop over the elements of an array using the of operator (introduced in ES2015), when you don't need the index: for(let color of colors) { console.log(color); } And you can use the forEach() method, which runs a function for each element and also allows access to the index , item and array inside the function: colors.forEach(function(i, color, colors) { console.log((i+1) + \": \" + color); }) You need to pass something into the funtion and most by default use index, item, array, but as you can see other more descriptive placeholders can be used. Multi-dimensional arrays are created in JavaScript as arrays of arrays: const points = [[200,300], [150,100], [100,300]]; You can retrieve individual items like this: const firstPoint = points[0]; const middleX = points[1][0]; JavaScript provides many ways to extract and insert data into an array. It's usually recommended to use methods whenever possible. The following table lists useful methods you can use on arrays. Some modify the array; others return new arrays and other types. The examples provided use the colors and numbers arrays as declared above. Try them out using your browser's JavaScript console: Method Description Example push(item) Modifies the array adding an item to the end. colors.push(\"yellow\"); // [\"red\", \"blue\", \"green\", \"yellow\"]; pop() Modifies the array, removing and returning the last item. const green = colors.pop(); // [\"red\", \"blue\"]; unshift(item) Modifies the array inserting an item at the beginning. colors.unshift(\"yellow\"); // [\"yellow\", \"red\", \"blue\", \"green\"]; shift() Modifies the array, removing and returning the first item. const red = colors.shift(); // [\"blue\", \"green\"]; splice(p, n, i) Modifies the array, starting at position p. Can be used to delete items, insert or replace. const s = numbers.splice(2,3); // s = [3,4,5] // numbers = [1,2,6] reverse() Modifies the array, reversing its order. numbers.reverse(); // [6,5,4,3,2,1] sort() Modifies the array sorting by string order (if no args) or by a comparator function. numbers.sort((a,b) => b \u2013 a); // numbers = [6,5,4,3,2,1] slice(b,e) Returns a shallow copy of the array between b and e. const mid = numbers.slice(2,4) // mid = [3,4] filter(function) Returns new array where the elements pass the test implemented by the function. const even = numbers.filter(n => n%2==0); // [2,4,6] find(function) Returns the first element that satisfies the test function const two = numbers.find(n => n%2==0); // 2 indexOf(item) Returns the index of the first occurrence of item in the array. const n = numbers.indexOf(3); // 4 includes(item) Returns true if an array contains item among its entries. const n = numbers.includes(3); // true lastIndexOf(item) Returns the index of the last occurrence of the item in the array. const n = colors.lastIndexOf(\"blue\"); // 1 concat(other) Returns a new array that merges the current array with another. const eight = numbers.concat([7,8]); // [1,2,3,4,5,6,7,8] join()join(delim) Returns a comma-separated string of the elements in the array (an optional delimiter may be used) const csv = numbers.join(); // \"1,2,3,4,5,6\"Copyconst conc = numbers.join(\"\"); // \"123456\" map(function) Returns new array with each element modified by function. const squares = numbers.map(n => n*n); // [1,4,9,16,25,36] reduce(function) Returns the result of an accumulation operation using the values in the array. const sum = numbers.reduce((a, n) => a + n); forEach(function) Executes the provided function once for each element in the array. const squares = []; numbers.forEach(n => squares.push(n*n) // squares = [1,4,9,16,26,36] Strings Strings are primitive types in JavaScript that can be created with single quotes or double quotes. There is no difference. It's only a matter of style. ES2015 introduced two new string features: template literals and multiline strings . Multiline strings can be created adding a backslash at the end of each line: const line = \"Multiline strings can be \\ reated adding a backslash \\ at the end of each line\"; Template literals are strings created with backticks . They allow the inclusion of JavaScript expressions inside ${} placeholders. The result is concatenated as a single string. const template = `The square root of 2 is ${Math.sqrt(2)}`; If you need to use a special character in a string, such as a double quote in a double quoted string, or a backslash, you need to precede it with a backslash: const s = \"This is a backslash \\\\ and this is a double quote \\\"\"; There are several methods for string manipulation. They all return new strings or other types. No methods modify the original strings. Method Description Example startsWith(s) Returns true if string starts with the string passed as a parameter const s = \"This is a test string\" const r = s.startsWith(\"This\"); // true endsWith(s) Returns true if string ends with the string passed as a parameter const s = \"This is a test string\" const r = s.endsWith(\"This\"); // false substring(s,e) Returns a substring between start (incl.) and end indexes (not incl.) const k = \"Aardvark\" const ardva = k.substring(1,6); split(regx)split(delim) Splits a string by a delimiter character or regular expression and returns an array const result = s.split(\" \"); // result = // [\"This\",\"is\",\"a\",\"test\",\"string\"] indexOf(s) Returns the index of the first occurrence of a substring const k = \"Aardvark\" const i = k.indexOf(\"ar\"); // i = 1 lastIndexOf(s) Returns the index of the last occurrence of a substring const k = \"Aardvark\" const i = k.lastIndexOf(\"ar\"); // i = 5 charAt(i) Returns char at index i. Also supported as \u2018string\u2019[i] const k = \"Aardvark\" const v = k.charAt(4); trim() Removes whitespace from both ends of a string. const text = \" data \" const r = data.trim(); // r = \"data\" match(regx) Returns an array as the result of matchin a regular expression against the string. const k = \"Aardvark\" const v = k.match(/[a-f]/g); // v = [\"a\", \"d\", \"a\"] replace(regx,r)replace(s,t) Returns new string replacing match of regexp applied to the string with replacement, or all occurrences of source string with a target string. const k = \"Aardvark\" const a = k.replace(/a/gi, 'u') // a = \"uurdvurk\" const b = k.replace('ardv', 'ntib') // b = \"Antibark\" Functions Functions are typically created in JavaScript using the function keyword, using one of the forms below: function f() { console.log('function1', this); } const g = function(name) { console.log('function ' + name, this); } f(); // calls f g('test'); // calls g() with a parameter The this keyword refers to the object that owns the function. If this code runs in a browser, and this is a top-level function created in the <script> block, the owner is the global window object. Any properties accessed via this refer to that object. const obj = {a: 5, b: 6} obj.method = function() { console.log('method', this) } obj.method() output: method {a: 5, b: 6, method: \u0192} Arrow functions were introduced in ES2015. They are much more compact and can lead to cleaner code, but the scope of this is no longer retained by the object. In the code below, it refers to the global window object. Code that uses this.a and this.b will not find any data in the object and will return undefined . obj.arrow = () => console.log('arrow', this) obj.arrow() output: arrow Window {parent: Window, opener: null, top: Window, length: 0, frames: Window, \u2026} Objects An object is an unordered collection of data. Values in an object are stored as key-value pairs. You can create an object by declaring a comma-separated list of key:value pairs within curly braces, or simply a pair of opening-closing curly braces if you want to start with an empty object: const color = {name: \"red\", code: \"ff0000\"}; const empty = {}; Objects can contain other objects and arrays, which can also contain objects. They can also contain functions (which have access to local properties and behave as methods): const city = {name: \"Sao Paulo\", location: {latitude: 23.555, longitude: 46.63}, airports: [\"SAO\",\"CGH\",\"GRU\",\"VCP\"]}; const circle = { x: 200, y: 100, r: 50, area: function() { return this.r * this.r * 3.14; } } A typical dataset used by a simple chart usually consists of an array of objects. var array2 = [ {continent: \"Asia\", areakm2: 43820000}, {continent: \"Europe\", areakm2: 10180000}, {continent: \"Africa\", areakm2: 30370000}, {continent: \"South America\", areakm2: 17840000}, {continent: \"Oceania\", areakm2: 9008500}, {continent: \"North America\", areakm2:24490000} ]; You can access the properties of an object using the dot operator or brackets containing the key as a string. You can run its methods using the dot operator: const latitude = city.location.latitude; const oceania = array2[4].continent; const code = color[\"code\"]; circle.r = 100; const area = circle.area(); You can also loop over the properties of an object: for(let key in color) { console.log(key + \": \" + color[key]); } Properties and functions can be added to objects. It's common to write code that declares an empty object in a global context so that operations in other contexts add data to it: coords = {lat:34.342352435, lng: 108.98375984} const map = {}; function getCoords(coords) { map.latitude = coords.lat; map.longitude = coords.lng; } console.log(map) output: {lat:34.342352435, lng: 108.98375984} Objects can also be created with a constructor. You can create an object that contains the current date/time using: const now = new Date(); JSON is a data format based on JavaScript objects. It has the same structure as JavaScript object but the property keys have to be placed within double quotes: {\"name\": \"Sao Paulo\", \"location\": {\"latitude\": 23.555, \"longitude\": 46.63}, \"airports\": [\"SAO\",\"CGH\",\"GRU\",\"VCP\"]}; To use a JSON string in JavaScript you have to parse it. Maps and Sets Besides arrays, ES2015 also introduced two new data structures: Map , an associative array with key-value pairs easier to use than simple objects, and Set , which doesn't allow repeated values. Both can be transformed to and from arrays. You can create a new Set object using: const set = new Set(); And add elements to it using: set.add(5); set.add(7); If you try to add elements that already exist, they won't be included in the set: set.add(1); set.add(5); // not added set.add(7); // not added console.log(set.size); // prints 3 You can check if a Set contains an element: console.log(set.has(3)); // false And convert a Set object into an array: const array1 = [... set]; // spread operator A Map can be more efficient than using an object to store key-value pairs in an associative array: const map = new Map(); map.set(\"a\", 123) map.set(\"b\", 456); console.log(map) output: Map(2) {\"a\" => 123, \"b\" => 456} You can then retrieve the value for each key using get(key): console.log(map.get(\"b\")) Or using iterative operations and the keys(), values() and entries() methods: for (let key of map.keys()) { console.log(key, map.get(key)) } for (let [key, value] of map.entries()) { console.log(key, value) } map.forEach ((k, v) => console.log(k, v)) Maps can be converted into arrays with the spread operator or Arrays.from(): const array2 = [... map.values()]; // an array of values const array3 = Array.from(map.values()); // an array of keys Promises Source Web Dev Simplified Define a promise. 1+1 is what we want the promise to do. The object itself accepts a function that has inputs resolve and reject . let p = new Promise((resolve, reject) => { let a = 1 + 1 // if you change of one these to 2 to make it fail you have to have the p.then().catch() code implemented of you will get a \"Uncaught (in promise) Failed\" message if (a == 2) { resolve('Success') } else { reject('Failed') } }) Interact with promises. the .then method runs if the promise resolves . If the promise rejects it runs the .catch method. p.then((message) => { console.log('This is in the then ' + message) }).catch ((message) => { console.log('This is in the catch '+ message) })","title":"Basics"},{"location":"notes/javascript/basics/#basics","text":"All JavaScript examples use ES2015 syntax, which include: const and let instead of var Arrow functions ( d => d instead of function(d) { return d; } ) where appropriate Spread operators [... iterable] , chained expressions, maps, sets and promises Template strings literals, defined using backticks Iterable collections, such as maps and sets","title":"Basics"},{"location":"notes/javascript/basics/#arrays","text":"const colors = [\"red\", \"blue\", \"green\"]; const geocoords = [27.2345, 34.9937]; const numbers = [1,2,3,4,5,6]; const empty = []; Each array has a length property that returns the number of elements. It's very useful to iterate using the array index: for(let i = 0; i < colors.length; i++) { console.log(colors[i]); } You can also loop over the elements of an array using the of operator (introduced in ES2015), when you don't need the index: for(let color of colors) { console.log(color); } And you can use the forEach() method, which runs a function for each element and also allows access to the index , item and array inside the function: colors.forEach(function(i, color, colors) { console.log((i+1) + \": \" + color); }) You need to pass something into the funtion and most by default use index, item, array, but as you can see other more descriptive placeholders can be used. Multi-dimensional arrays are created in JavaScript as arrays of arrays: const points = [[200,300], [150,100], [100,300]]; You can retrieve individual items like this: const firstPoint = points[0]; const middleX = points[1][0]; JavaScript provides many ways to extract and insert data into an array. It's usually recommended to use methods whenever possible. The following table lists useful methods you can use on arrays. Some modify the array; others return new arrays and other types. The examples provided use the colors and numbers arrays as declared above. Try them out using your browser's JavaScript console: Method Description Example push(item) Modifies the array adding an item to the end. colors.push(\"yellow\"); // [\"red\", \"blue\", \"green\", \"yellow\"]; pop() Modifies the array, removing and returning the last item. const green = colors.pop(); // [\"red\", \"blue\"]; unshift(item) Modifies the array inserting an item at the beginning. colors.unshift(\"yellow\"); // [\"yellow\", \"red\", \"blue\", \"green\"]; shift() Modifies the array, removing and returning the first item. const red = colors.shift(); // [\"blue\", \"green\"]; splice(p, n, i) Modifies the array, starting at position p. Can be used to delete items, insert or replace. const s = numbers.splice(2,3); // s = [3,4,5] // numbers = [1,2,6] reverse() Modifies the array, reversing its order. numbers.reverse(); // [6,5,4,3,2,1] sort() Modifies the array sorting by string order (if no args) or by a comparator function. numbers.sort((a,b) => b \u2013 a); // numbers = [6,5,4,3,2,1] slice(b,e) Returns a shallow copy of the array between b and e. const mid = numbers.slice(2,4) // mid = [3,4] filter(function) Returns new array where the elements pass the test implemented by the function. const even = numbers.filter(n => n%2==0); // [2,4,6] find(function) Returns the first element that satisfies the test function const two = numbers.find(n => n%2==0); // 2 indexOf(item) Returns the index of the first occurrence of item in the array. const n = numbers.indexOf(3); // 4 includes(item) Returns true if an array contains item among its entries. const n = numbers.includes(3); // true lastIndexOf(item) Returns the index of the last occurrence of the item in the array. const n = colors.lastIndexOf(\"blue\"); // 1 concat(other) Returns a new array that merges the current array with another. const eight = numbers.concat([7,8]); // [1,2,3,4,5,6,7,8] join()join(delim) Returns a comma-separated string of the elements in the array (an optional delimiter may be used) const csv = numbers.join(); // \"1,2,3,4,5,6\"Copyconst conc = numbers.join(\"\"); // \"123456\" map(function) Returns new array with each element modified by function. const squares = numbers.map(n => n*n); // [1,4,9,16,25,36] reduce(function) Returns the result of an accumulation operation using the values in the array. const sum = numbers.reduce((a, n) => a + n); forEach(function) Executes the provided function once for each element in the array. const squares = []; numbers.forEach(n => squares.push(n*n) // squares = [1,4,9,16,26,36]","title":"Arrays"},{"location":"notes/javascript/basics/#strings","text":"Strings are primitive types in JavaScript that can be created with single quotes or double quotes. There is no difference. It's only a matter of style. ES2015 introduced two new string features: template literals and multiline strings . Multiline strings can be created adding a backslash at the end of each line: const line = \"Multiline strings can be \\ reated adding a backslash \\ at the end of each line\"; Template literals are strings created with backticks . They allow the inclusion of JavaScript expressions inside ${} placeholders. The result is concatenated as a single string. const template = `The square root of 2 is ${Math.sqrt(2)}`; If you need to use a special character in a string, such as a double quote in a double quoted string, or a backslash, you need to precede it with a backslash: const s = \"This is a backslash \\\\ and this is a double quote \\\"\"; There are several methods for string manipulation. They all return new strings or other types. No methods modify the original strings. Method Description Example startsWith(s) Returns true if string starts with the string passed as a parameter const s = \"This is a test string\" const r = s.startsWith(\"This\"); // true endsWith(s) Returns true if string ends with the string passed as a parameter const s = \"This is a test string\" const r = s.endsWith(\"This\"); // false substring(s,e) Returns a substring between start (incl.) and end indexes (not incl.) const k = \"Aardvark\" const ardva = k.substring(1,6); split(regx)split(delim) Splits a string by a delimiter character or regular expression and returns an array const result = s.split(\" \"); // result = // [\"This\",\"is\",\"a\",\"test\",\"string\"] indexOf(s) Returns the index of the first occurrence of a substring const k = \"Aardvark\" const i = k.indexOf(\"ar\"); // i = 1 lastIndexOf(s) Returns the index of the last occurrence of a substring const k = \"Aardvark\" const i = k.lastIndexOf(\"ar\"); // i = 5 charAt(i) Returns char at index i. Also supported as \u2018string\u2019[i] const k = \"Aardvark\" const v = k.charAt(4); trim() Removes whitespace from both ends of a string. const text = \" data \" const r = data.trim(); // r = \"data\" match(regx) Returns an array as the result of matchin a regular expression against the string. const k = \"Aardvark\" const v = k.match(/[a-f]/g); // v = [\"a\", \"d\", \"a\"] replace(regx,r)replace(s,t) Returns new string replacing match of regexp applied to the string with replacement, or all occurrences of source string with a target string. const k = \"Aardvark\" const a = k.replace(/a/gi, 'u') // a = \"uurdvurk\" const b = k.replace('ardv', 'ntib') // b = \"Antibark\"","title":"Strings"},{"location":"notes/javascript/basics/#functions","text":"Functions are typically created in JavaScript using the function keyword, using one of the forms below: function f() { console.log('function1', this); } const g = function(name) { console.log('function ' + name, this); } f(); // calls f g('test'); // calls g() with a parameter The this keyword refers to the object that owns the function. If this code runs in a browser, and this is a top-level function created in the <script> block, the owner is the global window object. Any properties accessed via this refer to that object. const obj = {a: 5, b: 6} obj.method = function() { console.log('method', this) } obj.method() output: method {a: 5, b: 6, method: \u0192} Arrow functions were introduced in ES2015. They are much more compact and can lead to cleaner code, but the scope of this is no longer retained by the object. In the code below, it refers to the global window object. Code that uses this.a and this.b will not find any data in the object and will return undefined . obj.arrow = () => console.log('arrow', this) obj.arrow() output: arrow Window {parent: Window, opener: null, top: Window, length: 0, frames: Window, \u2026}","title":"Functions"},{"location":"notes/javascript/basics/#objects","text":"An object is an unordered collection of data. Values in an object are stored as key-value pairs. You can create an object by declaring a comma-separated list of key:value pairs within curly braces, or simply a pair of opening-closing curly braces if you want to start with an empty object: const color = {name: \"red\", code: \"ff0000\"}; const empty = {}; Objects can contain other objects and arrays, which can also contain objects. They can also contain functions (which have access to local properties and behave as methods): const city = {name: \"Sao Paulo\", location: {latitude: 23.555, longitude: 46.63}, airports: [\"SAO\",\"CGH\",\"GRU\",\"VCP\"]}; const circle = { x: 200, y: 100, r: 50, area: function() { return this.r * this.r * 3.14; } } A typical dataset used by a simple chart usually consists of an array of objects. var array2 = [ {continent: \"Asia\", areakm2: 43820000}, {continent: \"Europe\", areakm2: 10180000}, {continent: \"Africa\", areakm2: 30370000}, {continent: \"South America\", areakm2: 17840000}, {continent: \"Oceania\", areakm2: 9008500}, {continent: \"North America\", areakm2:24490000} ]; You can access the properties of an object using the dot operator or brackets containing the key as a string. You can run its methods using the dot operator: const latitude = city.location.latitude; const oceania = array2[4].continent; const code = color[\"code\"]; circle.r = 100; const area = circle.area(); You can also loop over the properties of an object: for(let key in color) { console.log(key + \": \" + color[key]); } Properties and functions can be added to objects. It's common to write code that declares an empty object in a global context so that operations in other contexts add data to it: coords = {lat:34.342352435, lng: 108.98375984} const map = {}; function getCoords(coords) { map.latitude = coords.lat; map.longitude = coords.lng; } console.log(map) output: {lat:34.342352435, lng: 108.98375984} Objects can also be created with a constructor. You can create an object that contains the current date/time using: const now = new Date(); JSON is a data format based on JavaScript objects. It has the same structure as JavaScript object but the property keys have to be placed within double quotes: {\"name\": \"Sao Paulo\", \"location\": {\"latitude\": 23.555, \"longitude\": 46.63}, \"airports\": [\"SAO\",\"CGH\",\"GRU\",\"VCP\"]}; To use a JSON string in JavaScript you have to parse it.","title":"Objects"},{"location":"notes/javascript/basics/#maps-and-sets","text":"Besides arrays, ES2015 also introduced two new data structures: Map , an associative array with key-value pairs easier to use than simple objects, and Set , which doesn't allow repeated values. Both can be transformed to and from arrays. You can create a new Set object using: const set = new Set(); And add elements to it using: set.add(5); set.add(7); If you try to add elements that already exist, they won't be included in the set: set.add(1); set.add(5); // not added set.add(7); // not added console.log(set.size); // prints 3 You can check if a Set contains an element: console.log(set.has(3)); // false And convert a Set object into an array: const array1 = [... set]; // spread operator A Map can be more efficient than using an object to store key-value pairs in an associative array: const map = new Map(); map.set(\"a\", 123) map.set(\"b\", 456); console.log(map) output: Map(2) {\"a\" => 123, \"b\" => 456} You can then retrieve the value for each key using get(key): console.log(map.get(\"b\")) Or using iterative operations and the keys(), values() and entries() methods: for (let key of map.keys()) { console.log(key, map.get(key)) } for (let [key, value] of map.entries()) { console.log(key, value) } map.forEach ((k, v) => console.log(k, v)) Maps can be converted into arrays with the spread operator or Arrays.from(): const array2 = [... map.values()]; // an array of values const array3 = Array.from(map.values()); // an array of keys","title":"Maps and Sets"},{"location":"notes/javascript/basics/#promises","text":"Source Web Dev Simplified Define a promise. 1+1 is what we want the promise to do. The object itself accepts a function that has inputs resolve and reject . let p = new Promise((resolve, reject) => { let a = 1 + 1 // if you change of one these to 2 to make it fail you have to have the p.then().catch() code implemented of you will get a \"Uncaught (in promise) Failed\" message if (a == 2) { resolve('Success') } else { reject('Failed') } }) Interact with promises. the .then method runs if the promise resolves . If the promise rejects it runs the .catch method. p.then((message) => { console.log('This is in the then ' + message) }).catch ((message) => { console.log('This is in the catch '+ message) })","title":"Promises"},{"location":"notes/javascript/D3js/links_to_checkout/","text":"Links For Tutorials That I Liked D3 Mapping tutrial","title":"Links To Checkout"},{"location":"notes/javascript/D3js/links_to_checkout/#links-for-tutorials-that-i-liked","text":"D3 Mapping tutrial","title":"Links For Tutorials That I Liked"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/","text":"References: D3.js documentation: d3js.org D3.js API reference: github.com/d3/d3/blob/master/API.md D3.js module repository: github.com/d3 D3.js gallery: github.com/d3/d3/wiki/Gallery D3.js wiki: github.com/d3/d3/wiki Bl.ocks portfolios: bl.ocks.org Observable notebooks: observablehq.com Source github What is D3? D3 Stands for Data Driven Documents. It is a javascript library but not a charting library. It focuses on the data. It will replace the DOM API and libraries such as jQuery. It uses data to drive everything. Data is first. \"This API is also used to bind and dispatch events, and to generate animated transitions. It can also parse different data formats, such as JSON and CSV, perform general data manipulation on objects and arrays. A typical D3.js script uses CSS selectors to select HTML or SVG elements and binds them to individual data items, removing, updating, or appending graphical elements automatically, when necessary. From the intro it seems you use CSS to create a selection object on a group of HTML tags and then bind data to them. You can also make a selection bound to data when the HTML doen't exist yet and have the HTML created automatically. Use https://d3js.org/ and click on any of the hexagon example to be taken to a tutorial. Two other platforms are https://www.bl.ocks.org and https://www.observablehq.com which contain interactive tutorials. Environment Setup npm $ npm install d3 From there in my home directory is: $ ls node_modules/d3/dist/ d3.js d3.min.js d3.node.js package.js Just source the min after copying it to your development directory as follows: <script src=\"js/d3/d3.v5.min.js\"></script> CDN link You can also just source it from the internet if you always have internet: <script src=\"https://d3js.org/d3.v5.min.js\"></script> Simple Web Server with node For simple examples using the file system to open the file is enough, but for larger ones that load external files, it is better to use a web server. $ npm install http-server -g Once installed move to the working directory and execute: $ http-server I had an issue running http-server out of the box and this thread fixed it. Ex: Create a sim link called node that points to nodejs bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ sudo ln -s /usr/bin/nodejs /usr/bin/node bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ http-server Starting up http-server, serving ./ Available on: http://127.0.0.1:8080 http://192.168.0.46:8080 Hit CTRL-C to stop the server Notice: bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ ls -la /usr/bin/node lrwxrwxrwx 1 root root 15 Feb 27 22:53 /usr/bin/node -> /usr/bin/nodejs bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ ls -la /usr/bin/nodejs -rwxr-xr-x 1 root root 11187096 Aug 9 2018 /usr/bin/nodejs Don't forget about codepen and jsfiddle as possible development environments Hello World Put this is the body tag and then the js in a separate doc and source or put it in a script tag. HTML <svg id=\"chart\" width=\"600\" height=\"200\"></svg> Constant const array = [100, 200, 300, 350, 375, 400, 500]; d3.select(\"#chart\") // select #chart .selectAll(\"circle\") // declares the elements we would like to create; Really this selects all circle elements in the <svg> tag but there are none so the selection is empty .data([100]) // sets data to drive element creation .enter() // creates a selection to add elements per data item .append(\"circle\") // appends an element of this type to each data item .attr(\"r\", \"10\") // sets \"r\" attribute .attr(\"cy\", 100) // sets \"cy\" attribute .attr(\"cx\", d => d) // sets \"cx\" attribute (same as function(d){ return d }); This is being set by the data; Binding (we changed [100] in .data() to array) const array = [100, 200, 300, 350, 375, 400, 500]; d3.select(\"#chart\") .selectAll(\"circle\") .data(array) // here we change [100] to array for multiple circles .enter() .append(\"circle\") .attr(\"r\", \"10\") .attr(\"cy\", 100) .attr(\"cx\", d => d) Update Now skip a line and add this code after the selection. After 2 seconds there is a 1/4 second transition. setTimeout(function() { d3.select(\"#chart\").selectAll(\"circle\") .data([50, 75, 125, 225, 325, 425, 450]) .attr(\"r\", 5) .attr(\"cx\", d => d) .style(\"fill\", \"red\") }, 2000) One very nice feature of D3 is how simple it is to animate transitions. You just need to chain a transition() command before the attributes and styles that changed. The default transition takes a quarter of a second. setTimeout(function() { d3.select(\"#chart\").selectAll(\"circle\") .data([50, 75, 125, 225, 325, 425, 450]) .transition().duration(1000) .attr(\"r\", 5) .attr(\"cx\", d => d) .style(\"fill\", \"red\") }, 2000) You'll notice the transition is much smoother now, taking time to switch to red dots. Modules (microlibraries) You can load select libraries like so: <script src=\"https://d3js.org/d3-selection.v1.min.js\"></script> <script src=\"https://d3js.org/d3-transition.v1.min.js\"></script> But if you need axes, maps, and other features, you will require more modules and dependencies. In this case, either use the default bundle, or set up a development environment where you can install each module using npm, since it automatically includes any dependencies. npm install module-name Data Manipulation The following modules listed are used to generate, manipulate, transform, parse, and format data, in the form of numbers, text, arrays, objects, and files. They are all included in the default d3.js bundle: Module Bundled (d3v5) Description d3-array Yes Several array utilities that extend the basic ES6 functions, optimized for use with datasets. Dependencies: none. d3-collection Yes Maps and sets optimized for use with datasets; functions for object collections and nesting data. Dependencies: none. d3-random Yes Random number generators. Dependencies: none. d3-dsv Yes Parser functions for delimiter-separated data. Dependencies: none. d3-interpolate Yes Several functions for interpolating numbers, colors, strings, and so on. Dependencies: d3-color. d3-scale Yes Generator functions to map data dimensions to graphical dimensions. Dependencies: d3-array, d3-collection, d3-format, d3-interpolate, d3-time-format, d3-time. d3-time Yes API for operations with time (intervals, ranges, and so on). Dependencies: none. d3-format Yes Locale-sensitive methods for number formatting. Dependencies: none. d3-time-format Yes Locale-sensitive methods for date/time formatting. Dependencies: d3-time. Documents Manipulation These are core modules in D3 used to select and manipulate HTML or SVG elements by providing a concise API to the DOM. With these modules, you can select and filter elements (using CSS selectors), create elements, append, insert, or remove from the DOM tree, add attributes and contents, change styles or classes, connect event handlers, and join data. Practically any D3 application uses at least d3-selection. Module Bundled (d3v5) Description d3-selection Yes Contains the essential DOM API for selection and manipulation of DOM elements. Dependencies: none. d3-selection-multi No Adds optional support for setting multiple attributes, styles, or properties in selections and transitions using an object syntax. Dependencies: d3-selection, d3-transition. There are more located here There are: colors ajax 2D geometry graphing API Spherical geometry and geometrical maps Layouts: node-link hierarchies, graphs, trees, networks, flow diagrams, tiles, and Voronoi diagrams","title":"Chapter 1"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#references","text":"D3.js documentation: d3js.org D3.js API reference: github.com/d3/d3/blob/master/API.md D3.js module repository: github.com/d3 D3.js gallery: github.com/d3/d3/wiki/Gallery D3.js wiki: github.com/d3/d3/wiki Bl.ocks portfolios: bl.ocks.org Observable notebooks: observablehq.com Source github","title":"References:"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#what-is-d3","text":"D3 Stands for Data Driven Documents. It is a javascript library but not a charting library. It focuses on the data. It will replace the DOM API and libraries such as jQuery. It uses data to drive everything. Data is first. \"This API is also used to bind and dispatch events, and to generate animated transitions. It can also parse different data formats, such as JSON and CSV, perform general data manipulation on objects and arrays. A typical D3.js script uses CSS selectors to select HTML or SVG elements and binds them to individual data items, removing, updating, or appending graphical elements automatically, when necessary. From the intro it seems you use CSS to create a selection object on a group of HTML tags and then bind data to them. You can also make a selection bound to data when the HTML doen't exist yet and have the HTML created automatically. Use https://d3js.org/ and click on any of the hexagon example to be taken to a tutorial. Two other platforms are https://www.bl.ocks.org and https://www.observablehq.com which contain interactive tutorials.","title":"What is D3?"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#environment-setup","text":"","title":"Environment Setup"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#npm","text":"$ npm install d3 From there in my home directory is: $ ls node_modules/d3/dist/ d3.js d3.min.js d3.node.js package.js Just source the min after copying it to your development directory as follows: <script src=\"js/d3/d3.v5.min.js\"></script>","title":"npm"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#cdn-link","text":"You can also just source it from the internet if you always have internet: <script src=\"https://d3js.org/d3.v5.min.js\"></script>","title":"CDN link"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#simple-web-server-with-node","text":"For simple examples using the file system to open the file is enough, but for larger ones that load external files, it is better to use a web server. $ npm install http-server -g Once installed move to the working directory and execute: $ http-server I had an issue running http-server out of the box and this thread fixed it. Ex: Create a sim link called node that points to nodejs bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ sudo ln -s /usr/bin/nodejs /usr/bin/node bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ http-server Starting up http-server, serving ./ Available on: http://127.0.0.1:8080 http://192.168.0.46:8080 Hit CTRL-C to stop the server Notice: bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ ls -la /usr/bin/node lrwxrwxrwx 1 root root 15 Feb 27 22:53 /usr/bin/node -> /usr/bin/nodejs bbearce@bbearce-XPS-15-9560:~/Desktop/D3_tutorials$ ls -la /usr/bin/nodejs -rwxr-xr-x 1 root root 11187096 Aug 9 2018 /usr/bin/nodejs Don't forget about codepen and jsfiddle as possible development environments","title":"Simple Web Server with node"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#hello-world","text":"Put this is the body tag and then the js in a separate doc and source or put it in a script tag. HTML <svg id=\"chart\" width=\"600\" height=\"200\"></svg>","title":"Hello World"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#constant","text":"const array = [100, 200, 300, 350, 375, 400, 500]; d3.select(\"#chart\") // select #chart .selectAll(\"circle\") // declares the elements we would like to create; Really this selects all circle elements in the <svg> tag but there are none so the selection is empty .data([100]) // sets data to drive element creation .enter() // creates a selection to add elements per data item .append(\"circle\") // appends an element of this type to each data item .attr(\"r\", \"10\") // sets \"r\" attribute .attr(\"cy\", 100) // sets \"cy\" attribute .attr(\"cx\", d => d) // sets \"cx\" attribute (same as function(d){ return d }); This is being set by the data;","title":"Constant"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#binding","text":"(we changed [100] in .data() to array) const array = [100, 200, 300, 350, 375, 400, 500]; d3.select(\"#chart\") .selectAll(\"circle\") .data(array) // here we change [100] to array for multiple circles .enter() .append(\"circle\") .attr(\"r\", \"10\") .attr(\"cy\", 100) .attr(\"cx\", d => d)","title":"Binding"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#update","text":"Now skip a line and add this code after the selection. After 2 seconds there is a 1/4 second transition. setTimeout(function() { d3.select(\"#chart\").selectAll(\"circle\") .data([50, 75, 125, 225, 325, 425, 450]) .attr(\"r\", 5) .attr(\"cx\", d => d) .style(\"fill\", \"red\") }, 2000) One very nice feature of D3 is how simple it is to animate transitions. You just need to chain a transition() command before the attributes and styles that changed. The default transition takes a quarter of a second. setTimeout(function() { d3.select(\"#chart\").selectAll(\"circle\") .data([50, 75, 125, 225, 325, 425, 450]) .transition().duration(1000) .attr(\"r\", 5) .attr(\"cx\", d => d) .style(\"fill\", \"red\") }, 2000) You'll notice the transition is much smoother now, taking time to switch to red dots.","title":"Update"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#modules-microlibraries","text":"You can load select libraries like so: <script src=\"https://d3js.org/d3-selection.v1.min.js\"></script> <script src=\"https://d3js.org/d3-transition.v1.min.js\"></script> But if you need axes, maps, and other features, you will require more modules and dependencies. In this case, either use the default bundle, or set up a development environment where you can install each module using npm, since it automatically includes any dependencies. npm install module-name","title":"Modules (microlibraries)"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#data-manipulation","text":"The following modules listed are used to generate, manipulate, transform, parse, and format data, in the form of numbers, text, arrays, objects, and files. They are all included in the default d3.js bundle: Module Bundled (d3v5) Description d3-array Yes Several array utilities that extend the basic ES6 functions, optimized for use with datasets. Dependencies: none. d3-collection Yes Maps and sets optimized for use with datasets; functions for object collections and nesting data. Dependencies: none. d3-random Yes Random number generators. Dependencies: none. d3-dsv Yes Parser functions for delimiter-separated data. Dependencies: none. d3-interpolate Yes Several functions for interpolating numbers, colors, strings, and so on. Dependencies: d3-color. d3-scale Yes Generator functions to map data dimensions to graphical dimensions. Dependencies: d3-array, d3-collection, d3-format, d3-interpolate, d3-time-format, d3-time. d3-time Yes API for operations with time (intervals, ranges, and so on). Dependencies: none. d3-format Yes Locale-sensitive methods for number formatting. Dependencies: none. d3-time-format Yes Locale-sensitive methods for date/time formatting. Dependencies: d3-time.","title":"Data Manipulation"},{"location":"notes/javascript/D3js/Rocha/chapter_1/chapter-1/#documents-manipulation","text":"These are core modules in D3 used to select and manipulate HTML or SVG elements by providing a concise API to the DOM. With these modules, you can select and filter elements (using CSS selectors), create elements, append, insert, or remove from the DOM tree, add attributes and contents, change styles or classes, connect event handlers, and join data. Practically any D3 application uses at least d3-selection. Module Bundled (d3v5) Description d3-selection Yes Contains the essential DOM API for selection and manipulation of DOM elements. Dependencies: none. d3-selection-multi No Adds optional support for setting multiple attributes, styles, or properties in selections and transitions using an object syntax. Dependencies: d3-selection, d3-transition. There are more located here There are: colors ajax 2D geometry graphing API Spherical geometry and geometrical maps Layouts: node-link hierarchies, graphs, trees, networks, flow diagrams, tiles, and Voronoi diagrams","title":"Documents Manipulation"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/","text":"Source github Technical Fundamentals This chapter covers fundamental standard web technologies used by D3: SVG, JavaScript (ES 2015), HTML Canvas and standard data formats such as JSON and CSV. It is intended as a general reference to these topics. Most data visualizations created with D3.js generate SVG graphics. Good knowledge of SVG is important to make the most of D3, but you only really need to know the basics. 1) SVG SVG stands for Scalable Vector Graphics. It\u2019s an XML-based image format that describes graphics using geometrical attributes. Unlike HTML5 Canvas, which is another standard for vector graphics, SVG primitives are made of individual XML elements described using tags and attributes. It is also object-based and provides a DOM, which allows CSS styling, dynamic shape creation and manipulation, and coordinate transforms using JavaScript or CSS. Scalable Vector Graphics (SVG) SVG stands for Scalable Vector Graphics. It\u2019s an XML-based image format that describes graphics using geometrical attributes. Unlike HTML5 Canvas, which is another standard for vector graphics, SVG primitives are made of individual XML elements described using tags and attributes. It is also object-based and provides a DOM, which allows CSS styling, dynamic shape creation and manipulation, and coordinate transforms using JavaScript or CSS. To control SVG elements with D3 you should understand basic SVG syntax and rules, how a document is structured, how each element is rendered, the effects caused by attributes and styles, as well as nesting and transformation rules. All the code used in this section is available in the SVG/ folder, from the GitHub repository for this chapter. You can see the results simply loading the pages in your browser. Viewport SVG graphics context (viewport) When SVG is embedded in HTML it creates a viewport. Ex (note the light grey style): <style> svg { border: solid 1px lightgray; background-color: hsla(240,100%,50%,0.2) } </style> <body> <h2>SVG viewport</h2> <svg width=\"600\" height=\"300\"></svg> </body> You can also create an SVG element using the DOM API, or D3, which is much simpler. The result is identical: <body> <script> d3.select(\"body\").append(\"svg\").attr(\"width\", 400).attr(\"height\", 300); </script> </body> Shapes Shapes are positioned in the viewport using x and y coordinates. They described by XML tags like <rect> , <circle> , <ellipse> , <path> , <polygon> and others. You create SVG graphics by placing these tags inside the <svg> element. Each supports attributes that configure their position in the viewport, and specific properties for each shape, such as radii, vertices or dimensions. A circle can be drawn in SVG using the <circle> element and at least the r attribute (radius). If you don't provide any other attributes, you will only see the lower-right quarter of the circle, since the default coordinates for its center will be (0,0). To illustrate an important js and raw html equivalency we will show code to draw 4 circles in the exact same way: HTML: <svg width=\"400\" height=\"300\"> <circle r=\"25\"></circle> <circle cx=\"250\" cy=\"200\" r=\"50\"></circle> <circle cx=\"50\" cy=\"50\" r=\"20\"></circle> <circle cx=\"400\" cy=\"300\" r=\"50\"></circle> </svg> JS: const svg = d3.select(\"body\") .append(\"svg\") .attr(\"width\", 400) .attr(\"height\", 300); svg.append(\"circle\").attr(\"r\", 25); svg.append(\"circle\").attr(\"cx\", 250).attr(\"cy\", 200).attr(\"r\", 50); svg.append(\"circle\").attr(\"cx\", 50).attr(\"cy\", 50).attr(\"r\", 20); svg.append(\"circle\").attr(\"cx\", 400).attr(\"cy\", 300).attr(\"r\", 50); Fills and Strokes Shapes have default black fill colors and transparent stroke borders, unless you assign different color strings to the SVG attributes (or CSS properties) fill and stroke. In the following SVG, three straight lines were drawn using the mandatory x1/y1 and x2/y2 attributes for elements. They would be invisible it the stroke attribute wasn't present. A stroke-width has a default value of 1. <svg width=\"400\" height=\"300\"> <line x2=\"400\" stroke=\"red\" stroke-width=\"5\"/> <line y2=\"150\" stroke=\"blue\" stroke-width=\"5\"/> <line x2=\"200\" y2=\"150\" stroke=\"black\" stroke-width=\"1\"/> </svg> You can also style with css: line { stroke-width: 20px; /* Overrides attr */ } Note lines at edge of screen might appear thinner as half the width could be off screen You can also apply CSS class and style attributes to different SVG objects. For example, consider the following CSS class declarations: .reds { fill: red; } .semitr { fill-opacity: 0.5; } Since the following rectangles each belong to one or more of these classes, they will inherit the style properties declared for each class: <svg width=\"600\" height=\"200\"> <rect x=\"50\" y=\"50\" width=\"90\" height=\"90\" class=\"semitr\"/> <rect x=\"200\" y=\"50\" width=\"175\" height=\"100\" rx=\"40\" ry=\"40\" class=\"reds semitr\"/> <rect x=\"450\" y=\"25\" width=\"100\" height=\"150\" class=\"reds\"/> </svg> One class applies the red fill, the other applies 50% transparency. The rectangle in the middle belongs to both classes, so it's both red and semi-transparent, as follows: Transparency Transparency When objects overlap in SVG, the code order determines which element will appear over the other. Preceding siblings are always overlapped by siblings that are declared after them. The CSS z-index property doesn\u2019t work in SVG. To move an object to the front, you have to modify the DOM tree. You can see through objects that overlap by changing their transparency. You can apply opacity levels to fills and strokes separately, using fill-opacity or stroke-opacity, or for the entire object using opacity. All attributes require a value between 0 (invisible) and 1 (opaque). An alternative, which achieves the same result, is to use the alpha component in rgba or hsla color strings (for example, \u2018rgba(255, 0, 0, 0.5)\u2019). The three squares below apply different transparency parameters on fills and strokes: <rect x=\"50\" y=\"50\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" stroke=\"red\" stroke-width=\"10\" fill-opacity=\"0\"/> <rect x=\"75\" y=\"75\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" fill=\"gray\" stroke=\"black\" stroke-width=\"10\" fill-opacity=\".7\"/> <rect x=\"100\" y=\"100\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" fill=\"yellow\" stroke=\"blue\" stroke-width=\"10\" stroke-opacity=\".6\"/> In the following code, two thin vertical rectangles appear behind a wide horizontal rectangle, and two other vertical rectangles appear in front of it: <rect x=\"300\" y=\"50\" height=\"150\" width=\"25\"/> <rect x=\"400\" y=\"50\" height=\"150\" width=\"25\" fill-opacity=\".5\"/> <rect x=\"250\" y=\"100\" height=\"50\" width=\"300\" fill=\"red\" stroke=\"orange\" stroke-width=\"10\" stroke-opacity=\".5\" /> <rect x=\"350\" y=\"50\" height=\"150\" width=\"25\"/> <rect x=\"450\" y=\"50\" height=\"150\" width=\"25\" fill-opacity=\".5\"/> The following image shows the result: Drawing rectangles with different fills, strokes and transparency. Code: 7-rect-opacity.html. Ellipses Ellipses have a center in cx and cy like circles, but also have two radii, which are set with rx and ry. The following code creates three ellipses in different positions. <svg width=\"600\" height=\"300\"> <ellipse cx=\"150\" cy=\"100\" rx=\"150\" ry=\"100\" fill=\"yellow\" fill-opacity=\"0.5\" stroke=\"blue\" stroke-width=\"1\" stroke-dasharray=\"5 5\"/> <ellipse cx=\"400\" cy=\"150\" rx=\"75\" ry=\"125\" fill=\"red\" fill-opacity=\"0.2\" stroke=\"red\" stroke-width=\"5\" stroke-opacity=\"0.5\"/> <ellipse cx=\"400\" cy=\"250\" rx=\"150\" ry=\"40\" fill=\"black\" fill-opacity=\"0\" stroke=\"green\" stroke-width=\"20\" stroke-opacity=\"0.2\" /> </svg> The preceding code produces the following result: Ellipses created with SVG. Code: SVG/8-ellipse.html. Polygons and polylines Polygons ( <polygon> ) and polylines ( <polyline> ) render closed or open shapes using straight lines specified by a list of vertices passed in the points attribute. The only difference between them is that polygons close the shape. To close a polyline you need to repeat the initial coordinates. They also have a fill-rule attribute that controls the winding order, and determines if a hole will be drawn inside the shape when an outline crosses with itself. The following code creates two polygons. The second one has a fill-rule that will reveal a hole: <svg width=\"600\" height=\"300\"> <polygon points=\"150,150 50,150 100,20 150,50 200,200 50,200 20,154 48,82 32,20\" fill=\"blue\"/> <polygon points=\"450,150 350,150 400,20 450,50 500,200 350,200 320,154 348,82 332,20\" fill=\"red\" fill-rule=\"evenodd\"/> </svg> The result is shown as follows: Polygons created with SVG. The second polygon uses fill-rule=\u201deven-odd\u201d and reveals a hole. Code: SVG/10-polygon. Paths With paths you can draw open and closed shapes mixing lines, curves and arcs using a compact drawing language in the d attribute of the <path> element. It can be used to create arbitrary paths mixing straight lines and curves: <path d=\"M100,200 C200,50 300,100 300,200 L400,250 500,100\" fill=\"yellow\" stroke=\"red\" stroke-width=\"4\"/> Don't worry about all those numbers and letters in the d attribute. It's the most important part of the path, but it can always be generated for you. Most of the shape generators you will use in D3 to create lines, pie slices and other arbitrary shapes generate path strings that you can use in the d attribute. The simple <path> above renders the image below (the dots are added separately and show the control points): A curve described by a element. Code: SVG/11-path-line.html. Text Unlike HTML, you can't simply insert text inside any element. You have to create text objects using the <text> element with text contents. You can position text using x and y attributes, but you must remember that y is actually the baseline (default). If y is zero or not present, only the parts of the text that extend below the baseline will be visible inside the graphics context. This example places both text and a rectangle in the same position: <rect x=\"0\" y=\"0\" height=\"36\" width=\"200\"/> <text font-size=\"36\" x=\"0\" y=\"0\" fill=\"lightgray\">ghijklmnop</text> The following illustration shows the result, at left. Note that only the parts of the text that extend below the baseline actually appear over the rectangle. The other two examples show text with a different baseline alignment: alignment-baseline=\"middle\" and alignment-baseline=\"hanging\". Drawing text in SVG and the baseline. Code: SVG/14-text.html. You can also align text horizontally using the text-anchor attribute. The following illustration describes properties and values used to align text horizontally or vertically. If you intend to rotate text relative to its position, these parameters will affect the result. Properties for aligning text. Code: SVG/15-text-align.html. It's best to configure baselines and alignments in CSS instead of using XML attributes. If your text spans multiple lines, you can use the <tspan> element inside <text> to move words or letters to positions relative to the parent <text> element (see SVG/16-tspan.html). Group containers You can group several shapes in SVG with the <g> element. It's analogous to a <div> in HTML. This is an invisible element and it's positioned at the center of coordinates. CSS properties applied to a group affect all the objects it contains. You can also apply matrix transforms to groups to move, rotate and scale all its contents. A group container can also contain other group containers. In the following SVG, circles and ellipses are in a group, and rectangles are in another. The color of the elements in each group is declared in CSS, and they are translated, scaled and rotated together: <style> svg { border: solid 1px lightgray; } #bars { fill: red;} #round { opacity: .7; fill: blue; } </style> <body> <svg width=\"600\" height=\"300\"> <g id=\"bars\" transform=\"translate(0,100) rotate(-90, 100, 150)\"> <rect x=\"100\" y=\"150\" height=\"20\" width=\"150\"></rect> <rect x=\"100\" y=\"180\" height=\"20\" width=\"100\"></rect> <rect x=\"100\" y=\"210\" height=\"20\" width=\"200\"></rect> </g> <g id=\"round\" transform=\"translate(200,100) scale(.3) \"> <circle cx=\"280\" cy=\"220\" r=\"50\"></circle> <ellipse cx=\"150\" cy=\"90\" rx=\"80\" ry=\"50\"></ellipse> </g> </svg> </body> The following screenshot shows the groups before applying any transforms or styles to their groups (at left), and after applying the transforms and styles from the code above (right): Left pic is without the <g> tags or with no arguments to them if they are there. Applying styles and transforms to grouped shapes. Code: SVG/18-groups-transforms.html. Styles applied directly to individual elements in the groups have precedence and will override any styles declared at the group level. Positions of elements inside the group are always relative to the coordinate system introduced by any transforms applied to the group. Reusing objects: use and defs An SVG can have a <defs> header containing shapes, groups and other elements that will not be displayed. Filters, clipping masks, gradients and reusable shapes are usually declared in the <defs> header. You will need to assign an id to each element you wish to reuse later. The element can be displayed later declaring the <use> element outside the <defs> block. This element references an existing element by id using standard xlink notation. In the following example, two rectangles are created at 0.0. Since they are in the <defs> header, they will not be displayed. Each is previously configured with colors, dimensions and position. Outside the <defs> header, each element is displayed twice when referenced by each <use> element, translating each one to a different position: <svg width=\"600\" height=\"200\"> <defs> <rect id=\"black\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" fill=\"rgb(64,32,32)\" /> <rect id=\"white\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" fill=\"rgb(255,225,200)\" /> </defs> <g transform=\"translate(10,10) scale(3)\"> <use xlink:href=\"#black\" /> <use xlink:href=\"#white\" transform=\"translate(20)\"/> <use xlink:href=\"#white\" transform=\"translate(0,20)\"/> <use xlink:href=\"#black\" transform=\"translate(20,20)\"/> </g> </svg> The result is shown in the following image. You can use this to create a checkerboard. Reusing objects with <defs> and <use> . Code: SVG/22-defs-use.html. You will rarely use <defs> and <use> in D3, but placing reusable code such as clipping masks, filters and gradients in a <defs> header is good practice. Matrix transforms Matrix transforms are used to scale, translate, skew or rotate any shape or group, or the entire SVG viewport. Transform commands are functions used in the transform XML attribute, in text notation and separated by spaces, for example: <g transform=\"translate(100,100) scale(3) rotate(-90)\"> \u2026 </g> The order is significant. If you call scale(.5) and then rotate(90), the result will be different if you call them in the reverse order. In translate(), scale() and skew(), the first parameter is an x coordinate value, and the second, if present, the y coordinate. In rotate(), the first parameter is an angle in degrees, and the next two parameters, if present, are the coordinates of the center of rotation (if not present, the object will rotate around 0,0 and may disappear from the viewport if the angle is big enough). Flipping an object can be achieved by scaling with negative values for x and/or y. The translate() transform can be used to move groups to different positions. In this case, the x and y coordinates of each object should be considered relative to the group. When creating objects that will be treated as a group, you might also choose to position all objects at the origin (not declaring any x or y coordinates, or use only values relative to the group) to later control the position using translate() You can also apply transforms as CSS styles, but you will need to use explicit units for degrees and distances, for example: <g style=\"transform: translate(100px,100px) scale(3) rotate(-90deg)\"> \u2026 </g> Consider the following SVG drawing of a pair of SVG coordinate axes: <g id=\"coords\"> <line x1=\"10\" y1=\"10\" x2=\"200\" y2=\"10\" /> <line x1=\"10\" y1=\"10\" x2=\"10\" y2=\"150\" /> <text x=\"200\" y=\"20\">x</text> <text x=\"20\" y=\"150\">y</text> </g> The following images show the results of applying translate, rotate/scale and skew to an image of the SVG coordinate axes, compared to the original object(in black): Applying transforms to shapes and groups. Code: SVG/19-translate.html, 20-scale-rotate.html, 21-skew.html. Configuring the SVG viewport You can configure the SVG viewport by changing the values in the viewBox attribute (which can also be used in some reusable SVG elements). The viewBox attribute contains four numbers separated by spaces. The first two are the center of coordinates, which default to 0 0, and the last two are the width and height, which default to the declared or default height and width of the SVG. If present, this attribute can move the viewport to a different position and its scale. For example, if you have an SVG with dimensions 400x300, the default viewport will be 0 0 400 300. If you declare a viewBox of 0 0 800 600, all the objects inside the SVG will be displayed at half the size, since a declared value of 100 is no longer 1/4 of the viewport's width, but 1/8. You can also change the origin of coordinates. For example, you can move it to the center of the SVG and position elements with negative coordinates if you have a viewBox of -200 -150 400 300 (remember that the coordinates start at the top-left corner). Consider the following SVG: <svg width=\"400\" height=\"300\" viewBox=\"...\"> <line x1=\"0\" y1=\"-300\" x2=\"0\" y2=\"300\"/> <line y1=\"0\" x1=\"-400\" y2=\"0\" x2=\"400\"/> <rect x=\"-130\" y=\"-130\" height=\"20\" width=\"200\" fill=\"red\"/> <rect x=\"130\" y=\"-130\" height=\"200\" width=\"20\" fill=\"blue\"/> <rect x=\"0\" y=\"100\" height=\"20\" width=\"200\" fill=\"green\"/> <rect x=\"-130\" y=\"-65\" height=\"200\" width=\"20\" fill=\"orange\"/> </svg> The images below show what would appear on the screen, depending on the values you include for the viewBox attribute. The first one is the default. Scaling and translating the viewport with the viewBox attribute. Code: SVG/26-viewBox-default.html, 28-viewBox-center.html, 28-viewBox-scaled.html You may rarely use viewBox with D3 since you can achieve the same results using matrix transforms, which are simpler. Gradients Linear gradients are created perpendicular to a line, so it requires the same attributes as the <line> element. Radial gradients use circle attributes. Gradients should declare at least one child <stop> with a non-zero offset and stop-color different than black, since the default color is black and the default offset is zero. Typically, gradients declare two or more stop colors. Gradients are used used as a fill or stroke value. They are usually defined in <defs> with an id that can be referenced later using url(#id). The following SVG code declares two gradients and applies one of them to a square, and the other to a circle: <svg width=\"600\" height=\"300\"> <defs> <linearGradient x2=\"1\" id=\"rainbow\"> <stop offset=\"0\" stop-color=\"rgb(255,0,0)\" /> <stop offset=\"0.25\" stop-color=\"rgb(255,255,64)\" /> <stop offset=\"0.5\" stop-color=\"rgb(64,255,64)\" /> <stop offset=\"0.75\" stop-color=\"rgb(64,64,255)\" /> <stop offset=\"1\" stop-color=\"rgb(128,0,255)\" /> </linearGradient> <radialGradient cx=\"0.35\" cy=\"0.35\" r=\"1\" id=\"glow\"> <stop offset=\"0\" stop-color=\"rgb(255,255,255)\" /> <stop offset=\"0.5\" stop-color=\"rgb(0,128,255)\" /> <stop offset=\"1\" stop-color=\"rgb(128,0,255)\" /> </radialGradient> </defs> <rect x=\"0\" y=\"0\" width=\"200\" height=\"200\" fill=\"url(#rainbow)\" transform=\"translate(50,50)\"/> <circle cx=\"100\" cy=\"100\" r=\"100\" fill=\"url(#glow)\" transform=\"translate(300,50)\"/> </svg> The result is shown in the following screenshot: Linear and radial gradients. Code: SVG/23-gradient.html. Clipping A clipping mask is created with any shape declared inside the <clipPath> element. This is usually done in <defs> , setting an id for the clipping mask that can be referenced in the object that should be clipped, using url(#id) inside a clip-path attribute, as shown below: <svg width=\"425\" height=\"425\"> <defs> <clipPath id=\"poly\"> <circle r=\"139\" cx=\"200\" cy=\"199\"/> </clipPath> </defs> <!-- Clipped image --> <image x=\"25\" y=\"25\" height=\"350\" width=\"350\" xlink:href=\"../Data/Images/pluto.jpg\" clip-path=\"url(#poly)\"/> </svg> The following SVG screenshots show the circular clipping mask above applied to an image. The SVG at left shows the original image and the clipped image is shown at right. Clipping an image in SVG. Code: SVG/24-clipping.html. Filters SVG filters can be applied to any shapes, text or images. They are usually created in the <defs> header with an id that can be referenced via url(#id) using the filter attribute. The <filter> element can contain several different filter types and you can also create composite filters. The following example creates two different configurations for the <feGaussianBlur> filter, applied to a text element and a circle: <svg width=\"600\" height=\"300\"> <defs> <filter id=\"filter1\"> <feGaussianBlur stdDeviation=\"7\" /> </filter> <filter id=\"filter2\" x=\"-100\" y=\"-100\" height=\"200\" width=\"200\"> <feGaussianBlur stdDeviation=\"0,5\" in=\"SourceGraphic\" /> </filter> </defs> <text id=\"text\" font-size=\"40\" fill=\"black\" x=\"50\" y=\"60\" filter=\"url(#filter2)\">Do you need glasses?</text> <g id=\"stardot\" transform=\"translate(100,25) scale(0.5)\"> <polygon id=\"star\" points=\"250,0 400,500 0,200 500,200 100,500\" fill=\"red\" fill-rule=\"evenodd\"/> <circle id=\"circ\" cx=\"250\" cy=\"283\" r=\"75\" fill=\"blue\" filter=\"url(#filter1)\" /> </g> </svg> The result is shown as follows: A Gaussian blur filter applied to different shapes. Code: SVG/25-filter.html. An SVG example The following code uses several SVG elements described in this section to draw some shapes, shadows, gradients and text: <svg width=\"300\" height=\"300\"> <defs> <filter id=\"shadow\"> <feDropShadow style=\"flood-color: green\" dx=\"5\" dy=\"5\" stdDeviation=\"3\"/> </filter> <linearGradient id=\"grad\" x1=\"0\" y1=\"0\" x2=\"100%\" y2=\"0\"> <stop offset=\"0%\" stop-color=\"magenta\"/> <stop offset=\"100%\" stop-color=\"yellow\"/> </linearGradient> <clipPath id=\"circle\"> <circle r=\"40\" cx=\"175\" cy=\"75\"/> </clipPath> </defs> <!-- rectangle --> <rect x=\"50\" y=\"50\" height=\"50\" width=\"50\" fill=\"red\" stroke=\"blue\" stroke-width=\"10\" stroke-opacity=\"0.5\"/> <!-- dashed shape --> <path id=\"path1\" d=\"M150,200 L150,150 L100,150 C100,200 150,250 200,250 L200,200 Z\" stroke-dasharray=\"5 2 1 2\" stroke-width=\"2\" stroke=\"blue\" fill=\"none\" style=\"filter:url(#shadow)\"/> <!-- gray quarter-circle --> <path d=\"M0,0 L0,-100 A100,100 0 0,0 -100,0 L0,0 Z\" transform=\"translate(100,250) scale(0.5) \" stroke=\"red\" stroke-opacity=\".5\" stroke-width=\"4\" fill-opacity=\".2\"/> <text fill=\"url(#grad)\" font-size=\"20\" x=\"200\" y=\"100\"> Scalable <tspan dy=\"20\" x=\"200\">Vector</tspan> <tspan dy=\"20\" x=\"200\">Graphics</tspan> </text> <image x=\"125\" y=\"25\" height=\"100\" width=\"100\" xlink:href=\"../Data/Images/pluto.jpg\" clip-path=\"url(#circle)\" opacity=\"0.75\"/> <!-- raindow half-circle --> <path d=\"M100,200 C100,100 250,100 250,200\" transform=\"scale(0.6) rotate(180,295,225) \" fill=\"url(#grad)\"/> </svg> Compare this code and the following image it generates it with an identical image created using D3 (SVG-with-D3/29-example.html) and HTML Canvas (Canvas/1-canvas-svg-compare.html): An image created using SVG. Code: SVG/29-example.html. 2) Essential JavaScript data structures I'm actually going to organize this somewhere else to help with my code journal. Link to the Basics 3) HTML Canvas HTML5 Canvas Most of your D3 applications will render graphics using SVG, but several shape generators in SVG can also generate Canvas, and you may choose to use Canvas in all or part of your application to improve performance if you have memory problems due to excessive objects created in the DOM. To draw using Canvas you need to create a <canvas> element in your page. You can do that using plain HTML: <body> <canvas id=\"canvas\" width=\"400\" height=\"300\"></canvas> </body> Or using D3: d3.select(\"body\").append(\"canvas\").attr(\"width\", 400).attr(\"height\", 300); If you declare the Canvas element in HTML, you can reference it by its ID using the DOM or D3: const canvas = d3.select(\"#canvas\").node(); // node() returns the DOM object Once you have a canvas object, you obtain a 2D graphics context and start drawing: const ctx = canvas.getContext(\"2d\"); Practically all the Canvas API consists of methods and properties called from the graphics context. Before drawing, you set properties such as font , fillStyle , strokeStyle : ctx.fillStyle = \"red\"; ctx.strokeStyle = \"rgba(255,127,0,0.7)\"; ctx.lineWidth = 10; And then fill or stroke rectangles and arbitrary paths containing lines and curves. The following commands will draw a red 50x50 pixel square with a 10 pixel wide yellow semi-transparent border at position 50,50: ctx.fillRect(50,50,50,50); ctx.strokeRect(50,50,50,50); You can also draw other shapes, text and images on the same canvas. The context properties will not change unless they are redefined or a previously saved context is restored. It's a good practice to save the context to the stack before applying properties or transforms, and restore it when you are done drawing an object. This allows you to always start with a clean context: Note: So far the below code does nothing that I can see with my eyes. ctx.save(); ctx.transform(50,60); ctx.scale(2); // \u2026 ctx.restore(); // starting with a new context Property or method Description fillStyle Sets the color to be used in fill() commands. strokeStyle Sets the color to be used in stroke() commands. lineWidth Sets the line width to be used in stroke() commands. lineCap Sets the style of the line caps: can be butt (default), round or square. textAlign Sets the alignment for text: can be start (default), center, end, left or right. textBaseline Sets the baseline for text: can be middle, hanging, top, bottom, ideographic or alphabetic (default). font Sets the font to be used in text commands, using the compact CSS font syntax. globalAlpha Sets the global opacity (0 = transparent, 1 = opaque) for the context. shadowBlur, shadowColor, shadowOffsetX, shadowOffsetY Sets shadow properties. Default shadow color is transparent black. Default numeric values are zero. setLineDash(dasharray) Sets the dash array (alternating line and space lengths) for strokes. translate(x,y) Sets the current translate transform for the context. scale(x,y) Sets the current scale transform for the context. rotate(angle) Sets the current rotate transform for the context. save() Saves the state of the current context (pushes into a stack). restore() Restores the state of the last context that was saved (pops it from the stack). The fillRect() command is typically used to clear the entire canvas before redrawing, but you can also use it to draw arbitrary rectangles. The following table lists methods you can use to draw rectangles, draw text and images: Method Description fillRect(x,y,w,h); Fills a rectangle. Typically used to clear the Canvas on redrawing. strokeRect(x,y,w,h) Draws a border around a rectangle. fillText(text,x,y); Fills text at position x, y (depends on current textAlign and textBaseline). strokeText(text, x, y); Draws a border around text. drawImage(image, x, y, w, h) Draws an image at x,y with width w and height h. Canvas context methods used to draw rectangles, text and images A path is a series of commands to move to points, draw lines, curves or arcs. To draw a path you need to first call ctx.beginPath() , then call a sequence of commands that move to points, draw lines and curves, and when you are done you can close the path (if it's a closed path) and call fill() and/or stroke() to draw it using the current styles. The following table lists several commands you can use in a path: Method Description beginPath() Starts a path. closePath() Closes a path. moveTo(x,y) Moves the cursor to a position in the path. lineTo(x,y) Moves the cursor to a position in the path, drawing a line along the way. bezierCurveTo(c1x,c1y,c2x,c2y,x,y) quadraticCurveTo(cx,cy,x,y) Draws curves with one (quadratic) or two (Bezier) control points in a path. arc(x,y,r,sa,ea) Draws an arc by specifying the center, radius, start and end angles in a path. arcTo(sx,sy,r,ex,ey) Draws an arc by specifying the coordinates of the starting point, the radius and the coordinates of the ending point. rect(x,y,w,h) Draws a rectangle in a path with coordinates of top-left corner, width and height. clip() Creates a clipping region with the shapes drawn by the path that will affect objects that are drawn afterwards. fill() Fills a path with the current color. Call this to fill the path when done. stroke() Strokes the path with the current color. Call this to stroke the path when done. A Canvas Example The following code uses several of the methods above to draw different shapes on the same Canvas context. It draws some shapes, text, images and paths, and applies transforms, shadows, clipping and gradients. Compare it to the example shown before in SVG that draws the image (see Canvas/1-canvas-svg-compare.html ): const canvas = document.getElementById(\"canvas\"); const ctx = canvas.getContext(\"2d\"); // rectangle ctx.save(); // save default context ctx.fillStyle = \"#ff0000\"; ctx.strokeStyle = \"blue\"; ctx.lineWidth = 10; ctx.fillRect(50,50,50,50); ctx.globalAlpha = 0.5; ctx.strokeRect(50,50,50,50); // dashed shape ctx.restore(); ctx.save(); ctx.strokeStyle = \"blue\"; ctx.lineWidth = 2; ctx.shadowBlur = 6; ctx.shadowColor = \"green\"; ctx.shadowOffsetX = ctx.shadowOffsetY = 5; ctx.setLineDash([5,2,1,2]); ctx.beginPath(); ctx.moveTo(150,200); ctx.lineTo(150,150); ctx.lineTo(100,150); ctx.bezierCurveTo(100,200,150,250,200,250); ctx.lineTo(200,200); ctx.closePath(); ctx.stroke(); ctx.restore(); ctx.save(); // quarter-circle ctx.translate(100,250); ctx.scale(0.5, 0.5); ctx.strokeStyle = \"red\"; ctx.lineWidth = 4; ctx.globalAlpha = 0.5; ctx.beginPath(); ctx.moveTo(0,0); ctx.lineTo(0,-100); ctx.arcTo(-100,-100,-100,0,100); ctx.lineTo(0,0); ctx.stroke(); ctx.globalAlpha = 0.2; ctx.beginPath(); ctx.arc(0,0,100,3.14,-1.57,false); ctx.lineTo(0,0); ctx.closePath(); ctx.fill(); ctx.restore(); ctx.save(); // text and half-circle const text = \"Canvas\" ctx.translate(250,150); ctx.font = \"24px monospace\"; const textWidth = ctx.measureText(text).width; const gradient = ctx.createLinearGradient(-50,-50,-50 + textWidth,-50); gradient.addColorStop(0,\"magenta\"); gradient.addColorStop(1, \"yellow\"); ctx.fillStyle = gradient; ctx.shadowColor = \"transparent\"; ctx.fillText(text, -45, -5); ctx.scale(1.1, 1.1) ctx.rotate(3.14); ctx.beginPath(); ctx.arc(0,0,40,3.14,0,false); ctx.fill(); ctx.restore(); ctx.save(); // image and clip ctx.beginPath(); ctx.arc(175,75,40,0,6.28,false); ctx.clip(); const image = new Image(100,100); image.onload = function() { ctx.globalAlpha = 0.75; ctx.drawImage(this, 125, 25, this.width, this.height); } image.src = \"reuse.png\"; ctx.save(); I don't have the pluto pic so the code won't show the planet. Data Formats Data used in visualizations are usually distributed in a standard format that can be shared. Even when the data is served from a database, the data is usually delivered in some standard format. Popular proprietary formats such as Excel spreadsheets are common, but most statistical data is stored or delivered in CSV, XML or JSON formats. CSV You can load and parse CSV in D3 using the d3.csv() function. const csv = d3.csv(\"Data/continents.csv\") csv output: Promise {<resolved>: Array(7)} __proto__: Promise [[PromiseStatus]]: \"resolved\" [[PromiseValue]]: Array(7) 0: {continent: \"North America\", population: \"579024000\", areakm2: \"24490000\"} 1: {continent: \"Asia\", population: \"4436224000\", areakm2: \"43820000\"} 2: {continent: \"Europe\", population: \"738849000\", areakm2: \"10180000\"} 3: {continent: \"Africa\", population: \"1216130000\", areakm2: \"30370000\"} 4: {continent: \"South America\", population: \"422535000\", areakm2: \"17840000\"} 5: {continent: \"Oceania\", population: \"39901000\", areakm2: \"9008500\"} 6: {continent: \"Antarctica\", population: \"1106\", areakm2: \"13720000\"} columns: (3) [\"continent\", \"population\", \"areakm2\"] length: 7 __proto__: Array(0) XML XML \u2013 eXtensible Markup Language is a very popular data format. Ajax responses from web services are usually returned as text or XML. It has standard native support in JavaScript via the DOM (document object model) APIs and doesn't require additional parsing. Although it is still common to find data in XML format, CSV and JSON alternatives, if available, are usually smaller and easier to work with. <continents> <continent> <name>North America</name> <population>579024000</population> <area unit=\"km\">24490000</area> </continent> <continent> <name>Asia</name> <population>4436224000</population> <area unit=\"km\">43820000</area> </continent> <continent> <name>Antarctica</name> <population>1106</population> <area>13720000</area> </continent> </continents> const xml = d3.xml(\"Data/continents.xml\") Not sure how to parse but who uses XML anymore gosh! JSON JSON stands for JavaScript Object Notation. It looks a lot like a JavaScript Object, but it has stricter formation rules. It's probably the easiest format to work with. It's compact and easy to parse, and it's gradually replacing XML as a preferred data format in Web Services. The data file containing continent data is shown below in JSON format ( Data/continents.json ). [ { \"continent\": \"North America\", \"population\": 579024000, \"areakm2\": 24490000 },{ \"continent\": \"Asia\", \"population\": 4436224000, \"areakm2\": 43820000 },{ \"continent\": \"Europe\", \"population\": 738849000, \"areakm2\": 10180000 },{ \"continent\": \"Africa\", \"population\": 1216130000, \"areakm2\": 30370000 },{ \"continent\": \"South America\", \"population\": 422535000, \"areakm2\": 17840000 },{ \"continent\": \"Oceania\", \"population\": 39901000, \"areakm2\": 9008500 },{ \"continent\": \"Antarctica\", \"population\": 1106, \"areakm2\": 13720000 } ] JSON is the preferred format for data manipulation in JavaScript. There are many online tools you can use to transform CSV and XML files into JSON. You can load and parse JSON in D3 using the d3.json() function. Promise {<resolved>: Array(7)} __proto__: Promise [[PromiseStatus]]: \"resolved\" [[PromiseValue]]: Array(7) 0: {continent: \"North America\", population: 579024000, areakm2: 24490000} 1: {continent: \"Asia\", population: 4436224000, areakm2: 43820000} 2: {continent: \"Europe\", population: 738849000, areakm2: 10180000} 3: {continent: \"Africa\", population: 1216130000, areakm2: 30370000} 4: {continent: \"South America\", population: 422535000, areakm2: 17840000} 5: {continent: \"Oceania\", population: 39901000, areakm2: 9008500} 6: {continent: \"Antarctica\", population: 1106, areakm2: 13720000} length: 7 __proto__: Array(0) Note that both csv and json loaded an easy to work with object. Just try to xml code....It'll make you sick.","title":"Chapter 2"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#technical-fundamentals","text":"This chapter covers fundamental standard web technologies used by D3: SVG, JavaScript (ES 2015), HTML Canvas and standard data formats such as JSON and CSV. It is intended as a general reference to these topics. Most data visualizations created with D3.js generate SVG graphics. Good knowledge of SVG is important to make the most of D3, but you only really need to know the basics.","title":"Technical Fundamentals"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#1-svg","text":"SVG stands for Scalable Vector Graphics. It\u2019s an XML-based image format that describes graphics using geometrical attributes. Unlike HTML5 Canvas, which is another standard for vector graphics, SVG primitives are made of individual XML elements described using tags and attributes. It is also object-based and provides a DOM, which allows CSS styling, dynamic shape creation and manipulation, and coordinate transforms using JavaScript or CSS. Scalable Vector Graphics (SVG) SVG stands for Scalable Vector Graphics. It\u2019s an XML-based image format that describes graphics using geometrical attributes. Unlike HTML5 Canvas, which is another standard for vector graphics, SVG primitives are made of individual XML elements described using tags and attributes. It is also object-based and provides a DOM, which allows CSS styling, dynamic shape creation and manipulation, and coordinate transforms using JavaScript or CSS. To control SVG elements with D3 you should understand basic SVG syntax and rules, how a document is structured, how each element is rendered, the effects caused by attributes and styles, as well as nesting and transformation rules. All the code used in this section is available in the SVG/ folder, from the GitHub repository for this chapter. You can see the results simply loading the pages in your browser.","title":"1) SVG"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#viewport","text":"SVG graphics context (viewport) When SVG is embedded in HTML it creates a viewport. Ex (note the light grey style): <style> svg { border: solid 1px lightgray; background-color: hsla(240,100%,50%,0.2) } </style> <body> <h2>SVG viewport</h2> <svg width=\"600\" height=\"300\"></svg> </body> You can also create an SVG element using the DOM API, or D3, which is much simpler. The result is identical: <body> <script> d3.select(\"body\").append(\"svg\").attr(\"width\", 400).attr(\"height\", 300); </script> </body>","title":"Viewport"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#shapes","text":"Shapes are positioned in the viewport using x and y coordinates. They described by XML tags like <rect> , <circle> , <ellipse> , <path> , <polygon> and others. You create SVG graphics by placing these tags inside the <svg> element. Each supports attributes that configure their position in the viewport, and specific properties for each shape, such as radii, vertices or dimensions. A circle can be drawn in SVG using the <circle> element and at least the r attribute (radius). If you don't provide any other attributes, you will only see the lower-right quarter of the circle, since the default coordinates for its center will be (0,0). To illustrate an important js and raw html equivalency we will show code to draw 4 circles in the exact same way: HTML: <svg width=\"400\" height=\"300\"> <circle r=\"25\"></circle> <circle cx=\"250\" cy=\"200\" r=\"50\"></circle> <circle cx=\"50\" cy=\"50\" r=\"20\"></circle> <circle cx=\"400\" cy=\"300\" r=\"50\"></circle> </svg> JS: const svg = d3.select(\"body\") .append(\"svg\") .attr(\"width\", 400) .attr(\"height\", 300); svg.append(\"circle\").attr(\"r\", 25); svg.append(\"circle\").attr(\"cx\", 250).attr(\"cy\", 200).attr(\"r\", 50); svg.append(\"circle\").attr(\"cx\", 50).attr(\"cy\", 50).attr(\"r\", 20); svg.append(\"circle\").attr(\"cx\", 400).attr(\"cy\", 300).attr(\"r\", 50);","title":"Shapes"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#fills-and-strokes","text":"Shapes have default black fill colors and transparent stroke borders, unless you assign different color strings to the SVG attributes (or CSS properties) fill and stroke. In the following SVG, three straight lines were drawn using the mandatory x1/y1 and x2/y2 attributes for elements. They would be invisible it the stroke attribute wasn't present. A stroke-width has a default value of 1. <svg width=\"400\" height=\"300\"> <line x2=\"400\" stroke=\"red\" stroke-width=\"5\"/> <line y2=\"150\" stroke=\"blue\" stroke-width=\"5\"/> <line x2=\"200\" y2=\"150\" stroke=\"black\" stroke-width=\"1\"/> </svg> You can also style with css: line { stroke-width: 20px; /* Overrides attr */ } Note lines at edge of screen might appear thinner as half the width could be off screen You can also apply CSS class and style attributes to different SVG objects. For example, consider the following CSS class declarations: .reds { fill: red; } .semitr { fill-opacity: 0.5; } Since the following rectangles each belong to one or more of these classes, they will inherit the style properties declared for each class: <svg width=\"600\" height=\"200\"> <rect x=\"50\" y=\"50\" width=\"90\" height=\"90\" class=\"semitr\"/> <rect x=\"200\" y=\"50\" width=\"175\" height=\"100\" rx=\"40\" ry=\"40\" class=\"reds semitr\"/> <rect x=\"450\" y=\"25\" width=\"100\" height=\"150\" class=\"reds\"/> </svg> One class applies the red fill, the other applies 50% transparency. The rectangle in the middle belongs to both classes, so it's both red and semi-transparent, as follows:","title":"Fills and Strokes"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#transparency","text":"Transparency When objects overlap in SVG, the code order determines which element will appear over the other. Preceding siblings are always overlapped by siblings that are declared after them. The CSS z-index property doesn\u2019t work in SVG. To move an object to the front, you have to modify the DOM tree. You can see through objects that overlap by changing their transparency. You can apply opacity levels to fills and strokes separately, using fill-opacity or stroke-opacity, or for the entire object using opacity. All attributes require a value between 0 (invisible) and 1 (opaque). An alternative, which achieves the same result, is to use the alpha component in rgba or hsla color strings (for example, \u2018rgba(255, 0, 0, 0.5)\u2019). The three squares below apply different transparency parameters on fills and strokes: <rect x=\"50\" y=\"50\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" stroke=\"red\" stroke-width=\"10\" fill-opacity=\"0\"/> <rect x=\"75\" y=\"75\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" fill=\"gray\" stroke=\"black\" stroke-width=\"10\" fill-opacity=\".7\"/> <rect x=\"100\" y=\"100\" height=\"100\" width=\"100\" rx=\"10\" ry=\"10\" fill=\"yellow\" stroke=\"blue\" stroke-width=\"10\" stroke-opacity=\".6\"/> In the following code, two thin vertical rectangles appear behind a wide horizontal rectangle, and two other vertical rectangles appear in front of it: <rect x=\"300\" y=\"50\" height=\"150\" width=\"25\"/> <rect x=\"400\" y=\"50\" height=\"150\" width=\"25\" fill-opacity=\".5\"/> <rect x=\"250\" y=\"100\" height=\"50\" width=\"300\" fill=\"red\" stroke=\"orange\" stroke-width=\"10\" stroke-opacity=\".5\" /> <rect x=\"350\" y=\"50\" height=\"150\" width=\"25\"/> <rect x=\"450\" y=\"50\" height=\"150\" width=\"25\" fill-opacity=\".5\"/> The following image shows the result: Drawing rectangles with different fills, strokes and transparency. Code: 7-rect-opacity.html.","title":"Transparency"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#ellipses","text":"Ellipses have a center in cx and cy like circles, but also have two radii, which are set with rx and ry. The following code creates three ellipses in different positions. <svg width=\"600\" height=\"300\"> <ellipse cx=\"150\" cy=\"100\" rx=\"150\" ry=\"100\" fill=\"yellow\" fill-opacity=\"0.5\" stroke=\"blue\" stroke-width=\"1\" stroke-dasharray=\"5 5\"/> <ellipse cx=\"400\" cy=\"150\" rx=\"75\" ry=\"125\" fill=\"red\" fill-opacity=\"0.2\" stroke=\"red\" stroke-width=\"5\" stroke-opacity=\"0.5\"/> <ellipse cx=\"400\" cy=\"250\" rx=\"150\" ry=\"40\" fill=\"black\" fill-opacity=\"0\" stroke=\"green\" stroke-width=\"20\" stroke-opacity=\"0.2\" /> </svg> The preceding code produces the following result: Ellipses created with SVG. Code: SVG/8-ellipse.html.","title":"Ellipses"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#polygons-and-polylines","text":"Polygons ( <polygon> ) and polylines ( <polyline> ) render closed or open shapes using straight lines specified by a list of vertices passed in the points attribute. The only difference between them is that polygons close the shape. To close a polyline you need to repeat the initial coordinates. They also have a fill-rule attribute that controls the winding order, and determines if a hole will be drawn inside the shape when an outline crosses with itself. The following code creates two polygons. The second one has a fill-rule that will reveal a hole: <svg width=\"600\" height=\"300\"> <polygon points=\"150,150 50,150 100,20 150,50 200,200 50,200 20,154 48,82 32,20\" fill=\"blue\"/> <polygon points=\"450,150 350,150 400,20 450,50 500,200 350,200 320,154 348,82 332,20\" fill=\"red\" fill-rule=\"evenodd\"/> </svg> The result is shown as follows: Polygons created with SVG. The second polygon uses fill-rule=\u201deven-odd\u201d and reveals a hole. Code: SVG/10-polygon.","title":"Polygons and polylines"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#paths","text":"With paths you can draw open and closed shapes mixing lines, curves and arcs using a compact drawing language in the d attribute of the <path> element. It can be used to create arbitrary paths mixing straight lines and curves: <path d=\"M100,200 C200,50 300,100 300,200 L400,250 500,100\" fill=\"yellow\" stroke=\"red\" stroke-width=\"4\"/> Don't worry about all those numbers and letters in the d attribute. It's the most important part of the path, but it can always be generated for you. Most of the shape generators you will use in D3 to create lines, pie slices and other arbitrary shapes generate path strings that you can use in the d attribute. The simple <path> above renders the image below (the dots are added separately and show the control points): A curve described by a element. Code: SVG/11-path-line.html.","title":"Paths"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#text","text":"Unlike HTML, you can't simply insert text inside any element. You have to create text objects using the <text> element with text contents. You can position text using x and y attributes, but you must remember that y is actually the baseline (default). If y is zero or not present, only the parts of the text that extend below the baseline will be visible inside the graphics context. This example places both text and a rectangle in the same position: <rect x=\"0\" y=\"0\" height=\"36\" width=\"200\"/> <text font-size=\"36\" x=\"0\" y=\"0\" fill=\"lightgray\">ghijklmnop</text> The following illustration shows the result, at left. Note that only the parts of the text that extend below the baseline actually appear over the rectangle. The other two examples show text with a different baseline alignment: alignment-baseline=\"middle\" and alignment-baseline=\"hanging\". Drawing text in SVG and the baseline. Code: SVG/14-text.html. You can also align text horizontally using the text-anchor attribute. The following illustration describes properties and values used to align text horizontally or vertically. If you intend to rotate text relative to its position, these parameters will affect the result. Properties for aligning text. Code: SVG/15-text-align.html. It's best to configure baselines and alignments in CSS instead of using XML attributes. If your text spans multiple lines, you can use the <tspan> element inside <text> to move words or letters to positions relative to the parent <text> element (see SVG/16-tspan.html).","title":"Text"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#group-containers","text":"You can group several shapes in SVG with the <g> element. It's analogous to a <div> in HTML. This is an invisible element and it's positioned at the center of coordinates. CSS properties applied to a group affect all the objects it contains. You can also apply matrix transforms to groups to move, rotate and scale all its contents. A group container can also contain other group containers. In the following SVG, circles and ellipses are in a group, and rectangles are in another. The color of the elements in each group is declared in CSS, and they are translated, scaled and rotated together: <style> svg { border: solid 1px lightgray; } #bars { fill: red;} #round { opacity: .7; fill: blue; } </style> <body> <svg width=\"600\" height=\"300\"> <g id=\"bars\" transform=\"translate(0,100) rotate(-90, 100, 150)\"> <rect x=\"100\" y=\"150\" height=\"20\" width=\"150\"></rect> <rect x=\"100\" y=\"180\" height=\"20\" width=\"100\"></rect> <rect x=\"100\" y=\"210\" height=\"20\" width=\"200\"></rect> </g> <g id=\"round\" transform=\"translate(200,100) scale(.3) \"> <circle cx=\"280\" cy=\"220\" r=\"50\"></circle> <ellipse cx=\"150\" cy=\"90\" rx=\"80\" ry=\"50\"></ellipse> </g> </svg> </body> The following screenshot shows the groups before applying any transforms or styles to their groups (at left), and after applying the transforms and styles from the code above (right): Left pic is without the <g> tags or with no arguments to them if they are there. Applying styles and transforms to grouped shapes. Code: SVG/18-groups-transforms.html. Styles applied directly to individual elements in the groups have precedence and will override any styles declared at the group level. Positions of elements inside the group are always relative to the coordinate system introduced by any transforms applied to the group.","title":"Group containers"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#reusing-objects-use-and-defs","text":"An SVG can have a <defs> header containing shapes, groups and other elements that will not be displayed. Filters, clipping masks, gradients and reusable shapes are usually declared in the <defs> header. You will need to assign an id to each element you wish to reuse later. The element can be displayed later declaring the <use> element outside the <defs> block. This element references an existing element by id using standard xlink notation. In the following example, two rectangles are created at 0.0. Since they are in the <defs> header, they will not be displayed. Each is previously configured with colors, dimensions and position. Outside the <defs> header, each element is displayed twice when referenced by each <use> element, translating each one to a different position: <svg width=\"600\" height=\"200\"> <defs> <rect id=\"black\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" fill=\"rgb(64,32,32)\" /> <rect id=\"white\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" fill=\"rgb(255,225,200)\" /> </defs> <g transform=\"translate(10,10) scale(3)\"> <use xlink:href=\"#black\" /> <use xlink:href=\"#white\" transform=\"translate(20)\"/> <use xlink:href=\"#white\" transform=\"translate(0,20)\"/> <use xlink:href=\"#black\" transform=\"translate(20,20)\"/> </g> </svg> The result is shown in the following image. You can use this to create a checkerboard. Reusing objects with <defs> and <use> . Code: SVG/22-defs-use.html. You will rarely use <defs> and <use> in D3, but placing reusable code such as clipping masks, filters and gradients in a <defs> header is good practice. Matrix transforms Matrix transforms are used to scale, translate, skew or rotate any shape or group, or the entire SVG viewport. Transform commands are functions used in the transform XML attribute, in text notation and separated by spaces, for example: <g transform=\"translate(100,100) scale(3) rotate(-90)\"> \u2026 </g> The order is significant. If you call scale(.5) and then rotate(90), the result will be different if you call them in the reverse order. In translate(), scale() and skew(), the first parameter is an x coordinate value, and the second, if present, the y coordinate. In rotate(), the first parameter is an angle in degrees, and the next two parameters, if present, are the coordinates of the center of rotation (if not present, the object will rotate around 0,0 and may disappear from the viewport if the angle is big enough). Flipping an object can be achieved by scaling with negative values for x and/or y. The translate() transform can be used to move groups to different positions. In this case, the x and y coordinates of each object should be considered relative to the group. When creating objects that will be treated as a group, you might also choose to position all objects at the origin (not declaring any x or y coordinates, or use only values relative to the group) to later control the position using translate() You can also apply transforms as CSS styles, but you will need to use explicit units for degrees and distances, for example: <g style=\"transform: translate(100px,100px) scale(3) rotate(-90deg)\"> \u2026 </g> Consider the following SVG drawing of a pair of SVG coordinate axes: <g id=\"coords\"> <line x1=\"10\" y1=\"10\" x2=\"200\" y2=\"10\" /> <line x1=\"10\" y1=\"10\" x2=\"10\" y2=\"150\" /> <text x=\"200\" y=\"20\">x</text> <text x=\"20\" y=\"150\">y</text> </g> The following images show the results of applying translate, rotate/scale and skew to an image of the SVG coordinate axes, compared to the original object(in black): Applying transforms to shapes and groups. Code: SVG/19-translate.html, 20-scale-rotate.html, 21-skew.html.","title":"Reusing objects: use and defs"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#configuring-the-svg-viewport","text":"You can configure the SVG viewport by changing the values in the viewBox attribute (which can also be used in some reusable SVG elements). The viewBox attribute contains four numbers separated by spaces. The first two are the center of coordinates, which default to 0 0, and the last two are the width and height, which default to the declared or default height and width of the SVG. If present, this attribute can move the viewport to a different position and its scale. For example, if you have an SVG with dimensions 400x300, the default viewport will be 0 0 400 300. If you declare a viewBox of 0 0 800 600, all the objects inside the SVG will be displayed at half the size, since a declared value of 100 is no longer 1/4 of the viewport's width, but 1/8. You can also change the origin of coordinates. For example, you can move it to the center of the SVG and position elements with negative coordinates if you have a viewBox of -200 -150 400 300 (remember that the coordinates start at the top-left corner). Consider the following SVG: <svg width=\"400\" height=\"300\" viewBox=\"...\"> <line x1=\"0\" y1=\"-300\" x2=\"0\" y2=\"300\"/> <line y1=\"0\" x1=\"-400\" y2=\"0\" x2=\"400\"/> <rect x=\"-130\" y=\"-130\" height=\"20\" width=\"200\" fill=\"red\"/> <rect x=\"130\" y=\"-130\" height=\"200\" width=\"20\" fill=\"blue\"/> <rect x=\"0\" y=\"100\" height=\"20\" width=\"200\" fill=\"green\"/> <rect x=\"-130\" y=\"-65\" height=\"200\" width=\"20\" fill=\"orange\"/> </svg> The images below show what would appear on the screen, depending on the values you include for the viewBox attribute. The first one is the default. Scaling and translating the viewport with the viewBox attribute. Code: SVG/26-viewBox-default.html, 28-viewBox-center.html, 28-viewBox-scaled.html You may rarely use viewBox with D3 since you can achieve the same results using matrix transforms, which are simpler.","title":"Configuring the SVG viewport"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#gradients","text":"Linear gradients are created perpendicular to a line, so it requires the same attributes as the <line> element. Radial gradients use circle attributes. Gradients should declare at least one child <stop> with a non-zero offset and stop-color different than black, since the default color is black and the default offset is zero. Typically, gradients declare two or more stop colors. Gradients are used used as a fill or stroke value. They are usually defined in <defs> with an id that can be referenced later using url(#id). The following SVG code declares two gradients and applies one of them to a square, and the other to a circle: <svg width=\"600\" height=\"300\"> <defs> <linearGradient x2=\"1\" id=\"rainbow\"> <stop offset=\"0\" stop-color=\"rgb(255,0,0)\" /> <stop offset=\"0.25\" stop-color=\"rgb(255,255,64)\" /> <stop offset=\"0.5\" stop-color=\"rgb(64,255,64)\" /> <stop offset=\"0.75\" stop-color=\"rgb(64,64,255)\" /> <stop offset=\"1\" stop-color=\"rgb(128,0,255)\" /> </linearGradient> <radialGradient cx=\"0.35\" cy=\"0.35\" r=\"1\" id=\"glow\"> <stop offset=\"0\" stop-color=\"rgb(255,255,255)\" /> <stop offset=\"0.5\" stop-color=\"rgb(0,128,255)\" /> <stop offset=\"1\" stop-color=\"rgb(128,0,255)\" /> </radialGradient> </defs> <rect x=\"0\" y=\"0\" width=\"200\" height=\"200\" fill=\"url(#rainbow)\" transform=\"translate(50,50)\"/> <circle cx=\"100\" cy=\"100\" r=\"100\" fill=\"url(#glow)\" transform=\"translate(300,50)\"/> </svg> The result is shown in the following screenshot: Linear and radial gradients. Code: SVG/23-gradient.html.","title":"Gradients"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#clipping","text":"A clipping mask is created with any shape declared inside the <clipPath> element. This is usually done in <defs> , setting an id for the clipping mask that can be referenced in the object that should be clipped, using url(#id) inside a clip-path attribute, as shown below: <svg width=\"425\" height=\"425\"> <defs> <clipPath id=\"poly\"> <circle r=\"139\" cx=\"200\" cy=\"199\"/> </clipPath> </defs> <!-- Clipped image --> <image x=\"25\" y=\"25\" height=\"350\" width=\"350\" xlink:href=\"../Data/Images/pluto.jpg\" clip-path=\"url(#poly)\"/> </svg> The following SVG screenshots show the circular clipping mask above applied to an image. The SVG at left shows the original image and the clipped image is shown at right. Clipping an image in SVG. Code: SVG/24-clipping.html.","title":"Clipping"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#filters","text":"SVG filters can be applied to any shapes, text or images. They are usually created in the <defs> header with an id that can be referenced via url(#id) using the filter attribute. The <filter> element can contain several different filter types and you can also create composite filters. The following example creates two different configurations for the <feGaussianBlur> filter, applied to a text element and a circle: <svg width=\"600\" height=\"300\"> <defs> <filter id=\"filter1\"> <feGaussianBlur stdDeviation=\"7\" /> </filter> <filter id=\"filter2\" x=\"-100\" y=\"-100\" height=\"200\" width=\"200\"> <feGaussianBlur stdDeviation=\"0,5\" in=\"SourceGraphic\" /> </filter> </defs> <text id=\"text\" font-size=\"40\" fill=\"black\" x=\"50\" y=\"60\" filter=\"url(#filter2)\">Do you need glasses?</text> <g id=\"stardot\" transform=\"translate(100,25) scale(0.5)\"> <polygon id=\"star\" points=\"250,0 400,500 0,200 500,200 100,500\" fill=\"red\" fill-rule=\"evenodd\"/> <circle id=\"circ\" cx=\"250\" cy=\"283\" r=\"75\" fill=\"blue\" filter=\"url(#filter1)\" /> </g> </svg> The result is shown as follows: A Gaussian blur filter applied to different shapes. Code: SVG/25-filter.html. An SVG example The following code uses several SVG elements described in this section to draw some shapes, shadows, gradients and text: <svg width=\"300\" height=\"300\"> <defs> <filter id=\"shadow\"> <feDropShadow style=\"flood-color: green\" dx=\"5\" dy=\"5\" stdDeviation=\"3\"/> </filter> <linearGradient id=\"grad\" x1=\"0\" y1=\"0\" x2=\"100%\" y2=\"0\"> <stop offset=\"0%\" stop-color=\"magenta\"/> <stop offset=\"100%\" stop-color=\"yellow\"/> </linearGradient> <clipPath id=\"circle\"> <circle r=\"40\" cx=\"175\" cy=\"75\"/> </clipPath> </defs> <!-- rectangle --> <rect x=\"50\" y=\"50\" height=\"50\" width=\"50\" fill=\"red\" stroke=\"blue\" stroke-width=\"10\" stroke-opacity=\"0.5\"/> <!-- dashed shape --> <path id=\"path1\" d=\"M150,200 L150,150 L100,150 C100,200 150,250 200,250 L200,200 Z\" stroke-dasharray=\"5 2 1 2\" stroke-width=\"2\" stroke=\"blue\" fill=\"none\" style=\"filter:url(#shadow)\"/> <!-- gray quarter-circle --> <path d=\"M0,0 L0,-100 A100,100 0 0,0 -100,0 L0,0 Z\" transform=\"translate(100,250) scale(0.5) \" stroke=\"red\" stroke-opacity=\".5\" stroke-width=\"4\" fill-opacity=\".2\"/> <text fill=\"url(#grad)\" font-size=\"20\" x=\"200\" y=\"100\"> Scalable <tspan dy=\"20\" x=\"200\">Vector</tspan> <tspan dy=\"20\" x=\"200\">Graphics</tspan> </text> <image x=\"125\" y=\"25\" height=\"100\" width=\"100\" xlink:href=\"../Data/Images/pluto.jpg\" clip-path=\"url(#circle)\" opacity=\"0.75\"/> <!-- raindow half-circle --> <path d=\"M100,200 C100,100 250,100 250,200\" transform=\"scale(0.6) rotate(180,295,225) \" fill=\"url(#grad)\"/> </svg> Compare this code and the following image it generates it with an identical image created using D3 (SVG-with-D3/29-example.html) and HTML Canvas (Canvas/1-canvas-svg-compare.html): An image created using SVG. Code: SVG/29-example.html.","title":"Filters"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#2-essential-javascript-data-structures","text":"I'm actually going to organize this somewhere else to help with my code journal. Link to the Basics","title":"2) Essential JavaScript data structures"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#3-html-canvas","text":"HTML5 Canvas Most of your D3 applications will render graphics using SVG, but several shape generators in SVG can also generate Canvas, and you may choose to use Canvas in all or part of your application to improve performance if you have memory problems due to excessive objects created in the DOM. To draw using Canvas you need to create a <canvas> element in your page. You can do that using plain HTML: <body> <canvas id=\"canvas\" width=\"400\" height=\"300\"></canvas> </body> Or using D3: d3.select(\"body\").append(\"canvas\").attr(\"width\", 400).attr(\"height\", 300); If you declare the Canvas element in HTML, you can reference it by its ID using the DOM or D3: const canvas = d3.select(\"#canvas\").node(); // node() returns the DOM object Once you have a canvas object, you obtain a 2D graphics context and start drawing: const ctx = canvas.getContext(\"2d\"); Practically all the Canvas API consists of methods and properties called from the graphics context. Before drawing, you set properties such as font , fillStyle , strokeStyle : ctx.fillStyle = \"red\"; ctx.strokeStyle = \"rgba(255,127,0,0.7)\"; ctx.lineWidth = 10; And then fill or stroke rectangles and arbitrary paths containing lines and curves. The following commands will draw a red 50x50 pixel square with a 10 pixel wide yellow semi-transparent border at position 50,50: ctx.fillRect(50,50,50,50); ctx.strokeRect(50,50,50,50); You can also draw other shapes, text and images on the same canvas. The context properties will not change unless they are redefined or a previously saved context is restored. It's a good practice to save the context to the stack before applying properties or transforms, and restore it when you are done drawing an object. This allows you to always start with a clean context: Note: So far the below code does nothing that I can see with my eyes. ctx.save(); ctx.transform(50,60); ctx.scale(2); // \u2026 ctx.restore(); // starting with a new context Property or method Description fillStyle Sets the color to be used in fill() commands. strokeStyle Sets the color to be used in stroke() commands. lineWidth Sets the line width to be used in stroke() commands. lineCap Sets the style of the line caps: can be butt (default), round or square. textAlign Sets the alignment for text: can be start (default), center, end, left or right. textBaseline Sets the baseline for text: can be middle, hanging, top, bottom, ideographic or alphabetic (default). font Sets the font to be used in text commands, using the compact CSS font syntax. globalAlpha Sets the global opacity (0 = transparent, 1 = opaque) for the context. shadowBlur, shadowColor, shadowOffsetX, shadowOffsetY Sets shadow properties. Default shadow color is transparent black. Default numeric values are zero. setLineDash(dasharray) Sets the dash array (alternating line and space lengths) for strokes. translate(x,y) Sets the current translate transform for the context. scale(x,y) Sets the current scale transform for the context. rotate(angle) Sets the current rotate transform for the context. save() Saves the state of the current context (pushes into a stack). restore() Restores the state of the last context that was saved (pops it from the stack). The fillRect() command is typically used to clear the entire canvas before redrawing, but you can also use it to draw arbitrary rectangles. The following table lists methods you can use to draw rectangles, draw text and images: Method Description fillRect(x,y,w,h); Fills a rectangle. Typically used to clear the Canvas on redrawing. strokeRect(x,y,w,h) Draws a border around a rectangle. fillText(text,x,y); Fills text at position x, y (depends on current textAlign and textBaseline). strokeText(text, x, y); Draws a border around text. drawImage(image, x, y, w, h) Draws an image at x,y with width w and height h. Canvas context methods used to draw rectangles, text and images A path is a series of commands to move to points, draw lines, curves or arcs. To draw a path you need to first call ctx.beginPath() , then call a sequence of commands that move to points, draw lines and curves, and when you are done you can close the path (if it's a closed path) and call fill() and/or stroke() to draw it using the current styles. The following table lists several commands you can use in a path: Method Description beginPath() Starts a path. closePath() Closes a path. moveTo(x,y) Moves the cursor to a position in the path. lineTo(x,y) Moves the cursor to a position in the path, drawing a line along the way. bezierCurveTo(c1x,c1y,c2x,c2y,x,y) quadraticCurveTo(cx,cy,x,y) Draws curves with one (quadratic) or two (Bezier) control points in a path. arc(x,y,r,sa,ea) Draws an arc by specifying the center, radius, start and end angles in a path. arcTo(sx,sy,r,ex,ey) Draws an arc by specifying the coordinates of the starting point, the radius and the coordinates of the ending point. rect(x,y,w,h) Draws a rectangle in a path with coordinates of top-left corner, width and height. clip() Creates a clipping region with the shapes drawn by the path that will affect objects that are drawn afterwards. fill() Fills a path with the current color. Call this to fill the path when done. stroke() Strokes the path with the current color. Call this to stroke the path when done.","title":"3) HTML Canvas"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#a-canvas-example","text":"The following code uses several of the methods above to draw different shapes on the same Canvas context. It draws some shapes, text, images and paths, and applies transforms, shadows, clipping and gradients. Compare it to the example shown before in SVG that draws the image (see Canvas/1-canvas-svg-compare.html ): const canvas = document.getElementById(\"canvas\"); const ctx = canvas.getContext(\"2d\"); // rectangle ctx.save(); // save default context ctx.fillStyle = \"#ff0000\"; ctx.strokeStyle = \"blue\"; ctx.lineWidth = 10; ctx.fillRect(50,50,50,50); ctx.globalAlpha = 0.5; ctx.strokeRect(50,50,50,50); // dashed shape ctx.restore(); ctx.save(); ctx.strokeStyle = \"blue\"; ctx.lineWidth = 2; ctx.shadowBlur = 6; ctx.shadowColor = \"green\"; ctx.shadowOffsetX = ctx.shadowOffsetY = 5; ctx.setLineDash([5,2,1,2]); ctx.beginPath(); ctx.moveTo(150,200); ctx.lineTo(150,150); ctx.lineTo(100,150); ctx.bezierCurveTo(100,200,150,250,200,250); ctx.lineTo(200,200); ctx.closePath(); ctx.stroke(); ctx.restore(); ctx.save(); // quarter-circle ctx.translate(100,250); ctx.scale(0.5, 0.5); ctx.strokeStyle = \"red\"; ctx.lineWidth = 4; ctx.globalAlpha = 0.5; ctx.beginPath(); ctx.moveTo(0,0); ctx.lineTo(0,-100); ctx.arcTo(-100,-100,-100,0,100); ctx.lineTo(0,0); ctx.stroke(); ctx.globalAlpha = 0.2; ctx.beginPath(); ctx.arc(0,0,100,3.14,-1.57,false); ctx.lineTo(0,0); ctx.closePath(); ctx.fill(); ctx.restore(); ctx.save(); // text and half-circle const text = \"Canvas\" ctx.translate(250,150); ctx.font = \"24px monospace\"; const textWidth = ctx.measureText(text).width; const gradient = ctx.createLinearGradient(-50,-50,-50 + textWidth,-50); gradient.addColorStop(0,\"magenta\"); gradient.addColorStop(1, \"yellow\"); ctx.fillStyle = gradient; ctx.shadowColor = \"transparent\"; ctx.fillText(text, -45, -5); ctx.scale(1.1, 1.1) ctx.rotate(3.14); ctx.beginPath(); ctx.arc(0,0,40,3.14,0,false); ctx.fill(); ctx.restore(); ctx.save(); // image and clip ctx.beginPath(); ctx.arc(175,75,40,0,6.28,false); ctx.clip(); const image = new Image(100,100); image.onload = function() { ctx.globalAlpha = 0.75; ctx.drawImage(this, 125, 25, this.width, this.height); } image.src = \"reuse.png\"; ctx.save(); I don't have the pluto pic so the code won't show the planet.","title":"A Canvas Example"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#data-formats","text":"Data used in visualizations are usually distributed in a standard format that can be shared. Even when the data is served from a database, the data is usually delivered in some standard format. Popular proprietary formats such as Excel spreadsheets are common, but most statistical data is stored or delivered in CSV, XML or JSON formats.","title":"Data Formats"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#csv","text":"You can load and parse CSV in D3 using the d3.csv() function. const csv = d3.csv(\"Data/continents.csv\") csv output: Promise {<resolved>: Array(7)} __proto__: Promise [[PromiseStatus]]: \"resolved\" [[PromiseValue]]: Array(7) 0: {continent: \"North America\", population: \"579024000\", areakm2: \"24490000\"} 1: {continent: \"Asia\", population: \"4436224000\", areakm2: \"43820000\"} 2: {continent: \"Europe\", population: \"738849000\", areakm2: \"10180000\"} 3: {continent: \"Africa\", population: \"1216130000\", areakm2: \"30370000\"} 4: {continent: \"South America\", population: \"422535000\", areakm2: \"17840000\"} 5: {continent: \"Oceania\", population: \"39901000\", areakm2: \"9008500\"} 6: {continent: \"Antarctica\", population: \"1106\", areakm2: \"13720000\"} columns: (3) [\"continent\", \"population\", \"areakm2\"] length: 7 __proto__: Array(0)","title":"CSV"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#xml","text":"XML \u2013 eXtensible Markup Language is a very popular data format. Ajax responses from web services are usually returned as text or XML. It has standard native support in JavaScript via the DOM (document object model) APIs and doesn't require additional parsing. Although it is still common to find data in XML format, CSV and JSON alternatives, if available, are usually smaller and easier to work with. <continents> <continent> <name>North America</name> <population>579024000</population> <area unit=\"km\">24490000</area> </continent> <continent> <name>Asia</name> <population>4436224000</population> <area unit=\"km\">43820000</area> </continent> <continent> <name>Antarctica</name> <population>1106</population> <area>13720000</area> </continent> </continents> const xml = d3.xml(\"Data/continents.xml\") Not sure how to parse but who uses XML anymore gosh!","title":"XML"},{"location":"notes/javascript/D3js/Rocha/chapter_2/chapter-2/#json","text":"JSON stands for JavaScript Object Notation. It looks a lot like a JavaScript Object, but it has stricter formation rules. It's probably the easiest format to work with. It's compact and easy to parse, and it's gradually replacing XML as a preferred data format in Web Services. The data file containing continent data is shown below in JSON format ( Data/continents.json ). [ { \"continent\": \"North America\", \"population\": 579024000, \"areakm2\": 24490000 },{ \"continent\": \"Asia\", \"population\": 4436224000, \"areakm2\": 43820000 },{ \"continent\": \"Europe\", \"population\": 738849000, \"areakm2\": 10180000 },{ \"continent\": \"Africa\", \"population\": 1216130000, \"areakm2\": 30370000 },{ \"continent\": \"South America\", \"population\": 422535000, \"areakm2\": 17840000 },{ \"continent\": \"Oceania\", \"population\": 39901000, \"areakm2\": 9008500 },{ \"continent\": \"Antarctica\", \"population\": 1106, \"areakm2\": 13720000 } ] JSON is the preferred format for data manipulation in JavaScript. There are many online tools you can use to transform CSV and XML files into JSON. You can load and parse JSON in D3 using the d3.json() function. Promise {<resolved>: Array(7)} __proto__: Promise [[PromiseStatus]]: \"resolved\" [[PromiseValue]]: Array(7) 0: {continent: \"North America\", population: 579024000, areakm2: 24490000} 1: {continent: \"Asia\", population: 4436224000, areakm2: 43820000} 2: {continent: \"Europe\", population: 738849000, areakm2: 10180000} 3: {continent: \"Africa\", population: 1216130000, areakm2: 30370000} 4: {continent: \"South America\", population: 422535000, areakm2: 17840000} 5: {continent: \"Oceania\", population: 39901000, areakm2: 9008500} 6: {continent: \"Antarctica\", population: 1106, areakm2: 13720000} length: 7 __proto__: Array(0) Note that both csv and json loaded an easy to work with object. Just try to xml code....It'll make you sick.","title":"JSON"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/","text":"Source github Selecting and Binding Data Selecting and appending The selection methods select() and selectAll() receive a CSS selector expression and return a D3 handle for a node or a set of nodes. You can convert the D3 handle into a DOM object calling node() or nodes() . Add the following HTML to the <body> of your HTML file (or use Selecting/1-select.html): <p>See results in console log.</p> <div id=\"section\"> <p class=first>Paragraph 1</p> <p>Paragraph 2</p> </div> <p>Paragraph 3</p> Now you can use the JavaScript console (or a <script> block in your page) to type in the following code, which shows how select() and selectAll() can be used: const div = d3.select(\"#section\"); // selects element with the // \"section\" ID const domDiv = div.node(); // converts the object into a // DOM node const firstP = d3.select(\"p\"); // selects the first <p> // in the page const allParagraphs = d3.selectAll(\"p\"); // selects all four <p> nodes // in the page const allPDom = allParagraphs.nodes(); // converts selection into // DOM nodelist const sectionParagraphs = d3.select(\"div\") .selectAll(\"p\"); // selects the two <p> nodes // inside <div> A selection object serves as a handle to elements in your page. You can use it to change their styles, properties, attributes, classes, and contents, using methods that can receive static or dynamic values (using functions) as parameters. The following examples demonstrate some of these methods (see Selecting/2-attributes-styles.html ): div.style('border', 'solid blue 2px'); // draws blue border around <div> firstP.classed('big', true); // adds class \u2018.big\u2019 to first paragraph firstP.text('This is paragraph zero'); // replaces contents of first paragraph allParagraphs.style('font-weight', 'bold'); // applies style to set of paragraphs // this looks promising, however there is nothing tagged with class \"first\" so nothing happes d3.select('.first') // selects first element of class \u2018.first\u2019 .attr('title', 'Tooltip') // adds an attribute .style('font-variant', 'small-caps') // applies a style .html('This is a <b>bold</b> paragraph.'); // replaces contents with HTML The select() method will always return only one object (if the selector matches more items, only the first will be returned). The selectAll() method returns a collection, which can be iterated with the each() method, which receives a callback function, which receives up to three parameters. Inside the function, this refers to the current DOM element, and d3.select(this) wraps it in a D3 selection object, which can be manipulated with selection methods, as follows: sectionParagraphs.each(function(d, i) { // i is the index d3.select(this).classed('red', true); console.log(`Paragraph ${i}: `, this); // this refers to the current element }); ``` This code applies the class ```.red``` to each element of a collection. If you open the JavaScript console, it will print each element and its index (starting in zero). You can also use selection methods to modify the DOM tree. Calling ```remove()``` on a D3 selection removes elements in the selection. You can call ```append()``` on a selection to add an element as its last child, or use ```insert()``` to add the new element in a position determined by a selector passed as the second argument. These methods are demonstrated as follows (see ```Selecting/3-append-remove.html)```: ```js d3.select(\"#section\") // current context is element with id #section .insert(\"p\", '.first') // add a <p> before the child being of class \u2018.first\u2019 .append('a') // context is now <p>; inserts an <a> in <p> .attr('href', 'https://d3js.org') // context is now <a> .text(\"D3 website\"); // contents of the <a> element d3.select('div') // context is first <div> in page .select('p:last-of-type') // context is now last <p> in <div> .remove(); // context is detached <p> (parentNode is null) const div = d3.select('div') div.append(\"p\") // adds a new <p> as last child of selected <div> .text(\"New paragraph\"); // sets the text contents of <p> d3.select(\"body\") // selects the <body> element of the page .insert(\"h1\", \"*\") // adds a new <h1> as the first child (before *) .text(\"New title\"); // sets the text contents of <h1> d3.select(\"body\") .append(\"h2\") // adds a new <h2> as the last child .text(\"Footer\"); The append() and insert() methods can also be chained, since they return the nodes they added. You just have to pay attention to the current so you don't modify or add data to the wrong elements. The remove() method returns the removed selection (a node or a set of nodes) after it's detached from the DOM ( null parent). D3 selections are not limited to HTML. In fact, they are most commonly applied to SVG elements, but can also be used with any embedded XML. This was a brief introduction. In Chapter 4, Data Binding , we will explore these methods in detail. Binding Data D3 makes it almost trivial to bind data to DOM elements. Data binding requires calling at least two of the following four methods in the context of a selection: data(array) or datum(object) : Receives an array or an object/value that contains data that should be mapped to DOM elements join() , enter() or exit() : Binds the data to DOM elements by populating an update array returned as a selection of elements that should be added or removed from the document The exit() method returns a selection of unbound elements. After an exit() , remove() is usually called, which removes the entire selection. The enter() method returns a selection of placeholders for new elements that need to be created. After an enter() , either append() or insert() is called, which connects the elements to the DOM tree. The join() method is a magic method that replaces enter() and exit() , automatically updating, removing, or adding elements, as necessary. Once data is connected to an element, the attributes, style, and contents of each element can be modified using the attr() , style() , text() , and html() methods that take a callback function as the second argument. This binding process is illustrated as follows: Let's try an example. Create an HTML page with this code in the <body> and open it in a browser (or use Binding/1-binding.html ): In a <script> block, create this array of numbers (or type it in your JavaScript console): const numbers = [6, 2, 5, 7, 9]; The following code will bind each number in the array to a paragraph. The data will be copied to an internal array and can be used to change the attributes, styles, or contents of the existing paragraphs: const selection = d3.select(\"#section\") .selectAll(\"p\") .data(numbers); The data() method returns the current selection. The value of each item in the array is received as the first parameter of a callback function in attr(),style(),text(), and so on. The text() method is used in the following code that replaces the contents of each paragraph with the first two values of the array: selection.text(d => d); // binds to existing paragraphs (update) But we need more paragraphs, since we have five elements of data. By calling enter() , a new selection is created with three more placeholder nodes, containing the remaining data, but not yet mapped to any element: const newSelection = selection.enter(); // binds data to a selection of placeholders We can't just call text() on the selection yet. First, we need to append or insert those elements into the DOM tree. You call append() only once, and it will be executed three times. Now, you can use text() to print the paragraph with the data: newSelection.append(\"p\") .text(d => d); // binds to new paragraphs (enter) Normally, the entire selecting-binding-appending process is written as a chain of commands. The following code achieves the same result, using the same array and paragraphs (see Binding/3-chain-text.html ): d3.select(\"body\") // select the body element .select(\"div\") // select the first div element inside <body> .selectAll(\"p\") // select all <p> elements (there are two) .data(numbers) // load the data from the numbers array .text(d => d) // set contents of existing paragraphs .enter() // bind remaining data to array with 3 elements .append(\"p\") // add 3 new <p> elements to end of <div> .text(d => d); // set the contents of the new paragraphs It's simpler (and more common, when using D3) to use JavaScript (and not HTML) to create all of the elements you need. After all, data is dynamic and you won't always know how many items you have to display. Most of the time, HTML is just used to provide the basic structure, such as the <body> tag, a container <div> , or <svg> . The following code achieves the same result starting with an empty <body> tag (and the same data array). The <div> and all the <p> are created using D3 commands (see Binding/4-empty-binding.html ): d3.select(\"body\") // select the body element .append(\"div\") // append a div element inside <body> .selectAll(\"p\") // select all <p> elements (there are none) .data(numbers) // load the data from the numbers array .enter() // bind the data to enter array with 5 placeholders .append(\"p\") // create 5 new <p> elements and add to end of <div> .text(d => d); // set the contents of the new paragraphs Note that even though there are no <p> elements in the page, the selectAll(\"p\") command is still necessary, since it provides the selection context for the data binding. The selectAll() command can use any CSS selectors to locate its elements, but it should return a selection containing the same type of elements added by the append() command. As you have seen, once you have data bound to DOM elements, you can use the data to change attributes, styles, classes, and properties, using callbacks. The callback function has a second parameter that contains the index of the data array, which is used in the following example to change the contents and style of list items: d3.select(\"body\") // select the body element .append(\"ul\") // append an <ul> element inside <body> .selectAll(\"li\") // select all <li> elements (there are none) .data(numbers) // load the data from the numbers array .enter() // create an enter array with 5 objects .append(\"li\") // create 5 new <li> elements and append to <ul> .text(function(d, i) { return \"Item \" + (i+1) + \": \" + d; }) .style(\"font-size\", function(d, i) { return ((i+2) * 5) + \"pt\"; }); The result is shown in the following screenshot: D3 used to create and style new elements from data. Code: Binding/5-callbacks.html . Callback functions also have a third parameter that contains the array of elements in the selection. You might need it if you want to obtain the current element when using arrow functions, since the this reference in these functions doesn't refer to the current element: .text( (d, i, nodes) => console.log(\"Current element: \" + nodes[i]) ); You can easily reuse the preceding code with different types of data. If you have an array of objects containing values of different types, you don't need to convert it to an array of numbers. You can directly pass the entire object array to the data() method, and later select which properties of each object you wish to use. For example, you can use practically the same code that was shown previously to display the contents of the following array: const distances = [ {name: \"Mercury\", distance: 0.387}, {name: \"Venus\", distance: 0.723}, {name: \"Earth\", distance: 1}, {name: \"Mars\", distance: 1.52}, {name: \"Jupiter\", distance: 5.2}, {name: \"Saturn\", distance: 9.54}, {name: \"Uranus\", distance: 19.2}, {name: \"Neptune\", distance: 30.1}, {name: \"Ceres\", distance: 2.765}, {name: \"Pluto\", distance: 39.481}, {name: \"Eris\", distance: 67.67}, {name: \"Haumea\", distance: 43}, {name: \"Makemake\", distance: 45.346} ]; The object array is received by the data() command. Each array item is available as the first parameter of the callback, which can be used to extract the property that contains the data (see Binding/6-object-array.html ): d3.select(\"body\") // select the body element .append(\"ul\") // append an <ul> element inside <body> .selectAll(\"li\") // select all <li> elements (there are none) .data(distances) // load the data from the numbers array .enter() // bind the data to enter array with 5 placeholders .append(\"li\") // create <li> from enter selection and append to <ul> .text(function(d, i) { return d.name + \": \" + d.distance; }); Since D3 Version 5.8, you can replace enter().append(element) in simple binding operations, as shown with the join(element) method (see Binding/7-join.html ): d3.select(\"body\") .append(\"ul\") .selectAll(\"li\") .data(distances) .join(\"li\") // obtains enter selection and appends 5 new <li> elements .text(function(d, i) { return d.name + \": \" + d.distance; }); The preceding code will generate an HTML list with the data. Creating a bar chart with D3 and HTML Now that you have had a quick introduction to D3's basic data-binding mechanisms, let's create some full data visualization examples that will demonstrate the power of D3. You should know HTML very well, so we will start with a simple bar chart using HTML and CSS. Later, we will repeat the exact same procedure using SVG, the main standard used by D3 to create visualizations. All the code for these examples is available in the GitHub repository for this chapter. Binding data to HTML In this example, we will draw a horizontal bar chart using HTML <div> elements. The colors and length of each bar are controlled by CSS, which we can configure using the selection.style() command. We will use the object array containing planetary distances from the last example. First, let's create a style sheet with classes for the bar-chart and each individual bar : <style> .bar-chart { border: solid 1px gray; /* a gray border around the container */ position: relative; } .bar { height: 20px; background-color: orange; position: absolute; } </style> The bars are positioned absolutely. Since each bar has an equal height (20px), we need to position each bar slightly below the previous one. This can be achieved by setting the value for the CSS top property (in pixels) so that it contains the sum of the heights of all the previous bars, plus an extra pixel to keep them slightly apart. The top property can be dynamically calculated as proportional to each entry's array index, using a callback in selection.style() (see HTML_Bar/1-bar-chart.html ): d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") // container div for the chart .style(\"height\", () => distances.length * 21 + \"px\") // set chart height .selectAll(\"div\").data(distances) // binds data .enter().append(\"div\") // appends a div for each data element .attr(\"class\", \"bar\") // these divs are the bars of the chart .style(\"top\", (d,i) => i * 21 + \"px\") // stacks bars .style(\"width\", \"100px\") // fixed width output: This prints all the bars, one on top of the other, but they all have the same static width! We need to use a callback to change the width of each individual bar. In a bar chart, the actual width should be proportional to the distance. A solution would be to multiply each value by 10. This will make the smallest bar three pixels wide, and the largest over 670 pixels: .style(\"width\", d => (d.distance * 10) + \"px\") Now, the lengths of the bars are proportional to the data values. This code is in HTML_Bar/2-bar-width.html . The result is shown as follows: HTML bar chart with D3: Drawing DIVs and using the data to change the CSS width. Code: HTML_Bar/2-bar-width.html Scales What if a new planet is discovered that is 10 times the largest distance? The bars wouldn't fit and their widths would need to be recalculated! D3 also provides a solution for this: scales . Scales are mappings between different dimensions. In our case, we have to deal with two dimensions: the dimension of the data, called the domain , and the dimension of the graphics context where the data will be displayed, called the range . Before using a scale, you have to configure its domain and range. There are several different kinds of scales in D3, including linear and logarithmic scales, with many configuration options. A scale is created with a special generator function available in the d3-scale module. To use this module, you need to load several dependencies, so let's replace the <script> tag with the default bundle: <script src=\"https://d3js.org/d3.v5.min.js\"></script> To create a scale function, you call a special generator function. A linear scale function can be created using the following: const barScale = d3.scaleLinear(); To use the scale function you created, you pass a value as the argument and receive the converted result: const result = barScale(45); // returns 45 This will return 45 , because the scale hasn't been configured yet (the default scale is 1:1). To configure the scale, you need to call two methods to set up the domain (data dimensions) and range (graphical dimensions). Each method receives an array. For example, we can set up the domain as follows: barScale.domain([0, 100]); // input domain from 0 to 100 This fits all the distances in our data. The range is set up in a similar fashion: barScale.range([0, 600]); // output domain from 0 to 600 This means that zero is mapped to zero, and 100 astronomical units (AU) is mapped to 600 (pixels). Intermediate values will be interpolated. If you now call barScale(45) , you will get 270 . We can use the scale to convert the distances in AU to pixels, replacing the previous expression with a scale conversion (see HTML_Bar/3-scales.html ): .style(\"width\", d => barScale(d.distance) + \"px\"); Array utilities We chose 100 as the upper limit in our scale domain because it's larger than any of the distances in the data, but the choice was rather arbitrary. We could have chosen the largest value in the array. If you have hundreds of lines of data, you can use JavaScript array functions (see Chapter 1, Introduction ) to find out the maximum value. D3 also includes a collection of array manipulation functions that extend JavaScript's native functions. Some look similar but may be more efficient for data manipulation (for example, by ignoring null , NaN , and undefined values). In order to use them, you need the d3-array module (which is also part of the default bundle). Here, we changed the configuration of our barScale() function so that it uses the largest distance from the distances object array as the upper value for the domain. This is achieved with by calling the d3.max() function, which receives an array and an accessor function for each array element, as follows: const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); The d3.max() function will scan the distances array and compare the distance property of each object, returning the largest one. There are many more useful functions in d3-array , which we will cover in the next chapter. Two of them, d3.descending(a,b) and d3.ascending(a,b) , are used to provide a sorting rule for JavaScript's native sort() method. We can use it to sort the array by the distance: distances.sort((a,b) => d3.ascending(a.distance, b.distance)); See and run the code after these transformations in HTML_Bar/4-max-sort.html . Adding labels Since the <div> elements in HTML can contain text, you can call the text() method for each <div> and set its contents based on the data: d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") // ...all previous code to build chart .text(d => d.distance); // display distance in the <div> With CSS, we can right-align the text and adjust the fonts. Since these styles are static and don't change with the data, instead of calling style() for each property, you should use a style sheet: <style> /* ... */ .bar { height: 20px; left: 100px; background-color: orange; position: absolute; text-align: right; padding: 0 5px; font-family: sans-serif; font-size: 9pt; } </style> As a result, the labels are placed inside the bars, as follows: Adding labels to the bars. Code: HTML_Bar/5-labels.html . More labels, formatting, and colors Each object in our data array also contains a name property that can be used to label each bar. To position text outside the bar, we will need to refactor the code so that each data entry contains a container <div> , which will be bound to the data values. This entry <div> will then contain three other <div> elements: a category label (the name of the planet), the bar, and a value label (the distance). Classes will be used to identify each <div> . The following style sheet contains the static properties for these elements: <style> .bar-chart { /* The container <div> for the entire chart */ border: solid 1px gray; position: relative; width: 800px; } .entry { /* a container <div> for each data entry */ position: absolute; width: 100%; } .bar { /* the colored rectangle */ height: 20px; top: 1px; left: 100px; background-color: orange; position: absolute; } .label { /* a text label */ padding: 4px 5px; font-family: sans-serif; font-size: 9pt; position: absolute; height: 20px; } .category { /* the category text label at left (name) */ text-align: right; width: 80px; } .value { /* the value text label at right (distance) */ text-align: left; } </style> Since each entry <div> has three children, we need to keep a reference to its selection so that the child elements can be appended. The following code saves references for each one of the container <div> elements. The chart constant contains a selection of the root <div> element, and the entries constant contains the selection of all entry <div> elements: // selects the entire chart (one node) const chart = d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21 + \"px\"); // selects each entry (a nodelist) const entries = chart.selectAll(\"div\").data(distances) .enter().append(\"div\") .attr(\"class\", \"entry\") .style(\"top\", (d,i) => i * 21 + \"px\"); You can now use the entries constant to append the child elements to each entry <div> (see HTML_Bar/6-entries.html ): entries.append(\"div\").attr(\"class\", \"label category\") .text(d => d.name); entries.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", d => barScale(d.distance) + \"px\"); entries.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", d => (barScale(d.distance) + 100) + \"px\") .text(d => d.distance + \" AU\"); Each append() shown is called once for each entry. The attributes, style, and text are set using the data that was bound to the parent container. You can add child elements without having to break the selection chain with the selection.each() method, which calls a function for each entry. Inside it, you can obtain a selection to the current element using d3.select(this) . The following code produces the same result (see HTML_Bar/7-entries-each.html ): entries.each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"div\").attr(\"class\", \"label category\") .text(d.name); entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\"); entry.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", (barScale(d.distance) + 100) + \"px\") .text(d.distance + \" AU\"); }); We can improve the rendering of the labels by formatting the numbers to display only two decimal places, using the d3.format() generator function (from the d3-format module). The following code creates a function fmt() that can be used to format numbers: const fmt = d3.format(\".2f\"); Now, we can use it to format the distances: .text(d => fmt(d.distance) + \" AU\"); The final result is shown as follows: Adding category names and placing the formatted label values outside the bars. Code: HTML_Bar/7-entries-each.html Changing colors The d3-colors module contains functions to generate, convert, and transform colors. Passing any CSS-compatible color representation to d3.color() generates an object with methods that can be used to modify the color. The darker() and brighter() methods receive a value between 0 (no change) and 1 (maximum change) to adjust the lightness component of a color and return a hexadecimal color string. Let's use this feature to darken the bars when the distance from the sun increases. We will need a scale that maps the distance domain to the [0-1] range, which contains the values accepted by the darker() function: colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) Now, this colorScale can be used to generate different tones for the bars: entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\") .style(\"background-color\", d3.color('orange') .darker(colorScale(d.distance))) See the code in HTML_Bar/8-colors.html . The result is shown as follows: Changing bar colors with the distance. Code: HTML_Bar/8-colors.html Loading external files Usually, your data will come from external files that need to be loaded with an Ajax request. You can, of course, use jQuery or ECMAScript fetch() commands (and promises, if you need to load multiple files). After loading, you will also need to parse the data and convert it into JavaScript arrays and objects. D3 again provides a more efficient solution in the d3-fetch module: a set of convenient methods for loading and parsing files in popular formats, such as XML, JSON, or CSV. The d3-fetch module is also included in the default bundle. The planetary data we used in the previous examples is actually part of a larger JSON file containing several properties for planets, asteroids, and satellites. The basic structure of this file ( Data/sol_2016.json ) is shown as follows: { \"star\":{\u2026}, \"planets\":[ { \"id\":\"p1\", \"name\":\"Mercury\", \"diameterKm\":4879, \"semiMajorAxisAU\":0.387, \u2026 },{ \"id\":\"p2\", \"name\":\"Venus\", \"diameterKm\":12104, \"semiMajorAxisAU\":0.723, \u2026 }, \u2026 } ``` You can load the data using the ```d3.json()``` function. After it loads and parses the file, it will become available in a callback provided as a parameter to the ```then()``` method (which is a JavaScript promise). We don't need all the loaded data. The distance used in our examples is stored in the property called ```semiMajorAxisAU```. After loading the file, we can filter the dataset to only save the ```semiMajorAxisAU``` and ```name``` properties. The following code demonstrates this. It loads the file, and it then uses the data obtained in the ```then()``` callback to loop through the planets array, adding only the chosen properties to a new object and pushing it into an array. The array is used to create an HTML list with these properties (see``` Loading/1-loading-json.html```): ```js //d3.json(\"../Data/sol_2016.json\") // if you down load it d3.json(\"https://raw.githubusercontent.com/PacktPublishing/Learn-D3.js/master/Chapter03/Data/sol_2016.json\") // if you use the git hub data. This is so cool! .then(function(data) { const planets = []; data.planets.forEach(function(obj) { planets.push({ name: obj.name, distance: obj.semiMajorAxisAU }); }); draw(planets); }); function draw(distances) { d3.select(\"body\").append(\"ol\") .selectAll(\"li\") .data(distances) .join(\"li\") .text(d => d.name + \" (\" + d.distance + \" AU)\"); } To apply this to our chart, you just need to replace the contents of the draw() method shown with the entire code used in the previous examples (except for the distances array). See an example in HTML_Bar/9-load-json.html . As an exercise, try feeding the bar chart with a list of Jupiter's moons, instead of the planets (use data.planets[4].satellites to obtain the array, and the semiMajorAxisKm property as the distance, and d3.format(\",.0f\") to format the distance values). Full Example: d3.json(\"https://raw.githubusercontent.com/PacktPublishing/Learn-D3.js/master/Chapter03/Data/sol_2016.json\") // if you use the git hub data. This is so cool! .then(function(data) { const planets = []; data.planets.forEach(function(obj) { planets.push({name: obj.name, distance: obj.semiMajorAxisAU}); }); draw(planets); }); function draw(distances) { const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); const colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) // selects the entire chart (one node) const chart = d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21 + \"px\"); // selects each entry (a nodelist) const entries = chart.selectAll(\"div\").data(distances.sort((a,b) => d3.ascending(a.distance, b.distance))) .enter().append(\"div\") .attr(\"class\", \"entry\") .style(\"top\", (d,i) => i * 21 + \"px\"); entries.each(function(d) { const entry = d3.select(this); // the current entry const fmt = d3.format(\".2f\"); // format for each entry entry.append(\"div\").attr(\"class\", \"label category\") .text(d.name); entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\") .style(\"background-color\", d3.color('orange') .darker(colorScale(d.distance))) entry.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", (barScale(d.distance) + 100) + \"px\") .text(d => fmt(d.distance) + \" AU\"); }); } Creating a bar chart with D3 and SVG It's easy to create a horizontal bar chart in HTML. It's a bit trickier to make a vertical one, and quite challenging to create other charts, such as line charts, pie charts, and network diagrams, since HTML wasn't intended for vector graphics. While you can create such charts using HTML and CSS, it's not at all simple, nor is it recommended . You can, of course, use Canvas, but there are no graphical DOM elements to bind in Canvas. Canvas is used in D3 charts, but mostly for performance optimization. D3 works best with SVG . The same bar chart that we created with HTML can be created with SVG. Do you think you can do it? Why not try it as an exercise before proceeding? You already learned how to use D3 and HTML, and you know the basics of SVG rectangles, attributes, and styles. That's all you need! Most of the code is the same. You can reuse the scales, formatting function, sorting function, and JSON parsing code. The CSS will be simpler, since positioning is done in SVG. You only need to change the selection code, replacing the bar <div> element with <rect> , text label <div> elements with <text> , and container <div> elements with <g> . Remember to use fill (and not background-color) to fill the bars. The same step-by-step files that were used in the HTML example are available for the SVG example in the SVG_Bar/ folder . The following, final code produces exactly the same chart as the HTML version. This is the CSS style sheet that's used. Since all positioning is done in SVG, it's much smaller: <style> * { font-family: sans-serif; } .bar-chart { border: solid 1px gray; width: 800px; } .bar { height: 20px; fill: orange; } .label { font-size: 9pt; } .category { text-anchor: end; } </style> The main container is the <svg> element, which is appended to the body, as shown in the following code snippet. We renamed the chart object svg : // selects the entire chart (one node) const svg = d3.select(\"body\") .append(\"svg\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21); The containers used for each entry are <g> elements, which group several child elements and can have their own coordinate system configured using a transform . Although the transform property is supported in both CSS and SVG, you should use the SVG version (selected with the attr() method), because it considers pixel values and degrees as the default. You don't have to append deg or px to any values: // selects each entry (a nodelist) const entries = svg.selectAll(\"g\").data(distances) .enter().append(\"g\") .attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0, ${i * 21})`); The entries are appended to the container <g> in the each() method, as follows. Compare this code to the HTML version: // sort distances distances = distances.sort((a,b) => d3.ascending(a.distance, b.distance)); // selects the entire chart (one node) const svg = d3.select(\"body\") .append(\"svg\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21); // barScales const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); // colorScale const colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) // decimal format const fmt = d3.format(\".2f\"); // selects each entry (a nodelist) const entries = svg.selectAll(\"g\").data(distances) .enter().append(\"g\") .attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0, ${i * 21})`); entries.each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"text\").attr(\"class\", \"label category\") .attr(\"y\", 15) .attr(\"x\", 90) .text(d.name); entry.append(\"rect\").attr(\"class\", \"bar\") .attr(\"x\", 100) .attr(\"width\", barScale(d.distance) + \"px\") .style(\"fill\", d3.color('orange') .darker(colorScale(d.distance))) entry.append(\"text\").attr(\"class\", \"label value\") .attr(\"y\", 15) .attr(\"x\", barScale(d.distance) + 105) .text(fmt(d.distance) + \" AU\"); }); Try the full code from SVG_Bar/9-load-json.html . If you open it in your browser, you will notice that the result is identical to the one you get with HTML_Bar/9-load-json.html . To explore more D3 features, in the next sections, we will use the SVG version of this bar chart. Updating Data Visualizations Once the data is bound to SVG elements, you can change the original data values and reflect the changes in the chart. Values can change immediately or transition smoothly. This section will provide an introduction on how to trigger data updates and configure smooth transitions using D3. First, we need more data. Let's change the code where we create our data object and include two more values that use the same dimensions: the aphelium (the longest distance between a planet and the sun) in the max property, and the perihelium (the shortest distance) in the min property. The distance is now stored in the avg property (see Updating/1-three-charts.html ): const planets = []; // this array will store all the data when loaded d3.json(\"../Data/sol_2016.json\") .then(function(data) { data.planets.forEach(function(obj) { planets.push({ name: obj.name, avg: obj.semiMajorAxisAU, max: obj.apheliumAU, min: obj.periheliumAU}); }); init(); }); You can create bar charts with any of these three values, or all of them. Let's create an application where the user can choose which chart to display using HTML buttons. The following is the HTML code for the page; it includes an <svg> element and some buttons: <body> <h1><span id=\"chart\">Average</span> distance from the Sun</h1> <svg class=\"bar-chart\"></svg> <form> <button type=\"button\" id=\"avg\">Average</button> <button type=\"button\" id=\"max\">Maximum</button> <button type=\"button\" id=\"min\">Minimum</button> </form> <script>...</script> </body> These buttons will be attached to event handlers. They will select the data that should be displayed in the bar charts. The following array relates a key to a title and a color. The key contains the name of a property from each element in the planets array. It's also used for the button IDs: const charts = [ {key: \"avg\", title: \"Average\", color: \"orange\"}, {key: \"max\", title: \"Maximum\", color: \"blue\"}, {key: \"min\", title: \"Minimum\", color: \"red\"}, ]; The chart object stores the dimensions of the current chart (which may have a variable height) and the chart that is currently displayed: const chart = { width: 800, height: 0, // the height is set after data is loaded current: charts[0] // chart to display first } These other global constants initialize scales, a formatting function, and a selection of the svg object: const barScale = d3.scaleLinear().range([0, 600]); const colorScale = d3.scaleLinear().range([0, 1]); const format = d3.format(\".2f\"); const svg = d3.select(\"svg.bar-chart\"); // the container SVG The init() function that is called right after the data is loaded and the planets array is populated performs basic initialization that requires the loaded data. In this case, it sets the height of the chart: function init() { // runs once chart.height = planets.length * 21; svg.attr(\"width\", chart.width) .attr(\"height\", chart.height); setupView(); // sets up scales; Not defined yet...see below // ... } The setupView() function called inside init() configures the current view. It disables the button that refers to the currently displayed view, replaces the <span> element in the title with the title of the current chart, sorts the planets array, and initializes the domains of the scales, based on the current data: function setupView() { // disable all buttons d3.selectAll(\"button\").property(\"disabled\", false); // enable only buttons that are not current chart d3.select(\"#\" + chart.current.key).property(\"disabled\", true); // update page title d3.select(\"#chart\").text(chart.current.title); // sort the planets using current data planets.sort((a,b) => d3.ascending(a[chart.current.key], b[chart.current.key])); // update scale domain with current data const maxValue = d3.max(planets, d => d[chart.current.key]); barScale.domain([0, maxValue]); colorScale.domain([0, maxValue]); } After setting up the view, the init() function uses the current data to render the chart. This code is identical to the code we used in the previous examples. The only difference is that it uses a key reference to access the data property: instead of d.avg , it uses d[chart.current.key] . This will allow the chart to reference other properties when the current key changes: function init() { // ... setupView(); svg.selectAll(\"g\") .data(planets) .enter().append(\"g\").attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0,${i * 21})`) .each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"text\").attr(\"class\", \"label category\") .attr(\"y\", 15) .attr(\"x\", 90) .text(d.name); entry.append(\"rect\").attr(\"class\", \"bar\") .attr(\"x\", 100) .attr(\"height\", 20) .attr(\"width\", barScale(d[chart.current.key]) ) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key])) ) entry.append(\"text\").attr(\"class\", \"label value\") .attr(\"y\", 15) .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); } The result is shown as follows. It's the same chart we created before, with a title and some buttons that don't work yet: The previous example adapted to show three different charts. Code: Updating/1-three-charts.html. Handling events The on() method is used to handle events, and it can be called from any selection. The first parameter is a standard JavaScript event name string (such as click or mouseover ), and the second parameter is the handler function that will execute when the event happens. The following code obtains a selection containing all button objects and attaches an event handler to all of them. It obtains the id of the button that was clicked and uses it to change the current chart by assigning a corresponding object from the charts array. After changing the current chart, it calls the draw() function, which will update the chart. This code should be placed in a global context, since it only needs to run once: d3.selectAll(\"button\") .on(\"click\", function() { chart.current = charts.filter(c => c.key == this.id)[0]; draw(); }); The draw function in this example only prints the current array. You can use it to test whether the buttons are selecting the correct chart, as expected. The call to setupView() will disable/enable the buttons and update the chart's title according to the current view (see Updating/2-events.html ): function draw() { console.log(chart.current.key); setupView(); } Data updates To update the data on a selection, you just need to update the styles and attributes. If the data has changed, you should call the selection.data() method with the new data, and then update everything that depends on it, such as attributes and styles, and any functions called by them, such as scales. In our case, the data bound to the container g.entry object is the entire planets array, which may have been sorted in a different order (in the setupView() function). It can be updated simply by reassigning it to the selection: svg.selectAll(\"g.entry\").data(planets) Then, you need to update the attributes and styles, which depend on properties from this array that have changed, but before doing that, you need to update the scales' domains, since they are called from the style() and attr() methods, and their maximum value depends on the new data. The scales were updated in the setupView() function. The rest of the draw() function contains the data updates: function draw() { setupView(); // sorts data and updates scales svg.selectAll(\"g.entry\").data(planets) .each(function (d) { d3.select(this).select(\".label.category\") .text(d.name); // the order may have changed d3.select(this).select(\".bar\") .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); d3.select(this).select(\".label.value\") .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); } Now, when you click any button, a new bar chart will be displayed. The chart reuses the same graphical elements, changing its colors, dimensions, and text contents. Bars are always sorted in ascending order (see Updating/3-updates.html ): Each button replaces the current chart with a new chart, reusing the same chart elements. Code: Updating/3-updates.html . Smooth transitions Instead of immediately replacing one chart with another, you can add transitions, so that they occur in smooth animations. Adding a transition is very easy. You just need to call the transition() method on a selection before setting the new properties and styles. Instead of changing the old data with the new data, it will interpolate intermediate values during a quarter of a second. Add it to your update selections, and you will notice that when you change the chart, the bars and labels will animate to their new sizes, colors, and positions. In our examples, transitions were added before changing the color and width of each bar, and before changing the position of each value label (see Updating/4-transitions.html ): svg.selectAll(\"g.entry\").data(planets) .each(function (d) { d3.select(this).select(\".label.category\").text(d.name); d3.select(this).select(\".bar\") .transition() // 1) transition fill and width .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); d3.select(this).select(\".label.value\") .transition() // 2) transition x position .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); If you want a slower transition, just add duration(value) after the transition() command, with a value in milliseconds. The following code will make the animation last one second (see Updating/5-durations.html ): d3.select(this).select(\".bar\") .transition() .duration(1000) // animate during a second .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); You can also configure a delay before the transition starts with the delay() method. It can receive a fixed value in milliseconds, or a callback function that will apply a different delay for each object. In the following example, each bar will wait an amount of milliseconds proportional to its array index (the second parameter in the each() function), making each bar animate in a sequence: .each(function (d,i) { // include the i (index) parameter // ... d3.select(this).select(\".bar\") .transition() .duration(1000) // animate during a second .delay(50 * i) // longer animations for each bar .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); // ... }); The following diagram shows a snapshot of the animation captured in the middle of a transition. Try it out and see the effects for yourself (see the code in Updating/6-delays.html ): Transition with delay. Code: Updating/6-delays.html . So far, you've used the most common features of D3, rendering charts in HTML and SVG. We will finish this chapter with a D3 visualization of a map of the world, using the same methods you have used in the previous examples.","title":"Chapter 3"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#selecting-and-binding-data","text":"","title":"Selecting and Binding Data"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#selecting-and-appending","text":"The selection methods select() and selectAll() receive a CSS selector expression and return a D3 handle for a node or a set of nodes. You can convert the D3 handle into a DOM object calling node() or nodes() . Add the following HTML to the <body> of your HTML file (or use Selecting/1-select.html): <p>See results in console log.</p> <div id=\"section\"> <p class=first>Paragraph 1</p> <p>Paragraph 2</p> </div> <p>Paragraph 3</p> Now you can use the JavaScript console (or a <script> block in your page) to type in the following code, which shows how select() and selectAll() can be used: const div = d3.select(\"#section\"); // selects element with the // \"section\" ID const domDiv = div.node(); // converts the object into a // DOM node const firstP = d3.select(\"p\"); // selects the first <p> // in the page const allParagraphs = d3.selectAll(\"p\"); // selects all four <p> nodes // in the page const allPDom = allParagraphs.nodes(); // converts selection into // DOM nodelist const sectionParagraphs = d3.select(\"div\") .selectAll(\"p\"); // selects the two <p> nodes // inside <div> A selection object serves as a handle to elements in your page. You can use it to change their styles, properties, attributes, classes, and contents, using methods that can receive static or dynamic values (using functions) as parameters. The following examples demonstrate some of these methods (see Selecting/2-attributes-styles.html ): div.style('border', 'solid blue 2px'); // draws blue border around <div> firstP.classed('big', true); // adds class \u2018.big\u2019 to first paragraph firstP.text('This is paragraph zero'); // replaces contents of first paragraph allParagraphs.style('font-weight', 'bold'); // applies style to set of paragraphs // this looks promising, however there is nothing tagged with class \"first\" so nothing happes d3.select('.first') // selects first element of class \u2018.first\u2019 .attr('title', 'Tooltip') // adds an attribute .style('font-variant', 'small-caps') // applies a style .html('This is a <b>bold</b> paragraph.'); // replaces contents with HTML The select() method will always return only one object (if the selector matches more items, only the first will be returned). The selectAll() method returns a collection, which can be iterated with the each() method, which receives a callback function, which receives up to three parameters. Inside the function, this refers to the current DOM element, and d3.select(this) wraps it in a D3 selection object, which can be manipulated with selection methods, as follows: sectionParagraphs.each(function(d, i) { // i is the index d3.select(this).classed('red', true); console.log(`Paragraph ${i}: `, this); // this refers to the current element }); ``` This code applies the class ```.red``` to each element of a collection. If you open the JavaScript console, it will print each element and its index (starting in zero). You can also use selection methods to modify the DOM tree. Calling ```remove()``` on a D3 selection removes elements in the selection. You can call ```append()``` on a selection to add an element as its last child, or use ```insert()``` to add the new element in a position determined by a selector passed as the second argument. These methods are demonstrated as follows (see ```Selecting/3-append-remove.html)```: ```js d3.select(\"#section\") // current context is element with id #section .insert(\"p\", '.first') // add a <p> before the child being of class \u2018.first\u2019 .append('a') // context is now <p>; inserts an <a> in <p> .attr('href', 'https://d3js.org') // context is now <a> .text(\"D3 website\"); // contents of the <a> element d3.select('div') // context is first <div> in page .select('p:last-of-type') // context is now last <p> in <div> .remove(); // context is detached <p> (parentNode is null) const div = d3.select('div') div.append(\"p\") // adds a new <p> as last child of selected <div> .text(\"New paragraph\"); // sets the text contents of <p> d3.select(\"body\") // selects the <body> element of the page .insert(\"h1\", \"*\") // adds a new <h1> as the first child (before *) .text(\"New title\"); // sets the text contents of <h1> d3.select(\"body\") .append(\"h2\") // adds a new <h2> as the last child .text(\"Footer\"); The append() and insert() methods can also be chained, since they return the nodes they added. You just have to pay attention to the current so you don't modify or add data to the wrong elements. The remove() method returns the removed selection (a node or a set of nodes) after it's detached from the DOM ( null parent). D3 selections are not limited to HTML. In fact, they are most commonly applied to SVG elements, but can also be used with any embedded XML. This was a brief introduction. In Chapter 4, Data Binding , we will explore these methods in detail.","title":"Selecting and appending"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#binding-data","text":"D3 makes it almost trivial to bind data to DOM elements. Data binding requires calling at least two of the following four methods in the context of a selection: data(array) or datum(object) : Receives an array or an object/value that contains data that should be mapped to DOM elements join() , enter() or exit() : Binds the data to DOM elements by populating an update array returned as a selection of elements that should be added or removed from the document The exit() method returns a selection of unbound elements. After an exit() , remove() is usually called, which removes the entire selection. The enter() method returns a selection of placeholders for new elements that need to be created. After an enter() , either append() or insert() is called, which connects the elements to the DOM tree. The join() method is a magic method that replaces enter() and exit() , automatically updating, removing, or adding elements, as necessary. Once data is connected to an element, the attributes, style, and contents of each element can be modified using the attr() , style() , text() , and html() methods that take a callback function as the second argument. This binding process is illustrated as follows: Let's try an example. Create an HTML page with this code in the <body> and open it in a browser (or use Binding/1-binding.html ): In a <script> block, create this array of numbers (or type it in your JavaScript console): const numbers = [6, 2, 5, 7, 9]; The following code will bind each number in the array to a paragraph. The data will be copied to an internal array and can be used to change the attributes, styles, or contents of the existing paragraphs: const selection = d3.select(\"#section\") .selectAll(\"p\") .data(numbers); The data() method returns the current selection. The value of each item in the array is received as the first parameter of a callback function in attr(),style(),text(), and so on. The text() method is used in the following code that replaces the contents of each paragraph with the first two values of the array: selection.text(d => d); // binds to existing paragraphs (update) But we need more paragraphs, since we have five elements of data. By calling enter() , a new selection is created with three more placeholder nodes, containing the remaining data, but not yet mapped to any element: const newSelection = selection.enter(); // binds data to a selection of placeholders We can't just call text() on the selection yet. First, we need to append or insert those elements into the DOM tree. You call append() only once, and it will be executed three times. Now, you can use text() to print the paragraph with the data: newSelection.append(\"p\") .text(d => d); // binds to new paragraphs (enter) Normally, the entire selecting-binding-appending process is written as a chain of commands. The following code achieves the same result, using the same array and paragraphs (see Binding/3-chain-text.html ): d3.select(\"body\") // select the body element .select(\"div\") // select the first div element inside <body> .selectAll(\"p\") // select all <p> elements (there are two) .data(numbers) // load the data from the numbers array .text(d => d) // set contents of existing paragraphs .enter() // bind remaining data to array with 3 elements .append(\"p\") // add 3 new <p> elements to end of <div> .text(d => d); // set the contents of the new paragraphs It's simpler (and more common, when using D3) to use JavaScript (and not HTML) to create all of the elements you need. After all, data is dynamic and you won't always know how many items you have to display. Most of the time, HTML is just used to provide the basic structure, such as the <body> tag, a container <div> , or <svg> . The following code achieves the same result starting with an empty <body> tag (and the same data array). The <div> and all the <p> are created using D3 commands (see Binding/4-empty-binding.html ): d3.select(\"body\") // select the body element .append(\"div\") // append a div element inside <body> .selectAll(\"p\") // select all <p> elements (there are none) .data(numbers) // load the data from the numbers array .enter() // bind the data to enter array with 5 placeholders .append(\"p\") // create 5 new <p> elements and add to end of <div> .text(d => d); // set the contents of the new paragraphs Note that even though there are no <p> elements in the page, the selectAll(\"p\") command is still necessary, since it provides the selection context for the data binding. The selectAll() command can use any CSS selectors to locate its elements, but it should return a selection containing the same type of elements added by the append() command. As you have seen, once you have data bound to DOM elements, you can use the data to change attributes, styles, classes, and properties, using callbacks. The callback function has a second parameter that contains the index of the data array, which is used in the following example to change the contents and style of list items: d3.select(\"body\") // select the body element .append(\"ul\") // append an <ul> element inside <body> .selectAll(\"li\") // select all <li> elements (there are none) .data(numbers) // load the data from the numbers array .enter() // create an enter array with 5 objects .append(\"li\") // create 5 new <li> elements and append to <ul> .text(function(d, i) { return \"Item \" + (i+1) + \": \" + d; }) .style(\"font-size\", function(d, i) { return ((i+2) * 5) + \"pt\"; }); The result is shown in the following screenshot: D3 used to create and style new elements from data. Code: Binding/5-callbacks.html . Callback functions also have a third parameter that contains the array of elements in the selection. You might need it if you want to obtain the current element when using arrow functions, since the this reference in these functions doesn't refer to the current element: .text( (d, i, nodes) => console.log(\"Current element: \" + nodes[i]) ); You can easily reuse the preceding code with different types of data. If you have an array of objects containing values of different types, you don't need to convert it to an array of numbers. You can directly pass the entire object array to the data() method, and later select which properties of each object you wish to use. For example, you can use practically the same code that was shown previously to display the contents of the following array: const distances = [ {name: \"Mercury\", distance: 0.387}, {name: \"Venus\", distance: 0.723}, {name: \"Earth\", distance: 1}, {name: \"Mars\", distance: 1.52}, {name: \"Jupiter\", distance: 5.2}, {name: \"Saturn\", distance: 9.54}, {name: \"Uranus\", distance: 19.2}, {name: \"Neptune\", distance: 30.1}, {name: \"Ceres\", distance: 2.765}, {name: \"Pluto\", distance: 39.481}, {name: \"Eris\", distance: 67.67}, {name: \"Haumea\", distance: 43}, {name: \"Makemake\", distance: 45.346} ]; The object array is received by the data() command. Each array item is available as the first parameter of the callback, which can be used to extract the property that contains the data (see Binding/6-object-array.html ): d3.select(\"body\") // select the body element .append(\"ul\") // append an <ul> element inside <body> .selectAll(\"li\") // select all <li> elements (there are none) .data(distances) // load the data from the numbers array .enter() // bind the data to enter array with 5 placeholders .append(\"li\") // create <li> from enter selection and append to <ul> .text(function(d, i) { return d.name + \": \" + d.distance; }); Since D3 Version 5.8, you can replace enter().append(element) in simple binding operations, as shown with the join(element) method (see Binding/7-join.html ): d3.select(\"body\") .append(\"ul\") .selectAll(\"li\") .data(distances) .join(\"li\") // obtains enter selection and appends 5 new <li> elements .text(function(d, i) { return d.name + \": \" + d.distance; }); The preceding code will generate an HTML list with the data.","title":"Binding Data"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#creating-a-bar-chart-with-d3-and-html","text":"Now that you have had a quick introduction to D3's basic data-binding mechanisms, let's create some full data visualization examples that will demonstrate the power of D3. You should know HTML very well, so we will start with a simple bar chart using HTML and CSS. Later, we will repeat the exact same procedure using SVG, the main standard used by D3 to create visualizations. All the code for these examples is available in the GitHub repository for this chapter.","title":"Creating a bar chart with D3 and HTML"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#binding-data-to-html","text":"In this example, we will draw a horizontal bar chart using HTML <div> elements. The colors and length of each bar are controlled by CSS, which we can configure using the selection.style() command. We will use the object array containing planetary distances from the last example. First, let's create a style sheet with classes for the bar-chart and each individual bar : <style> .bar-chart { border: solid 1px gray; /* a gray border around the container */ position: relative; } .bar { height: 20px; background-color: orange; position: absolute; } </style> The bars are positioned absolutely. Since each bar has an equal height (20px), we need to position each bar slightly below the previous one. This can be achieved by setting the value for the CSS top property (in pixels) so that it contains the sum of the heights of all the previous bars, plus an extra pixel to keep them slightly apart. The top property can be dynamically calculated as proportional to each entry's array index, using a callback in selection.style() (see HTML_Bar/1-bar-chart.html ): d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") // container div for the chart .style(\"height\", () => distances.length * 21 + \"px\") // set chart height .selectAll(\"div\").data(distances) // binds data .enter().append(\"div\") // appends a div for each data element .attr(\"class\", \"bar\") // these divs are the bars of the chart .style(\"top\", (d,i) => i * 21 + \"px\") // stacks bars .style(\"width\", \"100px\") // fixed width output: This prints all the bars, one on top of the other, but they all have the same static width! We need to use a callback to change the width of each individual bar. In a bar chart, the actual width should be proportional to the distance. A solution would be to multiply each value by 10. This will make the smallest bar three pixels wide, and the largest over 670 pixels: .style(\"width\", d => (d.distance * 10) + \"px\") Now, the lengths of the bars are proportional to the data values. This code is in HTML_Bar/2-bar-width.html . The result is shown as follows: HTML bar chart with D3: Drawing DIVs and using the data to change the CSS width. Code: HTML_Bar/2-bar-width.html","title":"Binding data to HTML"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#scales","text":"What if a new planet is discovered that is 10 times the largest distance? The bars wouldn't fit and their widths would need to be recalculated! D3 also provides a solution for this: scales . Scales are mappings between different dimensions. In our case, we have to deal with two dimensions: the dimension of the data, called the domain , and the dimension of the graphics context where the data will be displayed, called the range . Before using a scale, you have to configure its domain and range. There are several different kinds of scales in D3, including linear and logarithmic scales, with many configuration options. A scale is created with a special generator function available in the d3-scale module. To use this module, you need to load several dependencies, so let's replace the <script> tag with the default bundle: <script src=\"https://d3js.org/d3.v5.min.js\"></script> To create a scale function, you call a special generator function. A linear scale function can be created using the following: const barScale = d3.scaleLinear(); To use the scale function you created, you pass a value as the argument and receive the converted result: const result = barScale(45); // returns 45 This will return 45 , because the scale hasn't been configured yet (the default scale is 1:1). To configure the scale, you need to call two methods to set up the domain (data dimensions) and range (graphical dimensions). Each method receives an array. For example, we can set up the domain as follows: barScale.domain([0, 100]); // input domain from 0 to 100 This fits all the distances in our data. The range is set up in a similar fashion: barScale.range([0, 600]); // output domain from 0 to 600 This means that zero is mapped to zero, and 100 astronomical units (AU) is mapped to 600 (pixels). Intermediate values will be interpolated. If you now call barScale(45) , you will get 270 . We can use the scale to convert the distances in AU to pixels, replacing the previous expression with a scale conversion (see HTML_Bar/3-scales.html ): .style(\"width\", d => barScale(d.distance) + \"px\");","title":"Scales"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#array-utilities","text":"We chose 100 as the upper limit in our scale domain because it's larger than any of the distances in the data, but the choice was rather arbitrary. We could have chosen the largest value in the array. If you have hundreds of lines of data, you can use JavaScript array functions (see Chapter 1, Introduction ) to find out the maximum value. D3 also includes a collection of array manipulation functions that extend JavaScript's native functions. Some look similar but may be more efficient for data manipulation (for example, by ignoring null , NaN , and undefined values). In order to use them, you need the d3-array module (which is also part of the default bundle). Here, we changed the configuration of our barScale() function so that it uses the largest distance from the distances object array as the upper value for the domain. This is achieved with by calling the d3.max() function, which receives an array and an accessor function for each array element, as follows: const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); The d3.max() function will scan the distances array and compare the distance property of each object, returning the largest one. There are many more useful functions in d3-array , which we will cover in the next chapter. Two of them, d3.descending(a,b) and d3.ascending(a,b) , are used to provide a sorting rule for JavaScript's native sort() method. We can use it to sort the array by the distance: distances.sort((a,b) => d3.ascending(a.distance, b.distance)); See and run the code after these transformations in HTML_Bar/4-max-sort.html .","title":"Array utilities"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#adding-labels","text":"Since the <div> elements in HTML can contain text, you can call the text() method for each <div> and set its contents based on the data: d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") // ...all previous code to build chart .text(d => d.distance); // display distance in the <div> With CSS, we can right-align the text and adjust the fonts. Since these styles are static and don't change with the data, instead of calling style() for each property, you should use a style sheet: <style> /* ... */ .bar { height: 20px; left: 100px; background-color: orange; position: absolute; text-align: right; padding: 0 5px; font-family: sans-serif; font-size: 9pt; } </style> As a result, the labels are placed inside the bars, as follows: Adding labels to the bars. Code: HTML_Bar/5-labels.html .","title":"Adding labels"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#more-labels-formatting-and-colors","text":"Each object in our data array also contains a name property that can be used to label each bar. To position text outside the bar, we will need to refactor the code so that each data entry contains a container <div> , which will be bound to the data values. This entry <div> will then contain three other <div> elements: a category label (the name of the planet), the bar, and a value label (the distance). Classes will be used to identify each <div> . The following style sheet contains the static properties for these elements: <style> .bar-chart { /* The container <div> for the entire chart */ border: solid 1px gray; position: relative; width: 800px; } .entry { /* a container <div> for each data entry */ position: absolute; width: 100%; } .bar { /* the colored rectangle */ height: 20px; top: 1px; left: 100px; background-color: orange; position: absolute; } .label { /* a text label */ padding: 4px 5px; font-family: sans-serif; font-size: 9pt; position: absolute; height: 20px; } .category { /* the category text label at left (name) */ text-align: right; width: 80px; } .value { /* the value text label at right (distance) */ text-align: left; } </style> Since each entry <div> has three children, we need to keep a reference to its selection so that the child elements can be appended. The following code saves references for each one of the container <div> elements. The chart constant contains a selection of the root <div> element, and the entries constant contains the selection of all entry <div> elements: // selects the entire chart (one node) const chart = d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21 + \"px\"); // selects each entry (a nodelist) const entries = chart.selectAll(\"div\").data(distances) .enter().append(\"div\") .attr(\"class\", \"entry\") .style(\"top\", (d,i) => i * 21 + \"px\"); You can now use the entries constant to append the child elements to each entry <div> (see HTML_Bar/6-entries.html ): entries.append(\"div\").attr(\"class\", \"label category\") .text(d => d.name); entries.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", d => barScale(d.distance) + \"px\"); entries.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", d => (barScale(d.distance) + 100) + \"px\") .text(d => d.distance + \" AU\"); Each append() shown is called once for each entry. The attributes, style, and text are set using the data that was bound to the parent container. You can add child elements without having to break the selection chain with the selection.each() method, which calls a function for each entry. Inside it, you can obtain a selection to the current element using d3.select(this) . The following code produces the same result (see HTML_Bar/7-entries-each.html ): entries.each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"div\").attr(\"class\", \"label category\") .text(d.name); entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\"); entry.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", (barScale(d.distance) + 100) + \"px\") .text(d.distance + \" AU\"); }); We can improve the rendering of the labels by formatting the numbers to display only two decimal places, using the d3.format() generator function (from the d3-format module). The following code creates a function fmt() that can be used to format numbers: const fmt = d3.format(\".2f\"); Now, we can use it to format the distances: .text(d => fmt(d.distance) + \" AU\"); The final result is shown as follows: Adding category names and placing the formatted label values outside the bars. Code: HTML_Bar/7-entries-each.html","title":"More labels, formatting, and colors"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#changing-colors","text":"The d3-colors module contains functions to generate, convert, and transform colors. Passing any CSS-compatible color representation to d3.color() generates an object with methods that can be used to modify the color. The darker() and brighter() methods receive a value between 0 (no change) and 1 (maximum change) to adjust the lightness component of a color and return a hexadecimal color string. Let's use this feature to darken the bars when the distance from the sun increases. We will need a scale that maps the distance domain to the [0-1] range, which contains the values accepted by the darker() function: colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) Now, this colorScale can be used to generate different tones for the bars: entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\") .style(\"background-color\", d3.color('orange') .darker(colorScale(d.distance))) See the code in HTML_Bar/8-colors.html . The result is shown as follows: Changing bar colors with the distance. Code: HTML_Bar/8-colors.html","title":"Changing colors"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#loading-external-files","text":"Usually, your data will come from external files that need to be loaded with an Ajax request. You can, of course, use jQuery or ECMAScript fetch() commands (and promises, if you need to load multiple files). After loading, you will also need to parse the data and convert it into JavaScript arrays and objects. D3 again provides a more efficient solution in the d3-fetch module: a set of convenient methods for loading and parsing files in popular formats, such as XML, JSON, or CSV. The d3-fetch module is also included in the default bundle. The planetary data we used in the previous examples is actually part of a larger JSON file containing several properties for planets, asteroids, and satellites. The basic structure of this file ( Data/sol_2016.json ) is shown as follows: { \"star\":{\u2026}, \"planets\":[ { \"id\":\"p1\", \"name\":\"Mercury\", \"diameterKm\":4879, \"semiMajorAxisAU\":0.387, \u2026 },{ \"id\":\"p2\", \"name\":\"Venus\", \"diameterKm\":12104, \"semiMajorAxisAU\":0.723, \u2026 }, \u2026 } ``` You can load the data using the ```d3.json()``` function. After it loads and parses the file, it will become available in a callback provided as a parameter to the ```then()``` method (which is a JavaScript promise). We don't need all the loaded data. The distance used in our examples is stored in the property called ```semiMajorAxisAU```. After loading the file, we can filter the dataset to only save the ```semiMajorAxisAU``` and ```name``` properties. The following code demonstrates this. It loads the file, and it then uses the data obtained in the ```then()``` callback to loop through the planets array, adding only the chosen properties to a new object and pushing it into an array. The array is used to create an HTML list with these properties (see``` Loading/1-loading-json.html```): ```js //d3.json(\"../Data/sol_2016.json\") // if you down load it d3.json(\"https://raw.githubusercontent.com/PacktPublishing/Learn-D3.js/master/Chapter03/Data/sol_2016.json\") // if you use the git hub data. This is so cool! .then(function(data) { const planets = []; data.planets.forEach(function(obj) { planets.push({ name: obj.name, distance: obj.semiMajorAxisAU }); }); draw(planets); }); function draw(distances) { d3.select(\"body\").append(\"ol\") .selectAll(\"li\") .data(distances) .join(\"li\") .text(d => d.name + \" (\" + d.distance + \" AU)\"); } To apply this to our chart, you just need to replace the contents of the draw() method shown with the entire code used in the previous examples (except for the distances array). See an example in HTML_Bar/9-load-json.html . As an exercise, try feeding the bar chart with a list of Jupiter's moons, instead of the planets (use data.planets[4].satellites to obtain the array, and the semiMajorAxisKm property as the distance, and d3.format(\",.0f\") to format the distance values). Full Example: d3.json(\"https://raw.githubusercontent.com/PacktPublishing/Learn-D3.js/master/Chapter03/Data/sol_2016.json\") // if you use the git hub data. This is so cool! .then(function(data) { const planets = []; data.planets.forEach(function(obj) { planets.push({name: obj.name, distance: obj.semiMajorAxisAU}); }); draw(planets); }); function draw(distances) { const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); const colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) // selects the entire chart (one node) const chart = d3.select(\"body\") .append(\"div\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21 + \"px\"); // selects each entry (a nodelist) const entries = chart.selectAll(\"div\").data(distances.sort((a,b) => d3.ascending(a.distance, b.distance))) .enter().append(\"div\") .attr(\"class\", \"entry\") .style(\"top\", (d,i) => i * 21 + \"px\"); entries.each(function(d) { const entry = d3.select(this); // the current entry const fmt = d3.format(\".2f\"); // format for each entry entry.append(\"div\").attr(\"class\", \"label category\") .text(d.name); entry.append(\"div\").attr(\"class\", \"bar\") .style(\"width\", barScale(d.distance) + \"px\") .style(\"background-color\", d3.color('orange') .darker(colorScale(d.distance))) entry.append(\"div\").attr(\"class\", \"label value\") .style(\"left\", (barScale(d.distance) + 100) + \"px\") .text(d => fmt(d.distance) + \" AU\"); }); }","title":"Loading external files"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#creating-a-bar-chart-with-d3-and-svg","text":"It's easy to create a horizontal bar chart in HTML. It's a bit trickier to make a vertical one, and quite challenging to create other charts, such as line charts, pie charts, and network diagrams, since HTML wasn't intended for vector graphics. While you can create such charts using HTML and CSS, it's not at all simple, nor is it recommended . You can, of course, use Canvas, but there are no graphical DOM elements to bind in Canvas. Canvas is used in D3 charts, but mostly for performance optimization. D3 works best with SVG . The same bar chart that we created with HTML can be created with SVG. Do you think you can do it? Why not try it as an exercise before proceeding? You already learned how to use D3 and HTML, and you know the basics of SVG rectangles, attributes, and styles. That's all you need! Most of the code is the same. You can reuse the scales, formatting function, sorting function, and JSON parsing code. The CSS will be simpler, since positioning is done in SVG. You only need to change the selection code, replacing the bar <div> element with <rect> , text label <div> elements with <text> , and container <div> elements with <g> . Remember to use fill (and not background-color) to fill the bars. The same step-by-step files that were used in the HTML example are available for the SVG example in the SVG_Bar/ folder . The following, final code produces exactly the same chart as the HTML version. This is the CSS style sheet that's used. Since all positioning is done in SVG, it's much smaller: <style> * { font-family: sans-serif; } .bar-chart { border: solid 1px gray; width: 800px; } .bar { height: 20px; fill: orange; } .label { font-size: 9pt; } .category { text-anchor: end; } </style> The main container is the <svg> element, which is appended to the body, as shown in the following code snippet. We renamed the chart object svg : // selects the entire chart (one node) const svg = d3.select(\"body\") .append(\"svg\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21); The containers used for each entry are <g> elements, which group several child elements and can have their own coordinate system configured using a transform . Although the transform property is supported in both CSS and SVG, you should use the SVG version (selected with the attr() method), because it considers pixel values and degrees as the default. You don't have to append deg or px to any values: // selects each entry (a nodelist) const entries = svg.selectAll(\"g\").data(distances) .enter().append(\"g\") .attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0, ${i * 21})`); The entries are appended to the container <g> in the each() method, as follows. Compare this code to the HTML version: // sort distances distances = distances.sort((a,b) => d3.ascending(a.distance, b.distance)); // selects the entire chart (one node) const svg = d3.select(\"body\") .append(\"svg\").attr(\"class\", \"bar-chart\") .style(\"height\", distances.length * 21); // barScales const barScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0, 600]); // colorScale const colorScale = d3.scaleLinear() .domain([0, d3.max(distances, d => d.distance)]) .range([0,1]) // decimal format const fmt = d3.format(\".2f\"); // selects each entry (a nodelist) const entries = svg.selectAll(\"g\").data(distances) .enter().append(\"g\") .attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0, ${i * 21})`); entries.each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"text\").attr(\"class\", \"label category\") .attr(\"y\", 15) .attr(\"x\", 90) .text(d.name); entry.append(\"rect\").attr(\"class\", \"bar\") .attr(\"x\", 100) .attr(\"width\", barScale(d.distance) + \"px\") .style(\"fill\", d3.color('orange') .darker(colorScale(d.distance))) entry.append(\"text\").attr(\"class\", \"label value\") .attr(\"y\", 15) .attr(\"x\", barScale(d.distance) + 105) .text(fmt(d.distance) + \" AU\"); }); Try the full code from SVG_Bar/9-load-json.html . If you open it in your browser, you will notice that the result is identical to the one you get with HTML_Bar/9-load-json.html . To explore more D3 features, in the next sections, we will use the SVG version of this bar chart.","title":"Creating a bar chart with D3 and SVG"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#updating-data-visualizations","text":"Once the data is bound to SVG elements, you can change the original data values and reflect the changes in the chart. Values can change immediately or transition smoothly. This section will provide an introduction on how to trigger data updates and configure smooth transitions using D3. First, we need more data. Let's change the code where we create our data object and include two more values that use the same dimensions: the aphelium (the longest distance between a planet and the sun) in the max property, and the perihelium (the shortest distance) in the min property. The distance is now stored in the avg property (see Updating/1-three-charts.html ): const planets = []; // this array will store all the data when loaded d3.json(\"../Data/sol_2016.json\") .then(function(data) { data.planets.forEach(function(obj) { planets.push({ name: obj.name, avg: obj.semiMajorAxisAU, max: obj.apheliumAU, min: obj.periheliumAU}); }); init(); }); You can create bar charts with any of these three values, or all of them. Let's create an application where the user can choose which chart to display using HTML buttons. The following is the HTML code for the page; it includes an <svg> element and some buttons: <body> <h1><span id=\"chart\">Average</span> distance from the Sun</h1> <svg class=\"bar-chart\"></svg> <form> <button type=\"button\" id=\"avg\">Average</button> <button type=\"button\" id=\"max\">Maximum</button> <button type=\"button\" id=\"min\">Minimum</button> </form> <script>...</script> </body> These buttons will be attached to event handlers. They will select the data that should be displayed in the bar charts. The following array relates a key to a title and a color. The key contains the name of a property from each element in the planets array. It's also used for the button IDs: const charts = [ {key: \"avg\", title: \"Average\", color: \"orange\"}, {key: \"max\", title: \"Maximum\", color: \"blue\"}, {key: \"min\", title: \"Minimum\", color: \"red\"}, ]; The chart object stores the dimensions of the current chart (which may have a variable height) and the chart that is currently displayed: const chart = { width: 800, height: 0, // the height is set after data is loaded current: charts[0] // chart to display first } These other global constants initialize scales, a formatting function, and a selection of the svg object: const barScale = d3.scaleLinear().range([0, 600]); const colorScale = d3.scaleLinear().range([0, 1]); const format = d3.format(\".2f\"); const svg = d3.select(\"svg.bar-chart\"); // the container SVG The init() function that is called right after the data is loaded and the planets array is populated performs basic initialization that requires the loaded data. In this case, it sets the height of the chart: function init() { // runs once chart.height = planets.length * 21; svg.attr(\"width\", chart.width) .attr(\"height\", chart.height); setupView(); // sets up scales; Not defined yet...see below // ... } The setupView() function called inside init() configures the current view. It disables the button that refers to the currently displayed view, replaces the <span> element in the title with the title of the current chart, sorts the planets array, and initializes the domains of the scales, based on the current data: function setupView() { // disable all buttons d3.selectAll(\"button\").property(\"disabled\", false); // enable only buttons that are not current chart d3.select(\"#\" + chart.current.key).property(\"disabled\", true); // update page title d3.select(\"#chart\").text(chart.current.title); // sort the planets using current data planets.sort((a,b) => d3.ascending(a[chart.current.key], b[chart.current.key])); // update scale domain with current data const maxValue = d3.max(planets, d => d[chart.current.key]); barScale.domain([0, maxValue]); colorScale.domain([0, maxValue]); } After setting up the view, the init() function uses the current data to render the chart. This code is identical to the code we used in the previous examples. The only difference is that it uses a key reference to access the data property: instead of d.avg , it uses d[chart.current.key] . This will allow the chart to reference other properties when the current key changes: function init() { // ... setupView(); svg.selectAll(\"g\") .data(planets) .enter().append(\"g\").attr(\"class\", \"entry\") .attr(\"transform\", (d,i) => `translate(0,${i * 21})`) .each(function(d) { const entry = d3.select(this); // the current entry entry.append(\"text\").attr(\"class\", \"label category\") .attr(\"y\", 15) .attr(\"x\", 90) .text(d.name); entry.append(\"rect\").attr(\"class\", \"bar\") .attr(\"x\", 100) .attr(\"height\", 20) .attr(\"width\", barScale(d[chart.current.key]) ) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key])) ) entry.append(\"text\").attr(\"class\", \"label value\") .attr(\"y\", 15) .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); } The result is shown as follows. It's the same chart we created before, with a title and some buttons that don't work yet: The previous example adapted to show three different charts. Code: Updating/1-three-charts.html.","title":"Updating Data Visualizations"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#handling-events","text":"The on() method is used to handle events, and it can be called from any selection. The first parameter is a standard JavaScript event name string (such as click or mouseover ), and the second parameter is the handler function that will execute when the event happens. The following code obtains a selection containing all button objects and attaches an event handler to all of them. It obtains the id of the button that was clicked and uses it to change the current chart by assigning a corresponding object from the charts array. After changing the current chart, it calls the draw() function, which will update the chart. This code should be placed in a global context, since it only needs to run once: d3.selectAll(\"button\") .on(\"click\", function() { chart.current = charts.filter(c => c.key == this.id)[0]; draw(); }); The draw function in this example only prints the current array. You can use it to test whether the buttons are selecting the correct chart, as expected. The call to setupView() will disable/enable the buttons and update the chart's title according to the current view (see Updating/2-events.html ): function draw() { console.log(chart.current.key); setupView(); }","title":"Handling events"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#data-updates","text":"To update the data on a selection, you just need to update the styles and attributes. If the data has changed, you should call the selection.data() method with the new data, and then update everything that depends on it, such as attributes and styles, and any functions called by them, such as scales. In our case, the data bound to the container g.entry object is the entire planets array, which may have been sorted in a different order (in the setupView() function). It can be updated simply by reassigning it to the selection: svg.selectAll(\"g.entry\").data(planets) Then, you need to update the attributes and styles, which depend on properties from this array that have changed, but before doing that, you need to update the scales' domains, since they are called from the style() and attr() methods, and their maximum value depends on the new data. The scales were updated in the setupView() function. The rest of the draw() function contains the data updates: function draw() { setupView(); // sorts data and updates scales svg.selectAll(\"g.entry\").data(planets) .each(function (d) { d3.select(this).select(\".label.category\") .text(d.name); // the order may have changed d3.select(this).select(\".bar\") .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); d3.select(this).select(\".label.value\") .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); } Now, when you click any button, a new bar chart will be displayed. The chart reuses the same graphical elements, changing its colors, dimensions, and text contents. Bars are always sorted in ascending order (see Updating/3-updates.html ): Each button replaces the current chart with a new chart, reusing the same chart elements. Code: Updating/3-updates.html .","title":"Data updates"},{"location":"notes/javascript/D3js/Rocha/chapter_3/chapter-3/#smooth-transitions","text":"Instead of immediately replacing one chart with another, you can add transitions, so that they occur in smooth animations. Adding a transition is very easy. You just need to call the transition() method on a selection before setting the new properties and styles. Instead of changing the old data with the new data, it will interpolate intermediate values during a quarter of a second. Add it to your update selections, and you will notice that when you change the chart, the bars and labels will animate to their new sizes, colors, and positions. In our examples, transitions were added before changing the color and width of each bar, and before changing the position of each value label (see Updating/4-transitions.html ): svg.selectAll(\"g.entry\").data(planets) .each(function (d) { d3.select(this).select(\".label.category\").text(d.name); d3.select(this).select(\".bar\") .transition() // 1) transition fill and width .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); d3.select(this).select(\".label.value\") .transition() // 2) transition x position .attr(\"x\", barScale(d[chart.current.key]) + 105) .text(format(d[chart.current.key]) + \" AU\"); }); If you want a slower transition, just add duration(value) after the transition() command, with a value in milliseconds. The following code will make the animation last one second (see Updating/5-durations.html ): d3.select(this).select(\".bar\") .transition() .duration(1000) // animate during a second .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); You can also configure a delay before the transition starts with the delay() method. It can receive a fixed value in milliseconds, or a callback function that will apply a different delay for each object. In the following example, each bar will wait an amount of milliseconds proportional to its array index (the second parameter in the each() function), making each bar animate in a sequence: .each(function (d,i) { // include the i (index) parameter // ... d3.select(this).select(\".bar\") .transition() .duration(1000) // animate during a second .delay(50 * i) // longer animations for each bar .attr(\"width\", barScale(d[chart.current.key])) .style(\"fill\", d3.color(chart.current.color) .darker(colorScale(d[chart.current.key]))); // ... }); The following diagram shows a snapshot of the animation captured in the middle of a transition. Try it out and see the effects for yourself (see the code in Updating/6-delays.html ): Transition with delay. Code: Updating/6-delays.html . So far, you've used the most common features of D3, rendering charts in HTML and SVG. We will finish this chapter with a D3 visualization of a map of the world, using the same methods you have used in the previous examples.","title":"Smooth transitions"},{"location":"notes/javascript/D3js/Rocha/chapter_4/chapter-4/","text":"Not started","title":"Chapter 4"},{"location":"notes/javascript/D3js/Rocha/chapter_4/chapter-4/#not-started","text":"","title":"Not started"},{"location":"notes/javascript/D3js/d3js.org/basics/","text":"Basics of D3 Source square.github.io and the version of D3 is v5 from https://d3js.org/ I'll be using this template that you can use as well. Paste html code into the <body> and the css and js will be in separate files. Example: <!DOCTYPE html> <html> <head> <title>Title</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"styles.css\"> <script src=\"https://d3js.org/d3.v5.min.js\"></script> </head> <body> <!-- Insert HTML here!!! --> <script type=\"text/javascript\" src=\"javascript.js\"></script> </body> </html> DOM API versus the D3 API A comparison of the DOM API for the regular javascript and D3: Ex 1: Basic Selection HTML <div> <p>Normal paragraph</p> <p class=\"red\">Red paragraph</p> </div> <ol> <li id=\"some-id\">Unique element</li> <li>Another list element</li> <li> <p>Paragraph inside list element</p> <p>Second paragraph</p> </li> </ol> DOM API d3.select('#some-id')['_groups'][0][0] // <li id=\"some-id\">Unique element</li> d3.select('p').size(); // select() only finds one // 1 d3.selectAll('p').size(); // selectAll() finds all // 4 var reds = d3.selectAll('.red'); // [ > Array[1] ] reds.text(); // \"Red paragraph\" D3 API d3.select('p').size(); // select() only finds one // 1 d3.selectAll('p').size(); // selectAll() finds all // 4 var reds = d3.selectAll('.red'); // [ > Array[1] ] reds.text(); // \"Red paragraph\" Ex 2: CSS HTML <h1 id=\"click-me\"> Click on me! </h1> <p class=\"hover-me\"> Hover over me! </p> <p class=\"hover-me\"> OK now hover over here! </p> <p class=\"hover-me\"> Hover here too! </p> DOM API // DOM API var clickMe = document.getElementById('click-me'); clickMe.onclick = function() { if (this.style.backgroundColor) { this.style.backgroundColor = ''; } else { this.style.backgroundColor = 'red'; } } var hoverMe = document.getElementsByClassName('hover-me'); for(var i=0; i<hoverMe.length;i++){ hoverMe[i].onmouseover = function(){ if (this.style.backgroundColor) { this.style.backgroundColor = ''; } else { this.style.backgroundColor = 'yellow'; } } } D3 DOM // D3 Selection API. Note: it attaches the // callbacks to each element in the selection d3.selectAll('.hover-me') .on('mouseover', function() { this.style.backgroundColor = 'yellow'; }) .on('mouseleave', function() { this.style.backgroundColor = ''; }); Note: In the D3 examples, the methods on the selection can chain (that is, they return themselves, so we can group them visually). Ex 3: SVG SVG (Scalable Vector Graphics) is an XML format used for drawing. You can think of SVG in a lot of the same terms as the DOM \u2013 there are elements with parents and children and attributes, and you can respond to the same mouse/touch events. Even CSS styles and selectors can apply to SVG elements. The CSS attribute names for SVG come from the SVG definition, so they are sometimes different from their HTML brethren. (For example, to change the background color of a div to red you would select it then set background-color: red but to get the same effect on an SVG rectangle you would instead use the attribute fill: red since an SVG rect doesn\u2019t respond to background-color for styling.) SVG defines tags for lots of basic shapes, like <rect> and <circle> and <line> . HTML <svg width=\"300\" height=\"180\"> <circle cx=\"30\" cy=\"50\" r=\"25\" /> <circle cx=\"90\" cy=\"50\" r=\"25\" class=\"red\" /> <circle cx=\"150\" cy=\"50\" r=\"25\" class=\"fancy\" /> <rect x=\"10\" y=\"80\" width=\"40\" height=\"40\" fill=\"steelBlue\" /> <rect x=\"70\" y=\"80\" width=\"40\" height=\"40\" style=\"fill: steelBlue\" /> <rect x=\"130\" y=\"80\" width=\"40\" height=\"40\" class=\"fancy\" /> </svg> CSS .red { fill: red; /* not background-color! */ } .fancy { fill: none; stroke: black; /* similar to border-color */ stroke-width: 3pt; /* similar to border-width */ stroke-dasharray: 3,5,10; } Ex4: <g> tag Where HTML has the and tags, SVG has the tag for an arbitrary group. You\u2019ll see a lot in D3 examples. They\u2019re great for applying styles to a group (including re-positioning the groups). The tag is good for simple labels. The tag is powerful but complex, it can be used for either lines or arbitrary filled-in shapes depending on the styling. <svg width=\"300\" height=\"180\"> <g transform=\"translate(5, 15)\"> <text x=\"0\" y=\"0\">Howdy!</text> </g> <g transform=\"translate(5, 55)\"> <!-- M: move to (jump) L: line to Q: curve to (quadratic) --> <path d=\"M0,50 L50,0 Q100,0 100,50\" fill=\"none\" stroke-width=\"3\" stroke=\"black\" /> </g> <g transform=\"translate(5, 105)\"> <!-- C: curve to (cubic) Z: close shape --> <path d=\"M0,100 C0,0 25,0 125,100 Z\" fill=\"black\" /> </g> </svg>","title":"d3js.org"},{"location":"notes/javascript/D3js/d3js.org/basics/#basics-of-d3","text":"Source square.github.io and the version of D3 is v5 from https://d3js.org/ I'll be using this template that you can use as well. Paste html code into the <body> and the css and js will be in separate files. Example: <!DOCTYPE html> <html> <head> <title>Title</title> <link rel=\"stylesheet\" type=\"text/css\" href=\"styles.css\"> <script src=\"https://d3js.org/d3.v5.min.js\"></script> </head> <body> <!-- Insert HTML here!!! --> <script type=\"text/javascript\" src=\"javascript.js\"></script> </body> </html>","title":"Basics of D3"},{"location":"notes/javascript/D3js/d3js.org/basics/#dom-api-versus-the-d3-api","text":"A comparison of the DOM API for the regular javascript and D3:","title":"DOM API versus the D3 API"},{"location":"notes/javascript/D3js/d3js.org/basics/#ex-1-basic-selection","text":"HTML <div> <p>Normal paragraph</p> <p class=\"red\">Red paragraph</p> </div> <ol> <li id=\"some-id\">Unique element</li> <li>Another list element</li> <li> <p>Paragraph inside list element</p> <p>Second paragraph</p> </li> </ol> DOM API d3.select('#some-id')['_groups'][0][0] // <li id=\"some-id\">Unique element</li> d3.select('p').size(); // select() only finds one // 1 d3.selectAll('p').size(); // selectAll() finds all // 4 var reds = d3.selectAll('.red'); // [ > Array[1] ] reds.text(); // \"Red paragraph\" D3 API d3.select('p').size(); // select() only finds one // 1 d3.selectAll('p').size(); // selectAll() finds all // 4 var reds = d3.selectAll('.red'); // [ > Array[1] ] reds.text(); // \"Red paragraph\"","title":"Ex 1: Basic Selection"},{"location":"notes/javascript/D3js/d3js.org/basics/#ex-2-css","text":"HTML <h1 id=\"click-me\"> Click on me! </h1> <p class=\"hover-me\"> Hover over me! </p> <p class=\"hover-me\"> OK now hover over here! </p> <p class=\"hover-me\"> Hover here too! </p> DOM API // DOM API var clickMe = document.getElementById('click-me'); clickMe.onclick = function() { if (this.style.backgroundColor) { this.style.backgroundColor = ''; } else { this.style.backgroundColor = 'red'; } } var hoverMe = document.getElementsByClassName('hover-me'); for(var i=0; i<hoverMe.length;i++){ hoverMe[i].onmouseover = function(){ if (this.style.backgroundColor) { this.style.backgroundColor = ''; } else { this.style.backgroundColor = 'yellow'; } } } D3 DOM // D3 Selection API. Note: it attaches the // callbacks to each element in the selection d3.selectAll('.hover-me') .on('mouseover', function() { this.style.backgroundColor = 'yellow'; }) .on('mouseleave', function() { this.style.backgroundColor = ''; }); Note: In the D3 examples, the methods on the selection can chain (that is, they return themselves, so we can group them visually).","title":"Ex 2: CSS"},{"location":"notes/javascript/D3js/d3js.org/basics/#ex-3-svg","text":"SVG (Scalable Vector Graphics) is an XML format used for drawing. You can think of SVG in a lot of the same terms as the DOM \u2013 there are elements with parents and children and attributes, and you can respond to the same mouse/touch events. Even CSS styles and selectors can apply to SVG elements. The CSS attribute names for SVG come from the SVG definition, so they are sometimes different from their HTML brethren. (For example, to change the background color of a div to red you would select it then set background-color: red but to get the same effect on an SVG rectangle you would instead use the attribute fill: red since an SVG rect doesn\u2019t respond to background-color for styling.) SVG defines tags for lots of basic shapes, like <rect> and <circle> and <line> . HTML <svg width=\"300\" height=\"180\"> <circle cx=\"30\" cy=\"50\" r=\"25\" /> <circle cx=\"90\" cy=\"50\" r=\"25\" class=\"red\" /> <circle cx=\"150\" cy=\"50\" r=\"25\" class=\"fancy\" /> <rect x=\"10\" y=\"80\" width=\"40\" height=\"40\" fill=\"steelBlue\" /> <rect x=\"70\" y=\"80\" width=\"40\" height=\"40\" style=\"fill: steelBlue\" /> <rect x=\"130\" y=\"80\" width=\"40\" height=\"40\" class=\"fancy\" /> </svg> CSS .red { fill: red; /* not background-color! */ } .fancy { fill: none; stroke: black; /* similar to border-color */ stroke-width: 3pt; /* similar to border-width */ stroke-dasharray: 3,5,10; }","title":"Ex 3: SVG"},{"location":"notes/javascript/D3js/d3js.org/basics/#ex4-g-tag","text":"Where HTML has the and tags, SVG has the tag for an arbitrary group. You\u2019ll see a lot in D3 examples. They\u2019re great for applying styles to a group (including re-positioning the groups). The tag is good for simple labels. The tag is powerful but complex, it can be used for either lines or arbitrary filled-in shapes depending on the styling. <svg width=\"300\" height=\"180\"> <g transform=\"translate(5, 15)\"> <text x=\"0\" y=\"0\">Howdy!</text> </g> <g transform=\"translate(5, 55)\"> <!-- M: move to (jump) L: line to Q: curve to (quadratic) --> <path d=\"M0,50 L50,0 Q100,0 100,50\" fill=\"none\" stroke-width=\"3\" stroke=\"black\" /> </g> <g transform=\"translate(5, 105)\"> <!-- C: curve to (cubic) Z: close shape --> <path d=\"M0,100 C0,0 25,0 125,100 Z\" fill=\"black\" /> </g> </svg>","title":"Ex4: &lt;g&gt; tag"},{"location":"notes/javascript/NodeJS/issues_solved/","text":"Issues Solved Cannot upgrade Node.js using n How to Update Node.js to Latest Version","title":"Issues Solved"},{"location":"notes/javascript/NodeJS/issues_solved/#issues-solved","text":"Cannot upgrade Node.js using n How to Update Node.js to Latest Version","title":"Issues Solved"},{"location":"notes/javascript/Vue/links_to_checkout/","text":"Links To Checkout For Later Single Page app in Flask and Vue Similar tutorial related to the single_page_app(Vue tutorial with Flask) Component Architecture Code Sandbox Google Maps Demo Global Event Bus \"this\" is undefined in Vue Component How to catch Google Map Event with listener in vue.js","title":"Links To Checkout"},{"location":"notes/javascript/Vue/links_to_checkout/#links-to-checkout-for-later","text":"Single Page app in Flask and Vue Similar tutorial related to the single_page_app(Vue tutorial with Flask) Component Architecture Code Sandbox Google Maps Demo Global Event Bus \"this\" is undefined in Vue Component How to catch Google Map Event with listener in vue.js","title":"Links To Checkout For Later"},{"location":"notes/javascript/Vue/single_page_app/basics/","text":"Vue and Flask Single Page App Source testdriven ...I should fill this in.","title":"Vue - Flask Integration"},{"location":"notes/javascript/Vue/single_page_app/basics/#vue-and-flask-single-page-app","text":"Source testdriven ...I should fill this in.","title":"Vue and Flask Single Page App"},{"location":"notes/layouts/holy_grail/","text":"The Holy Grail This is apparently a hard style to make...or it was. Now that HTML5 and CSS3 have made lots of updates, styling is easier now. Below is my implementation of this layout. Preview Let's discuss what happened. First the HTML layout. HTML <!DOCTYPE html> <html> <head> <title>New Site</title> <link rel=\"stylesheet\" href=\"./style.css\"></link> <script src=\"https://kit.fontawesome.com/36fc456441.js\" crossorigin=\"anonymous\"></script> </head> <body> <header> <i class=\"fab fa-pied-piper-alt fa-5x\"></i> <ul class=contact-info> <ul>bbearce@somewhere.org</ul> <ul>(123)-456-7890</ul> <ul>Cool St., City, ST 01234</ul> </ul> <ul class=navbar> <li>Home</li> <li>About</li> <li>Details</li> <li>Jokes</li> </ul> </header> <div class=content> <div class=left> <ul> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> </ul> </div> <div class=middle> <p>Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.</p> </div> <div class=right> <ul> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> </ul> </div> </div> <footer> <div class=icons> <i class=\"fas fa-cloud\"></i> <i class=\"fas fa-heart\"></i> <i class=\"fas fa-car\"></i> <i class=\"fas fa-file\"></i> <i class=\"fas fa-bars\"></i> </div> </footer> <script type=\"text/javascript\" src=\"./javascript.js\"></script> </body> </html> scss html { height: 94vh; } body { height: 100%; background: rgba(100,100,100, 0.1); } header { background: rgba(100, 0, 0, 0.1); height: 20%; width: 100%; .navbar { width: 300px; display: block; margin-left: auto; margin-right: auto; li { display: inline-block; width: 50px; border: 1px solid; } } .contact-info { float: right; } } .content { height: 80%; } .left { color: blue; float: left; width: 20%; height: 100%; background: rgba(0,0,100, 0.1); display: inline-block; } .middle { width: 60%; height: 100%; background: rgba(100,100,100, 0.1); display: inline-block; } .right { color: green; float:right; width: 20%; height: 100%; background: rgba(0,100,0, 0.1); display: inline-block; } footer { background: rgba(100,0,0, 0.1); .icons { display: block; width: 100px; margin-left: auto; margin-right: auto; } } CSS html { height: 94vh; } body { height: 100%; background: rgba(100, 100, 100, 0.1); } header { background: rgba(100, 0, 0, 0.1); height: 20%; width: 100%; } header .navbar { width: 300px; display: block; margin-left: auto; margin-right: auto; } header .navbar li { display: inline-block; width: 50px; border: 1px solid; } header .contact-info { float: right; } .content { height: 80%; } .left { color: blue; float: left; width: 20%; height: 100%; background: rgba(0, 0, 100, 0.1); display: inline-block; } .middle { width: 60%; height: 100%; background: rgba(100, 100, 100, 0.1); display: inline-block; } .right { color: green; float: right; width: 20%; height: 100%; background: rgba(0, 100, 0, 0.1); display: inline-block; } footer { background: rgba(100, 0, 0, 0.1); } footer .icons { display: block; width: 100px; margin-left: auto; margin-right: auto; } /*# sourceMappingURL=style.css.map */ JS // none requred for this example","title":"Holy Grail"},{"location":"notes/layouts/holy_grail/#the-holy-grail","text":"This is apparently a hard style to make...or it was. Now that HTML5 and CSS3 have made lots of updates, styling is easier now. Below is my implementation of this layout. Preview Let's discuss what happened. First the HTML layout.","title":"The Holy Grail"},{"location":"notes/layouts/holy_grail/#html","text":"<!DOCTYPE html> <html> <head> <title>New Site</title> <link rel=\"stylesheet\" href=\"./style.css\"></link> <script src=\"https://kit.fontawesome.com/36fc456441.js\" crossorigin=\"anonymous\"></script> </head> <body> <header> <i class=\"fab fa-pied-piper-alt fa-5x\"></i> <ul class=contact-info> <ul>bbearce@somewhere.org</ul> <ul>(123)-456-7890</ul> <ul>Cool St., City, ST 01234</ul> </ul> <ul class=navbar> <li>Home</li> <li>About</li> <li>Details</li> <li>Jokes</li> </ul> </header> <div class=content> <div class=left> <ul> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> </ul> </div> <div class=middle> <p>Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.</p> </div> <div class=right> <ul> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> <li>stuff</li> </ul> </div> </div> <footer> <div class=icons> <i class=\"fas fa-cloud\"></i> <i class=\"fas fa-heart\"></i> <i class=\"fas fa-car\"></i> <i class=\"fas fa-file\"></i> <i class=\"fas fa-bars\"></i> </div> </footer> <script type=\"text/javascript\" src=\"./javascript.js\"></script> </body> </html>","title":"HTML"},{"location":"notes/layouts/holy_grail/#scss","text":"html { height: 94vh; } body { height: 100%; background: rgba(100,100,100, 0.1); } header { background: rgba(100, 0, 0, 0.1); height: 20%; width: 100%; .navbar { width: 300px; display: block; margin-left: auto; margin-right: auto; li { display: inline-block; width: 50px; border: 1px solid; } } .contact-info { float: right; } } .content { height: 80%; } .left { color: blue; float: left; width: 20%; height: 100%; background: rgba(0,0,100, 0.1); display: inline-block; } .middle { width: 60%; height: 100%; background: rgba(100,100,100, 0.1); display: inline-block; } .right { color: green; float:right; width: 20%; height: 100%; background: rgba(0,100,0, 0.1); display: inline-block; } footer { background: rgba(100,0,0, 0.1); .icons { display: block; width: 100px; margin-left: auto; margin-right: auto; } }","title":"scss"},{"location":"notes/layouts/holy_grail/#css","text":"html { height: 94vh; } body { height: 100%; background: rgba(100, 100, 100, 0.1); } header { background: rgba(100, 0, 0, 0.1); height: 20%; width: 100%; } header .navbar { width: 300px; display: block; margin-left: auto; margin-right: auto; } header .navbar li { display: inline-block; width: 50px; border: 1px solid; } header .contact-info { float: right; } .content { height: 80%; } .left { color: blue; float: left; width: 20%; height: 100%; background: rgba(0, 0, 100, 0.1); display: inline-block; } .middle { width: 60%; height: 100%; background: rgba(100, 100, 100, 0.1); display: inline-block; } .right { color: green; float: right; width: 20%; height: 100%; background: rgba(0, 100, 0, 0.1); display: inline-block; } footer { background: rgba(100, 0, 0, 0.1); } footer .icons { display: block; width: 100px; margin-left: auto; margin-right: auto; } /*# sourceMappingURL=style.css.map */","title":"CSS"},{"location":"notes/layouts/holy_grail/#js","text":"// none requred for this example","title":"JS"},{"location":"notes/nginx/nginx/","text":"Nginx","title":"Basics"},{"location":"notes/nginx/nginx/#nginx","text":"","title":"Nginx"},{"location":"notes/python/Installs/","text":"Installs Installing python3.6 Source1 realpython Source2 stackoverflow Ubuntu 17.10, Ubuntu 18.04 (and above) come with Python 3.6 by default. You should be able to invoke it with the command python3 . Ubuntu 16.10 and 17.04 do not come with Python 3.6 by default, but it is in the Universe repository. You should be able to install it with the following commands: $ sudo apt-get update $ sudo apt-get install python3.6 You can then invoke it with the command python3.6 . If you are using Ubuntu 14.04 or 16.04, Python 3.6 is not in the Universe repository, and you need to get it from a Personal Package Archive (PPA). For example, to install Python from the \u201cdeadsnakes\u201d PPA, do the following: $ sudo add-apt-repository ppa:deadsnakes/ppa $ sudo apt-get update $ sudo apt-get install python3.6 As above, invoke with the command python3.6 . Installing python3.7+ Source linuxize Install with Apt Installing Python 3.7 on Ubuntu with apt is a relatively straightforward process and will only take a few minutes: Start by updating the packages list and installing the prerequisites: sudo apt update sudo apt install software-properties-common Next, add the deadsnakes PPA to your sources list: sudo add-apt-repository ppa:deadsnakes/ppa When prompted press Enter to continue: Press [ENTER] to continue or Ctrl-c to cancel adding it. Once the repository is enabled, install Python 3.7 with: sudo apt install python3.7 At this point, Python 3.7 is installed on your Ubuntu system and ready to be used. You can verify it by typing: python3.7 --version Python 3.7.3 For virtual environments you need to install venv separately: qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ sudo apt install python3.7-venv Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: grub-pc-bin linux-headers-4.15.0-91 Use 'sudo apt autoremove' to remove them. The following NEW packages will be installed: python3.7-venv 0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded. Need to get 1801 kB of archives. After this operation, 2019 kB of additional disk space will be used. Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.7-venv amd64 3.7.7-1+bionic1 [1801 kB] Fetched 1801 kB in 1s (2022 kB/s) Selecting previously unselected package python3.7-venv. (Reading database ... 75546 files and directories currently installed.) Preparing to unpack .../python3.7-venv_3.7.7-1+bionic1_amd64.deb ... Unpacking python3.7-venv (3.7.7-1+bionic1) ... Setting up python3.7-venv (3.7.7-1+bionic1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ python3.7 -m venv venv qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ ls app.py database.json requirements.txt venv Install from source Source linuxize This first step may or may not be necessary but is probably a good idea. First, update the packages list and install the packages necessary to build Python source: sudo apt update sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev wget You an use wget to grab the latest like so: cd /usr/src sudo wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz You can also go to https://www.python.org/ and download it. This gave me a Python-3.7.4.tar.xz file. To unzip it use: $ tar -xf Python-3.7.4.tar.xz $ ls -la drwxr-xr-x 18 bbearce bbearce 4096 Jul 8 14:31 Python-3.7.4 -rw-rw-r-- 1 bbearce bbearce 17131432 Sep 4 11:09 Python-3.7.4.tar.xz To see other unzipping techniques for different file types try here Change into that new directory and use make to install Python: $ cd Python-3.7.4 Note: Notice the README.rst $ vi README.rst ... Build Instructions On Unix, Linux, BSD, macOS, and Cygwin:: ./configure make make test sudo make install This will install Python as python3 . ... Installing multiple versions On Unix and Mac systems if you intend to install multiple versions of Python using the same installation prefix ( --prefix argument to the configure script) you must take care that your primary python executable is not overwritten by the installation of a different version. All files and directories installed using make altinstall contain the major and minor version and can thus live side-by-side. make install also creates ${prefix}/bin/python3 which refers to ${prefix}/bin/pythonX.Y . If you intend to install multiple versions using the same prefix you must decide which version (if any) is your \"primary\" version. Install that version using make install . Install all other versions using make altinstall . To continue run make : Note that $ ./configure $ make $ make test $ sudo make altinstall Uninstall Source howtoinstall Uninstall python To remove just python package itself from Ubuntu 16.04 (Xenial Xerus) execute on terminal: sudo apt-get remove python Uninstall python and it's dependent packages To remove the python package and any other dependant package which are no longer needed from Ubuntu Xenial. sudo apt-get remove --auto-remove python Purging python If you also want to delete configuration and/or data files of python from Ubuntu Xenial then this will work: sudo apt-get purge python To delete configuration and/or data files of python and it's dependencies from Ubuntu Xenial then execute: sudo apt-get purge --auto-remove python","title":"Installs"},{"location":"notes/python/Installs/#installs","text":"","title":"Installs"},{"location":"notes/python/Installs/#installing-python36","text":"Source1 realpython Source2 stackoverflow Ubuntu 17.10, Ubuntu 18.04 (and above) come with Python 3.6 by default. You should be able to invoke it with the command python3 . Ubuntu 16.10 and 17.04 do not come with Python 3.6 by default, but it is in the Universe repository. You should be able to install it with the following commands: $ sudo apt-get update $ sudo apt-get install python3.6 You can then invoke it with the command python3.6 . If you are using Ubuntu 14.04 or 16.04, Python 3.6 is not in the Universe repository, and you need to get it from a Personal Package Archive (PPA). For example, to install Python from the \u201cdeadsnakes\u201d PPA, do the following: $ sudo add-apt-repository ppa:deadsnakes/ppa $ sudo apt-get update $ sudo apt-get install python3.6 As above, invoke with the command python3.6 .","title":"Installing python3.6"},{"location":"notes/python/Installs/#installing-python37","text":"Source linuxize","title":"Installing python3.7+"},{"location":"notes/python/Installs/#install-with-apt","text":"Installing Python 3.7 on Ubuntu with apt is a relatively straightforward process and will only take a few minutes: Start by updating the packages list and installing the prerequisites: sudo apt update sudo apt install software-properties-common Next, add the deadsnakes PPA to your sources list: sudo add-apt-repository ppa:deadsnakes/ppa When prompted press Enter to continue: Press [ENTER] to continue or Ctrl-c to cancel adding it. Once the repository is enabled, install Python 3.7 with: sudo apt install python3.7 At this point, Python 3.7 is installed on your Ubuntu system and ready to be used. You can verify it by typing: python3.7 --version Python 3.7.3 For virtual environments you need to install venv separately: qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ sudo apt install python3.7-venv Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: grub-pc-bin linux-headers-4.15.0-91 Use 'sudo apt autoremove' to remove them. The following NEW packages will be installed: python3.7-venv 0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded. Need to get 1801 kB of archives. After this operation, 2019 kB of additional disk space will be used. Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.7-venv amd64 3.7.7-1+bionic1 [1801 kB] Fetched 1801 kB in 1s (2022 kB/s) Selecting previously unselected package python3.7-venv. (Reading database ... 75546 files and directories currently installed.) Preparing to unpack .../python3.7-venv_3.7.7-1+bionic1_amd64.deb ... Unpacking python3.7-venv (3.7.7-1+bionic1) ... Setting up python3.7-venv (3.7.7-1+bionic1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ python3.7 -m venv venv qtim@MGH-Child-Care-Covid:~/Child-Care-App/server$ ls app.py database.json requirements.txt venv","title":"Install with Apt"},{"location":"notes/python/Installs/#install-from-source","text":"Source linuxize This first step may or may not be necessary but is probably a good idea. First, update the packages list and install the packages necessary to build Python source: sudo apt update sudo apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev wget You an use wget to grab the latest like so: cd /usr/src sudo wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz You can also go to https://www.python.org/ and download it. This gave me a Python-3.7.4.tar.xz file. To unzip it use: $ tar -xf Python-3.7.4.tar.xz $ ls -la drwxr-xr-x 18 bbearce bbearce 4096 Jul 8 14:31 Python-3.7.4 -rw-rw-r-- 1 bbearce bbearce 17131432 Sep 4 11:09 Python-3.7.4.tar.xz To see other unzipping techniques for different file types try here Change into that new directory and use make to install Python: $ cd Python-3.7.4 Note: Notice the README.rst $ vi README.rst ...","title":"Install from source"},{"location":"notes/python/Installs/#build-instructions","text":"On Unix, Linux, BSD, macOS, and Cygwin:: ./configure make make test sudo make install This will install Python as python3 . ...","title":"Build Instructions"},{"location":"notes/python/Installs/#installing-multiple-versions","text":"On Unix and Mac systems if you intend to install multiple versions of Python using the same installation prefix ( --prefix argument to the configure script) you must take care that your primary python executable is not overwritten by the installation of a different version. All files and directories installed using make altinstall contain the major and minor version and can thus live side-by-side. make install also creates ${prefix}/bin/python3 which refers to ${prefix}/bin/pythonX.Y . If you intend to install multiple versions using the same prefix you must decide which version (if any) is your \"primary\" version. Install that version using make install . Install all other versions using make altinstall . To continue run make : Note that $ ./configure $ make $ make test $ sudo make altinstall","title":"Installing multiple versions"},{"location":"notes/python/Installs/#uninstall","text":"Source howtoinstall","title":"Uninstall"},{"location":"notes/python/Installs/#uninstall-python","text":"To remove just python package itself from Ubuntu 16.04 (Xenial Xerus) execute on terminal: sudo apt-get remove python","title":"Uninstall python"},{"location":"notes/python/Installs/#uninstall-python-and-its-dependent-packages","text":"To remove the python package and any other dependant package which are no longer needed from Ubuntu Xenial. sudo apt-get remove --auto-remove python","title":"Uninstall python and it's dependent packages"},{"location":"notes/python/Installs/#purging-python","text":"If you also want to delete configuration and/or data files of python from Ubuntu Xenial then this will work: sudo apt-get purge python To delete configuration and/or data files of python and it's dependencies from Ubuntu Xenial then execute: sudo apt-get purge --auto-remove python","title":"Purging python"},{"location":"notes/python/s3/","text":"Working with s3 Courtesy of these resources: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html https://aws.amazon.com/cli/ https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html https://devcenter.heroku.com/articles/s3-upload-python import boto3 # Let's use Amazon S3 s3 = boto3.resource('s3') for bucket in s3.buckets.all(): print(bucket.name) # Upload a new file data = open('favicon-32x32.png', 'rb') s3.Bucket('thegratefulbrauer/recipe_description_images').put_object(Key='favicon-32x32.png', Body=data)","title":"S3"},{"location":"notes/python/s3/#working-with-s3","text":"Courtesy of these resources: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html https://aws.amazon.com/cli/ https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html https://devcenter.heroku.com/articles/s3-upload-python import boto3 # Let's use Amazon S3 s3 = boto3.resource('s3') for bucket in s3.buckets.all(): print(bucket.name) # Upload a new file data = open('favicon-32x32.png', 'rb') s3.Bucket('thegratefulbrauer/recipe_description_images').put_object(Key='favicon-32x32.png', Body=data)","title":"Working with s3"},{"location":"notes/python/advanced/Classes/","text":"Classes Basic Class Definition Intro: - Class: Blueprint - Object - Instance class Shark: # Basic Method definition def swim(self): print(\"The shark is swimming.\") def be_awesome(self): print(\"The shark is being awesome.\") Notice the use of self to reference an instance specifically...the one calling the method. Implementing sammy = Shark() sammy.swim() # >>> The shark is swimming. sammy.be_awesome() # >>> The shark is being awesome. Now let's dicuss init . class Shark: def __init__(self): print(\"This is the constructor method.\") >>> Shark() This is the constructor method. <__main__.Shark object at 0x10348d470> Finally, we can set the name of the Shark object sammy as equal to \"Sammy\" by passing it as a parameter of the Shark class: class Shark: def __init__(self, name): self.name = name def swim(self): print(self.name + \" is swimming.\") def be_awesome(self): print(self.name + \" is being awesome.\") def main(): # Set name of Shark object sammy = Shark(\"Sammy\") sammy.swim() sammy.be_awesome() if __name__ == \"__main__\": main() run... $ python shark.py Sammy is swimming. Sammy is being awesome. Inheritance super() and inheritance. In this tutorial, you\u2019ll learn about the following: The concept of inheritance in Python Multiple inheritance in Python How the super() function works How the super() function in single inheritance works How the super() function in multiple inheritance works Let's start with a simple example: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square: def __init__(self, length): self.length = length def area(self): return self.length * self.length def perimeter(self): return 4 * self.length >>> square = Square(4) >>> square.area() 16 >>> rectangle = Rectangle(2,4) >>> rectangle.area() 8 Here no references to inheritance are being made. super() in Single Inheritance super() gives you access to methods in a superclass from the subclass that inherits from it. super() alone returns a temporary object of the superclass that then allows you to call that superclass\u2019s methods. By using inheritance, you can reduce the amount of code you write while simultaneously reflecting the real-world relationship between rectangles and squares: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width # Here we declare that the Square class inherits from the Rectangle class class Square(Rectangle): def __init__(self, length): super().__init__(length, length) >>> square = Square(4) >>> square.area() 16 What Can super() Do for You? Like in other object-oriented languages, it allows you to call methods of the superclass in your subclass. The primary use case of this is to extend the functionality of the inherited method. In the example below, you will create a class Cube that inherits from Square and extends the functionality of .area() (inherited from the Rectangle class through Square) to calculate the surface area and volume of a Cube instance: class Square(Rectangle): def __init__(self, length): super().__init__(length, length) class Cube(Square): def surface_area(self): face_area = super().area() return face_area * 6 def volume(self): face_area = super().area() return face_area * self.length >>> cube = Cube(3) >>> cube.surface_area() 54 >>> cube.volume() 27 Here you have implemented two methods for the Cube class: .surface_area() and .volume(). Both of these calculations rely on calculating the area of a single face, so rather than reimplementing the area calculation, you use super() to extend the area calculation. Also notice that the Cube class definition does not have an . init (). Because Cube inherits from Square and . init () doesn\u2019t really do anything differently for Cube than it already does for Square, you can skip defining it, and the . init () of the superclass (Square) will be called automatically. A super() Deep Dive While the examples above (and below) call super() without any parameters, super() can also take two parameters: the first is the subclass, and the second parameter is an object that is an instance of that subclass. First, let\u2019s see two examples showing what manipulating the first variable can do, using the classes already shown: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square(Rectangle): def __init__(self, length): super(Square, self).__init__(length, length) In Python 3, the super(Square, self) call is equivalent to the parameterless super() call. The first parameter refers to the subclass Square, while the second parameter refers to a Square object which, in this case, is self. You can call super() with other classes as well: class Cube(Square): def surface_area(self): face_area = super(Square, self).area() return face_area * 6 def volume(self): face_area = super(Square, self).area() return face_area * self.length In this example, you are setting Square as the subclass argument to super(), instead of Cube. This causes super() to start searching for a matching method (in this case, .area()) at one level above Square in the instance hierarchy, in this case Rectangle. In this specific example, the behavior doesn\u2019t change. But imagine that Square also implemented an .area() function that you wanted to make sure Cube did not use. Calling super() in this way allows you to do that. What about the second parameter? Remember, this is an object that is an instance of the class used as the first parameter. For an example, isinstance(Cube, Square) must return True. By including an instantiated object, super() returns a bound method: a method that is bound to the object, which gives the method the object\u2019s context such as any instance attributes. If this parameter is not included, the method returned is just a function, unassociated with an object\u2019s context. For more information about bound methods, unbound methods, and functions, read the Python documentation on its descriptor system . Multiple Inheritance and Composition super() in Multiple Inheritance Now that you\u2019ve worked through an overview and some examples of super() and single inheritance, you will be introduced to an overview and some examples that will demonstrate how multiple inheritance works and how super() enables that functionality. Multiple Inheritance Overview There is another use case in which super() really shines, and this one isn\u2019t as common as the single inheritance scenario. In addition to single inheritance, Python supports multiple inheritance, in which a subclass can inherit from multiple superclasses that don\u2019t necessarily inherit from each other (also known as sibling classes). Superclass 1 Superclass 2 | | | | | | | | | | ------> Subclass <------- Let's get reacquainted with our base code: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square(Rectangle): def __init__(self, length): super(Square, self).__init__(length, length) Now let's proceed... To better illustrate multiple inheritance in action, here is some code for you to try out, showing how you can build a right pyramid (a pyramid with a square base) out of a Triangle and a Square: class Triangle: def __init__(self, base, height): self.base = base self.height = height def area(self): return 0.5 * self.base * self.height class RightPyramid(Triangle, Square): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area This example declares a Triangle class and a RightPyramid class that inherits from both Square and Triangle. You\u2019ll see another .area() method that uses super() just like in single inheritance, with the aim of it reaching the .perimeter() and .area() methods defined all the way up in the Rectangle class. The problem, though, is that both superclasses (Triangle and Square) define a .area(). Take a second and think about what might happen when you call .area() on RightPyramid, and then try calling it like below: >> pyramid = RightPyramid(2, 4) >> pyramid.area() Traceback (most recent call last): File \"shapes.py\", line 63, in print(pyramid.area()) File \"shapes.py\", line 47, in area base_area = super().area() File \"shapes.py\", line 38, in area return 0.5 * self.base * self.height AttributeError: 'RightPyramid' object has no attribute 'height' Did you guess that Python will try to call Triangle.area()? This is because of something called the method resolution order. Method Resolution Order The method resolution order (or MRO) tells Python how to search for inherited methods. This comes in handy when you\u2019re using super() because the MRO tells you exactly where Python will look for a method you\u2019re calling with super() and in what order. Every class has an . mro attribute that allows us to inspect the order, so let\u2019s do that: >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Triangle'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class 'object'>) This tells us that methods will be searched first in Rightpyramid, then in Triangle, then in Square, then Rectangle, and then, if nothing is found, in object, from which all classes originate. The problem here is that the interpreter is searching for .area() in Triangle before Square and Rectangle, and upon finding .area() in Triangle, Python calls it instead of the one you want. Because Triangle.area() expects there to be a .height and a .base attribute, Python throws an AttributeError. The problem here is that the interpreter is searching for .area() in Triangle before Square and Rectangle, and upon finding .area() in Triangle, Python calls it instead of the one you want. Because Triangle.area() expects there to be a .height and a .base attribute, Python throws an AttributeError. Luckily, you have some control over how the MRO is constructed. Just by changing the signature of the RightPyramid class, you can search in the order you want, and the methods will resolve correctly: class RightPyramid(Square, Triangle): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height super().__init__(self.base) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area Notice that RightPyramid initializes partially with the . init () from the Square class. This allows .area() to use the .length on the object, as is designed. Now, you can build a pyramid, inspect the MRO, and calculate the surface area: >>> pyramid = RightPyramid(2, 4) >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class '__main__.Triangle'>, <class 'object'>) >>> pyramid.area() 20.0 You see that the MRO is now what you\u2019d expect, and you can inspect the area of the pyramid as well, thanks to .area() and .perimeter(). There\u2019s still a problem here, though. For the sake of simplicity, I did a few things wrong in this example: the first, and arguably most importantly, was that I had two separate classes with the same method name and signature. This causes issues with method resolution, because the first instance of .area() that is encountered in the MRO list will be called. When you\u2019re using super() with multiple inheritance, it\u2019s imperative to design your classes to cooperate. Part of this is ensuring that your methods are unique so that they get resolved in the MRO, by making sure method signatures are unique\u2014whether by using method names or method parameters. In this case, to avoid a complete overhaul of your code, you can rename the Triangle class\u2019s .area() method to .tri_area(). This way, the area methods can continue using class properties rather than taking external parameters: class Triangle: def __init__(self, base, height): self.base = base self.height = height super().__init__() def tri_area(self): return 0.5 * self.base * self.height Let\u2019s also go ahead and use this in the RightPyramid class: class RightPyramid(Square, Triangle): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height super().__init__(self.base) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area def area_2(self): base_area = super().area() triangle_area = super().tri_area() return triangle_area * 4 + base_area The next issue here is that the code doesn\u2019t have a delegated Triangle object like it does for a Square object, so calling .area_2() will give us an AttributeError since .base and .height don\u2019t have any values. You need to do two things to fix this: All methods that are called with super() need to have a call to their superclass\u2019s version of that method. This means that you will need to add super(). init () to the . init () methods of Triangle and Rectangle. Redesign all the . init () calls to take a keyword dictionary. See the complete code below. class Rectangle: def __init__(self, length, width, **kwargs): self.length = length self.width = width super().__init__(**kwargs) def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width # Here we declare that the Square class inherits from # the Rectangle class class Square(Rectangle): def __init__(self, length, **kwargs): super().__init__(length=length, width=length, **kwargs) class Cube(Square): def surface_area(self): face_area = super().area() return face_area * 6 def volume(self): face_area = super().area() return face_area * self.length class Triangle: def __init__(self, base, height, **kwargs): self.base = base self.height = height super().__init__(**kwargs) def tri_area(self): return 0.5 * self.base * self.height class RightPyramid(Square, Triangle): def __init__(self, base, slant_height, **kwargs): self.base = base self.slant_height = slant_height kwargs[\"height\"] = slant_height kwargs[\"length\"] = base super().__init__(base=base, **kwargs) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area def area_2(self): base_area = super().area() triangle_area = super().tri_area() return triangle_area * 4 + base_area There are a number of important differences in this code: kwargs is modified in some places (such as RightPyramid. init ()): This will allow users of these objects to instantiate them only with the arguments that make sense for that particular object. Setting up named arguments before kwargs: You can see this in RightPyramid. init (). This has the neat effect of popping that key right out of the kwargs dictionary, so that by the time that it ends up at the end of the MRO in the object class, **kwargs is empty. Now, when you use these updated classes, you have this: >>> pyramid = RightPyramid(base=2, slant_height=4) >>> pyramid.area() 20.0 >>> pyramid.area_2() 20.0 It works! You\u2019ve used super() to successfully navigate a complicated class hierarchy while using both inheritance and composition to create new classes with minimal reimplementation. Quiz!!! When Rectangle calls super() what are the values of kwargs and what class gets it's init method called Answer: kwargs is still containing {'base': 2, 'height': 4} values as they haven't been removed from kwargs super().__init__(**kwargs) calls Triangle's init , but Rectangle doesn't inherit from Triangle so why? Remember the MRO? Let's look at RightPyramid's MRO >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class '__main__.Triangle'>, <class 'object'>) Even though Rectangle doesn't inherit from Triangle, Triangle is next in the list of classes to search for an init method. You'll notice object is at the end of the list. super().__init__(**kwargs) Multiple Inheritance Alternatives As you can see, multiple inheritance can be useful but also lead to very complicated situations and code that is hard to read. It\u2019s also rare to have objects that neatly inherit everything from more than multiple other objects. If you see yourself beginning to use multiple inheritance and a complicated class hierarchy, it\u2019s worth asking yourself if you can achieve code that is cleaner and easier to understand by using composition instead of inheritance. With composition, you can add very specific functionality to your classes from a specialized, simple class called a mixin. Since this article is focused on inheritance, I won\u2019t go into too much detail on composition and how to wield it in Python, but here\u2019s a short example using VolumeMixin to give specific functionality to our 3D objects\u2014in this case, a volume calculation: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width class Square(Rectangle): def __init__(self, length): super().__init__(length, length) class VolumeMixin: def volume(self): return self.area() * self.height class Cube(VolumeMixin, Square): def __init__(self, length): super().__init__(length) self.height = length def face_area(self): return super().area() def surface_area(self): return super().area() * 6 In this example, the code was reworked to include a mixin called VolumeMixin. The mixin is then used by Cube and gives Cube the ability to calculate its volume, which is shown below: >>> cube = Cube(2) >>> cube.surface_area() 24 >>> cube.volume() 8 A super() Recap In this tutorial, you learned how to supercharge your classes with super(). Your journey started with a review of single inheritance and then showed how to call superclass methods easily with super(). You then learned how multiple inheritance works in Python, and techniques to combine super() with multiple inheritance. You also learned about how Python resolves method calls using the method resolution order (MRO), as well as how to inspect and modify the MRO to ensure appropriate methods are called at appropriate times. For more information about object-oriented programming in Python and using super(), check out these resources: Official super() documentation Python\u2019s super() Considered Super by Raymond Hettinger Object-Oriented Programming in Python 3","title":"Classes"},{"location":"notes/python/advanced/Classes/#classes","text":"","title":"Classes"},{"location":"notes/python/advanced/Classes/#basic-class-definition","text":"Intro: - Class: Blueprint - Object - Instance class Shark: # Basic Method definition def swim(self): print(\"The shark is swimming.\") def be_awesome(self): print(\"The shark is being awesome.\") Notice the use of self to reference an instance specifically...the one calling the method. Implementing sammy = Shark() sammy.swim() # >>> The shark is swimming. sammy.be_awesome() # >>> The shark is being awesome. Now let's dicuss init . class Shark: def __init__(self): print(\"This is the constructor method.\") >>> Shark() This is the constructor method. <__main__.Shark object at 0x10348d470> Finally, we can set the name of the Shark object sammy as equal to \"Sammy\" by passing it as a parameter of the Shark class: class Shark: def __init__(self, name): self.name = name def swim(self): print(self.name + \" is swimming.\") def be_awesome(self): print(self.name + \" is being awesome.\") def main(): # Set name of Shark object sammy = Shark(\"Sammy\") sammy.swim() sammy.be_awesome() if __name__ == \"__main__\": main() run... $ python shark.py Sammy is swimming. Sammy is being awesome.","title":"Basic Class Definition"},{"location":"notes/python/advanced/Classes/#inheritance","text":"super() and inheritance. In this tutorial, you\u2019ll learn about the following: The concept of inheritance in Python Multiple inheritance in Python How the super() function works How the super() function in single inheritance works How the super() function in multiple inheritance works Let's start with a simple example: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square: def __init__(self, length): self.length = length def area(self): return self.length * self.length def perimeter(self): return 4 * self.length >>> square = Square(4) >>> square.area() 16 >>> rectangle = Rectangle(2,4) >>> rectangle.area() 8 Here no references to inheritance are being made. super() in Single Inheritance super() gives you access to methods in a superclass from the subclass that inherits from it. super() alone returns a temporary object of the superclass that then allows you to call that superclass\u2019s methods. By using inheritance, you can reduce the amount of code you write while simultaneously reflecting the real-world relationship between rectangles and squares: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width # Here we declare that the Square class inherits from the Rectangle class class Square(Rectangle): def __init__(self, length): super().__init__(length, length) >>> square = Square(4) >>> square.area() 16 What Can super() Do for You? Like in other object-oriented languages, it allows you to call methods of the superclass in your subclass. The primary use case of this is to extend the functionality of the inherited method. In the example below, you will create a class Cube that inherits from Square and extends the functionality of .area() (inherited from the Rectangle class through Square) to calculate the surface area and volume of a Cube instance: class Square(Rectangle): def __init__(self, length): super().__init__(length, length) class Cube(Square): def surface_area(self): face_area = super().area() return face_area * 6 def volume(self): face_area = super().area() return face_area * self.length >>> cube = Cube(3) >>> cube.surface_area() 54 >>> cube.volume() 27 Here you have implemented two methods for the Cube class: .surface_area() and .volume(). Both of these calculations rely on calculating the area of a single face, so rather than reimplementing the area calculation, you use super() to extend the area calculation. Also notice that the Cube class definition does not have an . init (). Because Cube inherits from Square and . init () doesn\u2019t really do anything differently for Cube than it already does for Square, you can skip defining it, and the . init () of the superclass (Square) will be called automatically. A super() Deep Dive While the examples above (and below) call super() without any parameters, super() can also take two parameters: the first is the subclass, and the second parameter is an object that is an instance of that subclass. First, let\u2019s see two examples showing what manipulating the first variable can do, using the classes already shown: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square(Rectangle): def __init__(self, length): super(Square, self).__init__(length, length) In Python 3, the super(Square, self) call is equivalent to the parameterless super() call. The first parameter refers to the subclass Square, while the second parameter refers to a Square object which, in this case, is self. You can call super() with other classes as well: class Cube(Square): def surface_area(self): face_area = super(Square, self).area() return face_area * 6 def volume(self): face_area = super(Square, self).area() return face_area * self.length In this example, you are setting Square as the subclass argument to super(), instead of Cube. This causes super() to start searching for a matching method (in this case, .area()) at one level above Square in the instance hierarchy, in this case Rectangle. In this specific example, the behavior doesn\u2019t change. But imagine that Square also implemented an .area() function that you wanted to make sure Cube did not use. Calling super() in this way allows you to do that. What about the second parameter? Remember, this is an object that is an instance of the class used as the first parameter. For an example, isinstance(Cube, Square) must return True. By including an instantiated object, super() returns a bound method: a method that is bound to the object, which gives the method the object\u2019s context such as any instance attributes. If this parameter is not included, the method returned is just a function, unassociated with an object\u2019s context. For more information about bound methods, unbound methods, and functions, read the Python documentation on its descriptor system .","title":"Inheritance"},{"location":"notes/python/advanced/Classes/#multiple-inheritance-and-composition","text":"super() in Multiple Inheritance Now that you\u2019ve worked through an overview and some examples of super() and single inheritance, you will be introduced to an overview and some examples that will demonstrate how multiple inheritance works and how super() enables that functionality. Multiple Inheritance Overview There is another use case in which super() really shines, and this one isn\u2019t as common as the single inheritance scenario. In addition to single inheritance, Python supports multiple inheritance, in which a subclass can inherit from multiple superclasses that don\u2019t necessarily inherit from each other (also known as sibling classes). Superclass 1 Superclass 2 | | | | | | | | | | ------> Subclass <------- Let's get reacquainted with our base code: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width class Square(Rectangle): def __init__(self, length): super(Square, self).__init__(length, length) Now let's proceed... To better illustrate multiple inheritance in action, here is some code for you to try out, showing how you can build a right pyramid (a pyramid with a square base) out of a Triangle and a Square: class Triangle: def __init__(self, base, height): self.base = base self.height = height def area(self): return 0.5 * self.base * self.height class RightPyramid(Triangle, Square): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area This example declares a Triangle class and a RightPyramid class that inherits from both Square and Triangle. You\u2019ll see another .area() method that uses super() just like in single inheritance, with the aim of it reaching the .perimeter() and .area() methods defined all the way up in the Rectangle class. The problem, though, is that both superclasses (Triangle and Square) define a .area(). Take a second and think about what might happen when you call .area() on RightPyramid, and then try calling it like below: >> pyramid = RightPyramid(2, 4) >> pyramid.area() Traceback (most recent call last): File \"shapes.py\", line 63, in print(pyramid.area()) File \"shapes.py\", line 47, in area base_area = super().area() File \"shapes.py\", line 38, in area return 0.5 * self.base * self.height AttributeError: 'RightPyramid' object has no attribute 'height' Did you guess that Python will try to call Triangle.area()? This is because of something called the method resolution order. Method Resolution Order The method resolution order (or MRO) tells Python how to search for inherited methods. This comes in handy when you\u2019re using super() because the MRO tells you exactly where Python will look for a method you\u2019re calling with super() and in what order. Every class has an . mro attribute that allows us to inspect the order, so let\u2019s do that: >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Triangle'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class 'object'>) This tells us that methods will be searched first in Rightpyramid, then in Triangle, then in Square, then Rectangle, and then, if nothing is found, in object, from which all classes originate. The problem here is that the interpreter is searching for .area() in Triangle before Square and Rectangle, and upon finding .area() in Triangle, Python calls it instead of the one you want. Because Triangle.area() expects there to be a .height and a .base attribute, Python throws an AttributeError. The problem here is that the interpreter is searching for .area() in Triangle before Square and Rectangle, and upon finding .area() in Triangle, Python calls it instead of the one you want. Because Triangle.area() expects there to be a .height and a .base attribute, Python throws an AttributeError. Luckily, you have some control over how the MRO is constructed. Just by changing the signature of the RightPyramid class, you can search in the order you want, and the methods will resolve correctly: class RightPyramid(Square, Triangle): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height super().__init__(self.base) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area Notice that RightPyramid initializes partially with the . init () from the Square class. This allows .area() to use the .length on the object, as is designed. Now, you can build a pyramid, inspect the MRO, and calculate the surface area: >>> pyramid = RightPyramid(2, 4) >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class '__main__.Triangle'>, <class 'object'>) >>> pyramid.area() 20.0 You see that the MRO is now what you\u2019d expect, and you can inspect the area of the pyramid as well, thanks to .area() and .perimeter(). There\u2019s still a problem here, though. For the sake of simplicity, I did a few things wrong in this example: the first, and arguably most importantly, was that I had two separate classes with the same method name and signature. This causes issues with method resolution, because the first instance of .area() that is encountered in the MRO list will be called. When you\u2019re using super() with multiple inheritance, it\u2019s imperative to design your classes to cooperate. Part of this is ensuring that your methods are unique so that they get resolved in the MRO, by making sure method signatures are unique\u2014whether by using method names or method parameters. In this case, to avoid a complete overhaul of your code, you can rename the Triangle class\u2019s .area() method to .tri_area(). This way, the area methods can continue using class properties rather than taking external parameters: class Triangle: def __init__(self, base, height): self.base = base self.height = height super().__init__() def tri_area(self): return 0.5 * self.base * self.height Let\u2019s also go ahead and use this in the RightPyramid class: class RightPyramid(Square, Triangle): def __init__(self, base, slant_height): self.base = base self.slant_height = slant_height super().__init__(self.base) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area def area_2(self): base_area = super().area() triangle_area = super().tri_area() return triangle_area * 4 + base_area The next issue here is that the code doesn\u2019t have a delegated Triangle object like it does for a Square object, so calling .area_2() will give us an AttributeError since .base and .height don\u2019t have any values. You need to do two things to fix this: All methods that are called with super() need to have a call to their superclass\u2019s version of that method. This means that you will need to add super(). init () to the . init () methods of Triangle and Rectangle. Redesign all the . init () calls to take a keyword dictionary. See the complete code below. class Rectangle: def __init__(self, length, width, **kwargs): self.length = length self.width = width super().__init__(**kwargs) def area(self): return self.length * self.width def perimeter(self): return 2 * self.length + 2 * self.width # Here we declare that the Square class inherits from # the Rectangle class class Square(Rectangle): def __init__(self, length, **kwargs): super().__init__(length=length, width=length, **kwargs) class Cube(Square): def surface_area(self): face_area = super().area() return face_area * 6 def volume(self): face_area = super().area() return face_area * self.length class Triangle: def __init__(self, base, height, **kwargs): self.base = base self.height = height super().__init__(**kwargs) def tri_area(self): return 0.5 * self.base * self.height class RightPyramid(Square, Triangle): def __init__(self, base, slant_height, **kwargs): self.base = base self.slant_height = slant_height kwargs[\"height\"] = slant_height kwargs[\"length\"] = base super().__init__(base=base, **kwargs) def area(self): base_area = super().area() perimeter = super().perimeter() return 0.5 * perimeter * self.slant_height + base_area def area_2(self): base_area = super().area() triangle_area = super().tri_area() return triangle_area * 4 + base_area There are a number of important differences in this code: kwargs is modified in some places (such as RightPyramid. init ()): This will allow users of these objects to instantiate them only with the arguments that make sense for that particular object. Setting up named arguments before kwargs: You can see this in RightPyramid. init (). This has the neat effect of popping that key right out of the kwargs dictionary, so that by the time that it ends up at the end of the MRO in the object class, **kwargs is empty. Now, when you use these updated classes, you have this: >>> pyramid = RightPyramid(base=2, slant_height=4) >>> pyramid.area() 20.0 >>> pyramid.area_2() 20.0 It works! You\u2019ve used super() to successfully navigate a complicated class hierarchy while using both inheritance and composition to create new classes with minimal reimplementation. Quiz!!! When Rectangle calls super() what are the values of kwargs and what class gets it's init method called Answer: kwargs is still containing {'base': 2, 'height': 4} values as they haven't been removed from kwargs super().__init__(**kwargs) calls Triangle's init , but Rectangle doesn't inherit from Triangle so why? Remember the MRO? Let's look at RightPyramid's MRO >>> RightPyramid.__mro__ (<class '__main__.RightPyramid'>, <class '__main__.Square'>, <class '__main__.Rectangle'>, <class '__main__.Triangle'>, <class 'object'>) Even though Rectangle doesn't inherit from Triangle, Triangle is next in the list of classes to search for an init method. You'll notice object is at the end of the list. super().__init__(**kwargs) Multiple Inheritance Alternatives As you can see, multiple inheritance can be useful but also lead to very complicated situations and code that is hard to read. It\u2019s also rare to have objects that neatly inherit everything from more than multiple other objects. If you see yourself beginning to use multiple inheritance and a complicated class hierarchy, it\u2019s worth asking yourself if you can achieve code that is cleaner and easier to understand by using composition instead of inheritance. With composition, you can add very specific functionality to your classes from a specialized, simple class called a mixin. Since this article is focused on inheritance, I won\u2019t go into too much detail on composition and how to wield it in Python, but here\u2019s a short example using VolumeMixin to give specific functionality to our 3D objects\u2014in this case, a volume calculation: class Rectangle: def __init__(self, length, width): self.length = length self.width = width def area(self): return self.length * self.width class Square(Rectangle): def __init__(self, length): super().__init__(length, length) class VolumeMixin: def volume(self): return self.area() * self.height class Cube(VolumeMixin, Square): def __init__(self, length): super().__init__(length) self.height = length def face_area(self): return super().area() def surface_area(self): return super().area() * 6 In this example, the code was reworked to include a mixin called VolumeMixin. The mixin is then used by Cube and gives Cube the ability to calculate its volume, which is shown below: >>> cube = Cube(2) >>> cube.surface_area() 24 >>> cube.volume() 8 A super() Recap In this tutorial, you learned how to supercharge your classes with super(). Your journey started with a review of single inheritance and then showed how to call superclass methods easily with super(). You then learned how multiple inheritance works in Python, and techniques to combine super() with multiple inheritance. You also learned about how Python resolves method calls using the method resolution order (MRO), as well as how to inspect and modify the MRO to ensure appropriate methods are called at appropriate times. For more information about object-oriented programming in Python and using super(), check out these resources: Official super() documentation Python\u2019s super() Considered Super by Raymond Hettinger Object-Oriented Programming in Python 3","title":"Multiple Inheritance and Composition"},{"location":"notes/python/advanced/Decorators/","text":"Decorators @Add_This_Functionality_To_Any_Function! Simple Example: def my_decorator(original_function): def new_function(*args,**kwargs): print(\"you did it!\") of = original_function(*args,**kwargs) return of return new_function @my_decorator def my_func1(stuff): print(\"do things\") @my_decorator def my_func2(stuff): print(\"do things\") @my_decorator def my_func3(stuff): print(\"do things\") my_func1(1);my_func2(1);my_func3(1); Tutorial: We need to discuss global variables and function closure [1] functions: def foo(): return 1; [2] scope a_string = 'this is a string' def foo(): print(locals()) foo() # {} print(globals()) # {..., 'a_string': 'this is a string'} [3] varible resolution rules Local variables with the same name as global ones don't modify the global one. [4] variable lifetime def foo(): x = 1; foo() # NameError: name 'x' is not defined [5] function arguments and parameters def foo(x): print(locals()) foo(1) # {'x': 1} def foo(x, y=0): # remember if no default it's mandatory return x - y foo() # Traceback (most recent call last): # File \"\", line 1, in # TypeError: foo() missing 1 required positional argument: 'x' [6] Nested Functions def outer(): x = 1 def inner(): print(x) inner() outer() # 1 [7] Functions are first class objectsin python # all objects in Python inherit from a common baseclass issubclass(int, object) # True def foo(): pass foo.__class__ # <type 'function'> issubclass(foo.__class__, object) # True #..so what? def add(x,y): return x + y def sub(x,y): return x - y def apply(func, x, y): return func(x, y) apply(add, 2, 1) # 3 apply(sub, 2, 1) # 1 #.. closure lead in def outer(): def inner(): print('this is inner') return inner # the function not what it returns foo = outer() foo # .inner at 0x10be011e0> foo() # 'this is inner' [8] Closures def outer(): x = 1 def inner(): print(x) return inner # the function not what it returns foo = outer() foo.__closure__ # (,) # aside: cell objects are used to store the free variables of a closure # without closures x would have not existed as after the call to outer x is gone based on variable life time. # inner functions defined in non-global scope remember what their enclosing namespaces looked like at definition time. foo() # 1 # let's tweak it def outer(x): def inner(): print(x) return inner print1 = outer(1) print2 = outer(2) print1() # 1 print2() # 2 [9] Decorators def outer(some_func): def inner(): print('before some_func') ret = some_func() # 1 return ret + 1 return inner def foo(): return 1 decorated = outer(foo) decorated() # we've added functionality to foo()! # what if we did this foo = outer(foo) foo() # before some_func # 2 # Let's write a more useful decorator class Coordinate(object): def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return \"Coord: \" + str(self.__dict__) def add(a, b): return Coordinate(a.x + b.x, a.y + b.y) def sub(a, b): return Coordinate(a.x - b.x, a.y - b.y) one = Coordinate(100, 200) two = Coordinate(300, 200) add(one, two) # Coord: {'y': 400, 'x': 400} # add this instance three = Coordinate(-100, -100) def wrapper(func): def checker(a, b): # 1 if a.x < 0 or a.y < 0: a = Coordinate(a.x if a.x > 0 else 0, a.y if a.y > 0 else 0) if b.x < 0 or b.y < 0: b = Coordinate(b.x if b.x > 0 else 0, b.y if b.y > 0 else 0) ret = func(a, b) if ret.x < 0 or ret.y < 0: ret = Coordinate(ret.x if ret.x > 0 else 0, ret.y if ret.y > 0 else 0) return ret return checker add = wrapper(add) sub = wrapper(sub) sub(one, two) add(one, three) [10] the @ symbol # so instead of wrapper(add)\\wrapper(sub), use @wrapper @wrapper def add(a, b): return Coordinate(a.x + b.x, a.y + b.y) @wrapper def sub(a, b): return Coordinate(a.x - b.x, a.y - b.y) [11] *args and **kwargs def one(*args): print(args) one() # () one(1,2,3) # (1, 2, 3) def two(x, y, *args): print(x, y, args) two('a','b','c') # Reminder # l = (1,2,3) # one(l[0], l[1], l[2]) # (1, 2, 3) # one(*l) # (1, 2, 3) def foo(**kwargs): print(kwargs) foo() foo(x=1, y=2) [12] More generic decorators def logger(func): def inner(*args, **kwargs): print('Arguments were: {}, {}'.format(args,kwargs)) return func(*args, **kwargs) return inner @logger def foo1(x,y=1): return x * y @logger def foo2(): return 2 foo1(5,4) # Arguments were: (5, 4), {} # 20 foo2() # Arguments were: (), {} # 2","title":"Decorators"},{"location":"notes/python/advanced/Decorators/#decorators","text":"@Add_This_Functionality_To_Any_Function!","title":"Decorators"},{"location":"notes/python/advanced/Decorators/#simple-example","text":"def my_decorator(original_function): def new_function(*args,**kwargs): print(\"you did it!\") of = original_function(*args,**kwargs) return of return new_function @my_decorator def my_func1(stuff): print(\"do things\") @my_decorator def my_func2(stuff): print(\"do things\") @my_decorator def my_func3(stuff): print(\"do things\") my_func1(1);my_func2(1);my_func3(1);","title":"Simple Example:"},{"location":"notes/python/advanced/Decorators/#tutorial","text":"We need to discuss global variables and function closure","title":"Tutorial:"},{"location":"notes/python/advanced/Decorators/#1-functions","text":"def foo(): return 1;","title":"[1] functions:"},{"location":"notes/python/advanced/Decorators/#2-scope","text":"a_string = 'this is a string' def foo(): print(locals()) foo() # {} print(globals()) # {..., 'a_string': 'this is a string'}","title":"[2] scope"},{"location":"notes/python/advanced/Decorators/#3-varible-resolution-rules","text":"Local variables with the same name as global ones don't modify the global one.","title":"[3] varible resolution rules"},{"location":"notes/python/advanced/Decorators/#4-variable-lifetime","text":"def foo(): x = 1; foo() # NameError: name 'x' is not defined","title":"[4] variable lifetime"},{"location":"notes/python/advanced/Decorators/#5-function-arguments-and-parameters","text":"def foo(x): print(locals()) foo(1) # {'x': 1} def foo(x, y=0): # remember if no default it's mandatory return x - y foo() # Traceback (most recent call last): # File \"\", line 1, in # TypeError: foo() missing 1 required positional argument: 'x'","title":"[5] function arguments and parameters"},{"location":"notes/python/advanced/Decorators/#6-nested-functions","text":"def outer(): x = 1 def inner(): print(x) inner() outer() # 1","title":"[6] Nested Functions"},{"location":"notes/python/advanced/Decorators/#7-functions-are-first-class-objectsin-python","text":"# all objects in Python inherit from a common baseclass issubclass(int, object) # True def foo(): pass foo.__class__ # <type 'function'> issubclass(foo.__class__, object) # True #..so what? def add(x,y): return x + y def sub(x,y): return x - y def apply(func, x, y): return func(x, y) apply(add, 2, 1) # 3 apply(sub, 2, 1) # 1 #.. closure lead in def outer(): def inner(): print('this is inner') return inner # the function not what it returns foo = outer() foo # .inner at 0x10be011e0> foo() # 'this is inner'","title":"[7] Functions are first class objectsin python"},{"location":"notes/python/advanced/Decorators/#8-closures","text":"def outer(): x = 1 def inner(): print(x) return inner # the function not what it returns foo = outer() foo.__closure__ # (,) # aside: cell objects are used to store the free variables of a closure # without closures x would have not existed as after the call to outer x is gone based on variable life time. # inner functions defined in non-global scope remember what their enclosing namespaces looked like at definition time. foo() # 1 # let's tweak it def outer(x): def inner(): print(x) return inner print1 = outer(1) print2 = outer(2) print1() # 1 print2() # 2","title":"[8] Closures"},{"location":"notes/python/advanced/Decorators/#9-decorators","text":"def outer(some_func): def inner(): print('before some_func') ret = some_func() # 1 return ret + 1 return inner def foo(): return 1 decorated = outer(foo) decorated() # we've added functionality to foo()! # what if we did this foo = outer(foo) foo() # before some_func # 2 # Let's write a more useful decorator class Coordinate(object): def __init__(self, x, y): self.x = x self.y = y def __repr__(self): return \"Coord: \" + str(self.__dict__) def add(a, b): return Coordinate(a.x + b.x, a.y + b.y) def sub(a, b): return Coordinate(a.x - b.x, a.y - b.y) one = Coordinate(100, 200) two = Coordinate(300, 200) add(one, two) # Coord: {'y': 400, 'x': 400} # add this instance three = Coordinate(-100, -100) def wrapper(func): def checker(a, b): # 1 if a.x < 0 or a.y < 0: a = Coordinate(a.x if a.x > 0 else 0, a.y if a.y > 0 else 0) if b.x < 0 or b.y < 0: b = Coordinate(b.x if b.x > 0 else 0, b.y if b.y > 0 else 0) ret = func(a, b) if ret.x < 0 or ret.y < 0: ret = Coordinate(ret.x if ret.x > 0 else 0, ret.y if ret.y > 0 else 0) return ret return checker add = wrapper(add) sub = wrapper(sub) sub(one, two) add(one, three)","title":"[9] Decorators"},{"location":"notes/python/advanced/Decorators/#10-the-symbol","text":"# so instead of wrapper(add)\\wrapper(sub), use @wrapper @wrapper def add(a, b): return Coordinate(a.x + b.x, a.y + b.y) @wrapper def sub(a, b): return Coordinate(a.x - b.x, a.y - b.y)","title":"[10] the @ symbol"},{"location":"notes/python/advanced/Decorators/#11-args-and-kwargs","text":"def one(*args): print(args) one() # () one(1,2,3) # (1, 2, 3) def two(x, y, *args): print(x, y, args) two('a','b','c') # Reminder # l = (1,2,3) # one(l[0], l[1], l[2]) # (1, 2, 3) # one(*l) # (1, 2, 3) def foo(**kwargs): print(kwargs) foo() foo(x=1, y=2)","title":"[11] *args and **kwargs"},{"location":"notes/python/advanced/Decorators/#12-more-generic-decorators","text":"def logger(func): def inner(*args, **kwargs): print('Arguments were: {}, {}'.format(args,kwargs)) return func(*args, **kwargs) return inner @logger def foo1(x,y=1): return x * y @logger def foo2(): return 2 foo1(5,4) # Arguments were: (5, 4), {} # 20 foo2() # Arguments were: (), {} # 2","title":"[12] More generic decorators"},{"location":"notes/python/advanced/Generators/","text":"Generators A simplified Iterator Quick Example: def all_even(): n = 0 while True: yield n n += 2 x = all_even() print(next(x)) # 0 print(next(x)) # 2 print(next(x)) # 4 #.... forever What are generators in Python? There is a lot of overhead in building an iterator in Python; we have to implement a class with iter () and next () method, keep track of internal states, raise StopIteration when there was no values to be returned etc. This is both lengthy and counter intuitive. Generator comes into rescue in such situations. Python generators are a simple way of creating iterators. All the overhead we mentioned above are automatically handled by generators in Python. Simply speaking, a generator is a function that returns an object (iterator) which we can iterate over (one value at a time). How to create a generator in Python? It is fairly simple to create a generator in Python. It is as easy as defining a normal function with yield statement instead of a return statement. If a function contains at least one yield statement (it may contain other yield or return statements), it becomes a generator function. Both yield and return will return some value from a function. The difference is that, while a return statement terminates a function entirely, yield statement pauses the function saving all its states and later continues from there on successive calls. Differences between Generator function and a Normal function Generator function contains one or more yield statement. When called, it returns an object (iterator) but does not start execution immediately. Methods like iter () and next () are implemented automatically. So we can iterate through the items using next(). Once the function yields, the function is paused and the control is transferred to the caller. Local variables and their states are remembered between successive calls. Finally, when the function terminates, StopIteration is raised automatically on further calls. Simple Example: # A simple generator function def my_gen(): n = 1 print('This is printed first') # Generator function contains yield statements yield n n += 1 print('This is printed second') yield n n += 1 print('This is printed at last') yield n mg = my_gen() next(mg) # This is printed first next(mg) # This is printed second next(mg) # This is printed third next(mg) # Raises error... One interesting thing to note in the above example is that, the value of variable n is remembered between each call. Unlike normal functions, the local variables are not destroyed when the function yields. Furthermore, the generator object can be iterated only once. To restart the process we need to create another generator object using something like a = my_gen(). Note: One final thing to note is that we can use generators with for loops directly. This is because, a for loop takes an iterator and iterates over it using next() function. It automatically ends when StopIteration is raised. Check here to know how a for loop is actually implemented in Python . Python Generators with a Loop The above example is of less use and we studied it just to get an idea of what was happening in the background. Normally, generator functions are implemented with a loop having a suitable terminating condition. Let's take an example of a generator that reverses a string. def rev_str(my_str): length = len(my_str) for i in range(length - 1,-1,-1): yield my_str[i] # For loop to reverse the string # Output: # o # l # l # e # h for char in rev_str(\"hello\"): print(char) In this example, we use range() function to get the index in reverse order using the for loop. It turns out that this generator function not only works with string, but also with other kind of iterables like list, tuple etc. Python Generator Expression Simple generators can be easily created on the fly using generator expressions. It makes building generators easy. Same as lambda function creates an anonymous function, generator expression creates an anonymous generator function. The syntax for generator expression is similar to that of a list comprehension in Python. But the square brackets are replaced with round parentheses. The major difference between a list comprehension and a generator expression is that while list comprehension produces the entire list, generator expression produces one item at a time. They are kind of lazy, producing items only when asked for. For this reason, a generator expression is much more memory efficient than an equivalent list comprehension. # Initialize the list my_list = [1, 3, 6, 10] # square each term using list comprehension # Output: [1, 9, 36, 100] [x**2 for x in my_list] # same thing can be done using generator expression # Output: at 0x0000000002EBDAF8> y = (x**2 for x in my_list) print(next(y)) # 1 print(next(y)) # 9 print(next(y)) # 36 print(next(y)) # 100 print(next(y)) # error Generator expression can be used inside functions. When used in such a way, the round parentheses can be dropped. >>> sum(x**2 for x in my_list) 146 >>> max(x**2 for x in my_list) 100 Why generators are used in Python? Easy to Implement: Generators can be implemented in a clear and concise way as compared to their iterator class counterpart. Following is an example to implement a sequence of power of 2's using iterator class. class PowTwo: def __init__(self, max = 0): self.max = max def __iter__(self): self.n = 0 return self def __next__(self): if self.n > self.max: raise StopIteration result = 2 ** self.n self.n += 1 return result This was lengthy. Now lets do the same using a generator function. def PowTwoGen(max = 0): n = 0 while n < max: yield 2 ** n n += 1 Memory Efficient: A normal function to return a sequence will create the entire sequence in memory before returning the result. This is an overkill if the number of items in the sequence is very large. Generator implementation of such sequence is memory friendly and is preferred since it only produces one item at a time. Represent Infinite Stream: Generators are excellent medium to represent an infinite stream of data. Infinite streams cannot be stored in memory and since generators produce only one item at a time, it can represent infinite stream of data. The following example can generate all the even numbers (at least in theory). def all_even(): n = 0 while True: yield n n += 2 Pipelining Generators: Generators can be used to pipeline a series of operations. This is best illustrated using an example. Suppose we have a log file from a famous fast food chain. The log file has a column (4th column) that keeps track of the number of pizza sold every hour and we want to sum it to find the total pizzas sold in 5 years. Assume everything is in string and numbers that are not available are marked as 'N/A'. A generator implementation of this could be as follows. # - sells.log - # # 1 2 3 1 # 1 2 3 2 # 1 2 3 3 # 1 2 3 4 # 1 2 3 5 # 1 2 3 6 # 1 2 3 7 # 1 2 3 8 # 1 2 3 9 # 1 2 3 10 # - - # with open('sells.log') as file: pizza_col = (line.split()[3] for line in file) per_hour = (int(x) for x in pizza_col if x != 'N/A') print(\"Total pizzas sold = \",sum(per_hour)) This pipelining is efficient and easy to read (and yes, a lot cooler!).","title":"Generators"},{"location":"notes/python/advanced/Generators/#generators","text":"A simplified Iterator","title":"Generators"},{"location":"notes/python/advanced/Generators/#quick-example","text":"def all_even(): n = 0 while True: yield n n += 2 x = all_even() print(next(x)) # 0 print(next(x)) # 2 print(next(x)) # 4 #.... forever","title":"Quick Example:"},{"location":"notes/python/advanced/Generators/#what-are-generators-in-python","text":"There is a lot of overhead in building an iterator in Python; we have to implement a class with iter () and next () method, keep track of internal states, raise StopIteration when there was no values to be returned etc. This is both lengthy and counter intuitive. Generator comes into rescue in such situations. Python generators are a simple way of creating iterators. All the overhead we mentioned above are automatically handled by generators in Python. Simply speaking, a generator is a function that returns an object (iterator) which we can iterate over (one value at a time).","title":"What are generators in Python?"},{"location":"notes/python/advanced/Generators/#how-to-create-a-generator-in-python","text":"It is fairly simple to create a generator in Python. It is as easy as defining a normal function with yield statement instead of a return statement. If a function contains at least one yield statement (it may contain other yield or return statements), it becomes a generator function. Both yield and return will return some value from a function. The difference is that, while a return statement terminates a function entirely, yield statement pauses the function saving all its states and later continues from there on successive calls.","title":"How to create a generator in Python?"},{"location":"notes/python/advanced/Generators/#differences-between-generator-function-and-a-normal-function","text":"Generator function contains one or more yield statement. When called, it returns an object (iterator) but does not start execution immediately. Methods like iter () and next () are implemented automatically. So we can iterate through the items using next(). Once the function yields, the function is paused and the control is transferred to the caller. Local variables and their states are remembered between successive calls. Finally, when the function terminates, StopIteration is raised automatically on further calls. Simple Example: # A simple generator function def my_gen(): n = 1 print('This is printed first') # Generator function contains yield statements yield n n += 1 print('This is printed second') yield n n += 1 print('This is printed at last') yield n mg = my_gen() next(mg) # This is printed first next(mg) # This is printed second next(mg) # This is printed third next(mg) # Raises error... One interesting thing to note in the above example is that, the value of variable n is remembered between each call. Unlike normal functions, the local variables are not destroyed when the function yields. Furthermore, the generator object can be iterated only once. To restart the process we need to create another generator object using something like a = my_gen(). Note: One final thing to note is that we can use generators with for loops directly. This is because, a for loop takes an iterator and iterates over it using next() function. It automatically ends when StopIteration is raised. Check here to know how a for loop is actually implemented in Python .","title":"Differences between Generator function and a Normal function"},{"location":"notes/python/advanced/Generators/#python-generators-with-a-loop","text":"The above example is of less use and we studied it just to get an idea of what was happening in the background. Normally, generator functions are implemented with a loop having a suitable terminating condition. Let's take an example of a generator that reverses a string. def rev_str(my_str): length = len(my_str) for i in range(length - 1,-1,-1): yield my_str[i] # For loop to reverse the string # Output: # o # l # l # e # h for char in rev_str(\"hello\"): print(char) In this example, we use range() function to get the index in reverse order using the for loop. It turns out that this generator function not only works with string, but also with other kind of iterables like list, tuple etc.","title":"Python Generators with a Loop"},{"location":"notes/python/advanced/Generators/#python-generator-expression","text":"Simple generators can be easily created on the fly using generator expressions. It makes building generators easy. Same as lambda function creates an anonymous function, generator expression creates an anonymous generator function. The syntax for generator expression is similar to that of a list comprehension in Python. But the square brackets are replaced with round parentheses. The major difference between a list comprehension and a generator expression is that while list comprehension produces the entire list, generator expression produces one item at a time. They are kind of lazy, producing items only when asked for. For this reason, a generator expression is much more memory efficient than an equivalent list comprehension. # Initialize the list my_list = [1, 3, 6, 10] # square each term using list comprehension # Output: [1, 9, 36, 100] [x**2 for x in my_list] # same thing can be done using generator expression # Output: at 0x0000000002EBDAF8> y = (x**2 for x in my_list) print(next(y)) # 1 print(next(y)) # 9 print(next(y)) # 36 print(next(y)) # 100 print(next(y)) # error Generator expression can be used inside functions. When used in such a way, the round parentheses can be dropped. >>> sum(x**2 for x in my_list) 146 >>> max(x**2 for x in my_list) 100","title":"Python Generator Expression"},{"location":"notes/python/advanced/Generators/#why-generators-are-used-in-python","text":"Easy to Implement: Generators can be implemented in a clear and concise way as compared to their iterator class counterpart. Following is an example to implement a sequence of power of 2's using iterator class. class PowTwo: def __init__(self, max = 0): self.max = max def __iter__(self): self.n = 0 return self def __next__(self): if self.n > self.max: raise StopIteration result = 2 ** self.n self.n += 1 return result This was lengthy. Now lets do the same using a generator function. def PowTwoGen(max = 0): n = 0 while n < max: yield 2 ** n n += 1 Memory Efficient: A normal function to return a sequence will create the entire sequence in memory before returning the result. This is an overkill if the number of items in the sequence is very large. Generator implementation of such sequence is memory friendly and is preferred since it only produces one item at a time. Represent Infinite Stream: Generators are excellent medium to represent an infinite stream of data. Infinite streams cannot be stored in memory and since generators produce only one item at a time, it can represent infinite stream of data. The following example can generate all the even numbers (at least in theory). def all_even(): n = 0 while True: yield n n += 2 Pipelining Generators: Generators can be used to pipeline a series of operations. This is best illustrated using an example. Suppose we have a log file from a famous fast food chain. The log file has a column (4th column) that keeps track of the number of pizza sold every hour and we want to sum it to find the total pizzas sold in 5 years. Assume everything is in string and numbers that are not available are marked as 'N/A'. A generator implementation of this could be as follows. # - sells.log - # # 1 2 3 1 # 1 2 3 2 # 1 2 3 3 # 1 2 3 4 # 1 2 3 5 # 1 2 3 6 # 1 2 3 7 # 1 2 3 8 # 1 2 3 9 # 1 2 3 10 # - - # with open('sells.log') as file: pizza_col = (line.split()[3] for line in file) per_hour = (int(x) for x in pizza_col if x != 'N/A') print(\"Total pizzas sold = \",sum(per_hour)) This pipelining is efficient and easy to read (and yes, a lot cooler!).","title":"Why generators are used in Python?"},{"location":"notes/python/base_modules/Pdb/","text":"Pdb Useage - Docs You can use import pdb pdb.set_trace() or New: as of 3.7 breakpoint() breakpoint() to enter debug mode. Hidden gems Use ! to escape default pdb commands like n(ext), c(ont(inue)) and h(elp) >>> c = 1 >>> breakpoint() --Return-- > <stdin>(1)<module>()->None (Pdb) !c 1 (Pdb)","title":"Pdb"},{"location":"notes/python/base_modules/Pdb/#pdb","text":"","title":"Pdb"},{"location":"notes/python/base_modules/Pdb/#useage-docs","text":"You can use import pdb pdb.set_trace() or New: as of 3.7 breakpoint() breakpoint() to enter debug mode.","title":"Useage - Docs"},{"location":"notes/python/base_modules/Pdb/#hidden-gems","text":"Use ! to escape default pdb commands like n(ext), c(ont(inue)) and h(elp) >>> c = 1 >>> breakpoint() --Return-- > <stdin>(1)<module>()->None (Pdb) !c 1 (Pdb)","title":"Hidden gems"},{"location":"notes/python/base_modules/collections/Counter/","text":"Counter Counter is a dict subclass for counting hashable objects: >>> c = Counter() # a new, empty counter >>> c Counter() >>> c = Counter('gallahad') # a new counter from an iterable >>> c Counter({'a': 3, 'l': 2, 'g': 1, 'h': 1, 'd': 1}) # a new counter from a mapping >>> c = Counter({'red': 4, 'blue': 2}) # a new counter from a mapping >>> c Counter({'red': 4, 'blue': 2}) >>> c = Counter(cats=4, dogs=8) # a new counter from keyword args >>> c Counter({'dogs': 8, 'cats': 4}) >>> Counter([1,2,2,3,3,3,4,4,4,4]) Counter({4: 4, 3: 3, 2: 2, 1: 1}) Delete objects as shown below: # Delete records as shown below: >>> c = Counter(['eggs', 'ham']) >>> c Counter({'eggs': 1, 'ham': 1}) >>> del c['ham'] >>> c Counter({'eggs': 1}) Counter objects support three methods beyond those available for all dictionaries: elements() Return an iterator over elements repeating each as many times as its count. Elements are returned in arbitrary order. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2) >>> list(c.elements()) ['a', 'a', 'a', 'a', 'b', 'b'] most_common([n]) Return a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered arbitrarily: >>> Counter('abracadabra').most_common(3) [('a', 5), ('r', 2), ('b', 2)] subtract([iterable-or-mapping]) Elements are subtracted from an iterable or from another mapping (or counter). Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative. >>> c = Counter(a=4, b=2, c=0, d=-2) >>> d = Counter(a=1, b=2, c=3, d=4) >>> c.subtract(d) >>> c Counter({'a': 3, 'b': 0, 'c': -3, 'd': -6})","title":"Counter"},{"location":"notes/python/base_modules/collections/Counter/#counter","text":"Counter is a dict subclass for counting hashable objects: >>> c = Counter() # a new, empty counter >>> c Counter() >>> c = Counter('gallahad') # a new counter from an iterable >>> c Counter({'a': 3, 'l': 2, 'g': 1, 'h': 1, 'd': 1}) # a new counter from a mapping >>> c = Counter({'red': 4, 'blue': 2}) # a new counter from a mapping >>> c Counter({'red': 4, 'blue': 2}) >>> c = Counter(cats=4, dogs=8) # a new counter from keyword args >>> c Counter({'dogs': 8, 'cats': 4}) >>> Counter([1,2,2,3,3,3,4,4,4,4]) Counter({4: 4, 3: 3, 2: 2, 1: 1})","title":"Counter"},{"location":"notes/python/base_modules/collections/Counter/#delete-objects-as-shown-below","text":"# Delete records as shown below: >>> c = Counter(['eggs', 'ham']) >>> c Counter({'eggs': 1, 'ham': 1}) >>> del c['ham'] >>> c Counter({'eggs': 1}) Counter objects support three methods beyond those available for all dictionaries:","title":"Delete objects as shown below:"},{"location":"notes/python/base_modules/collections/Counter/#elements","text":"Return an iterator over elements repeating each as many times as its count. Elements are returned in arbitrary order. If an element\u2019s count is less than one, elements() will ignore it. >>> c = Counter(a=4, b=2, c=0, d=-2) >>> list(c.elements()) ['a', 'a', 'a', 'a', 'b', 'b']","title":"elements()"},{"location":"notes/python/base_modules/collections/Counter/#most_commonn","text":"Return a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered arbitrarily: >>> Counter('abracadabra').most_common(3) [('a', 5), ('r', 2), ('b', 2)]","title":"most_common([n])"},{"location":"notes/python/base_modules/collections/Counter/#subtractiterable-or-mapping","text":"Elements are subtracted from an iterable or from another mapping (or counter). Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative. >>> c = Counter(a=4, b=2, c=0, d=-2) >>> d = Counter(a=1, b=2, c=3, d=4) >>> c.subtract(d) >>> c Counter({'a': 3, 'b': 0, 'c': -3, 'd': -6})","title":"subtract([iterable-or-mapping])"},{"location":"notes/python/base_modules/collections/Default%20Dict/","text":"Default Dict Initialize dictionary values with a data type. Returns a new dictionary-like object. defaultdict is a subclass of the built-in dict class. The first argument provides the initial value for the default_factory attribute; it defaults to None . All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments. from collections import defaultdict # Try to append all numbers in a list to their descriptions s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = {} for k, v in s: d[k].append(v) # You can't because there is no default data type for the values of the keys being created. >>> Traceback (most recent call last): File \"\", line 1, in File \"\", line 6, in KeyError: 'yellow' # Use a default dict to accomplish this: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = defaultdict(list) for k, v in s: d[k].append(v) >>> d defaultdict(, {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}) # You can accomplish this with the base dictionary like this: >>> d = {} >>> d.setdefault('yellow', []) [] >>> d {'yellow': []} or >>> d = {} >>> d.setdefault('yellow', list) >>> d {'yellow': } # ...and our previous example: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = {} for k, v in s: d.setdefault(k, []).append(v) >>> d {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}","title":"default_dict()"},{"location":"notes/python/base_modules/collections/Default%20Dict/#default-dict","text":"Initialize dictionary values with a data type. Returns a new dictionary-like object. defaultdict is a subclass of the built-in dict class. The first argument provides the initial value for the default_factory attribute; it defaults to None . All remaining arguments are treated the same as if they were passed to the dict constructor, including keyword arguments. from collections import defaultdict # Try to append all numbers in a list to their descriptions s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = {} for k, v in s: d[k].append(v) # You can't because there is no default data type for the values of the keys being created. >>> Traceback (most recent call last): File \"\", line 1, in File \"\", line 6, in KeyError: 'yellow' # Use a default dict to accomplish this: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = defaultdict(list) for k, v in s: d[k].append(v) >>> d defaultdict(, {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}) # You can accomplish this with the base dictionary like this: >>> d = {} >>> d.setdefault('yellow', []) [] >>> d {'yellow': []} or >>> d = {} >>> d.setdefault('yellow', list) >>> d {'yellow': } # ...and our previous example: s = [('yellow', 1), ('blue', 2), ('yellow', 3), ('blue', 4), ('red', 1)] d = {} for k, v in s: d.setdefault(k, []).append(v) >>> d {'yellow': [1, 3], 'blue': [2, 4], 'red': [1]}","title":"Default Dict"},{"location":"notes/python/flask/basics/","text":"Flask Setup Source Miguel Grinberg and Real Python This is a great login tutorial flask_cors_and_redesign.py Here all we need to do is import the app from app import app app/__init__.py from flask import Flask from flask_cors import CORS app = Flask(__name__) CORS(app) from app import routes app/routes.py from app import app from flask import render_template, jsonify import math @app.route('/', methods=['GET']) def index(): return render_template('index.html', variable='test_variable') @app.route('/api', methods=['GET']) def api(): return jsonify({'data' : 'Hello World!'}) @app.route('/map', methods=['GET']) def map(): return render_template('maps.html') # return jsonify({'data' : 'Hello World!'}) @app.route('/get_latlngs', methods=['GET']) def get_latlngs(): zip_latlng = { 'lat': 42.3601, 'lng': -71.0589, } post_latlngs = [ {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, ] num_of_posts = len(post_latlngs) rad_spacing = 2*math.pi/num_of_posts radius = 1 for i,v in enumerate(post_latlngs): post_latlngs[i]['lat'] = zip_latlng['lat'] + radius*math.cos(i*rad_spacing) post_latlngs[i]['lng'] = zip_latlng['lng'] + radius*math.sin(i*rad_spacing) return jsonify(post_latlngs) app/models.py # Not shown, but this layout is for directory structure mainly. Before running it, though, Flask needs to be told how to import it, by setting the FLASK_APP environment variable: (venv) $ export FLASK_APP=microblog.py If you are using Microsoft Windows, use set instead of export in the command above. Are you ready to be blown away? You can run your first web application, with the following command: bash (venv) $ flask run * Serving Flask app \"microblog\" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)","title":"Basics"},{"location":"notes/python/flask/basics/#flask-setup","text":"Source Miguel Grinberg and Real Python This is a great login tutorial flask_cors_and_redesign.py Here all we need to do is import the app from app import app app/__init__.py from flask import Flask from flask_cors import CORS app = Flask(__name__) CORS(app) from app import routes app/routes.py from app import app from flask import render_template, jsonify import math @app.route('/', methods=['GET']) def index(): return render_template('index.html', variable='test_variable') @app.route('/api', methods=['GET']) def api(): return jsonify({'data' : 'Hello World!'}) @app.route('/map', methods=['GET']) def map(): return render_template('maps.html') # return jsonify({'data' : 'Hello World!'}) @app.route('/get_latlngs', methods=['GET']) def get_latlngs(): zip_latlng = { 'lat': 42.3601, 'lng': -71.0589, } post_latlngs = [ {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, {'lat': 0, 'lng': 0}, ] num_of_posts = len(post_latlngs) rad_spacing = 2*math.pi/num_of_posts radius = 1 for i,v in enumerate(post_latlngs): post_latlngs[i]['lat'] = zip_latlng['lat'] + radius*math.cos(i*rad_spacing) post_latlngs[i]['lng'] = zip_latlng['lng'] + radius*math.sin(i*rad_spacing) return jsonify(post_latlngs) app/models.py # Not shown, but this layout is for directory structure mainly. Before running it, though, Flask needs to be told how to import it, by setting the FLASK_APP environment variable: (venv) $ export FLASK_APP=microblog.py If you are using Microsoft Windows, use set instead of export in the command above. Are you ready to be blown away? You can run your first web application, with the following command: bash (venv) $ flask run * Serving Flask app \"microblog\" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)","title":"Flask Setup"},{"location":"notes/python/flask/links_to_checkout/","text":"Some Good Links For Additional Tutorials Getting Started with Flask. It has the new flask directory structure Continuing the tutorial: Part 1 - MySQL, Flask-SQLAlchemy, Flask-Migrate, Flask-Login, Blueprints, Flask-WTF Part 2 - doesn't exist yet","title":"Links to Check Out"},{"location":"notes/python/flask/links_to_checkout/#some-good-links-for-additional-tutorials","text":"Getting Started with Flask. It has the new flask directory structure Continuing the tutorial: Part 1 - MySQL, Flask-SQLAlchemy, Flask-Migrate, Flask-Login, Blueprints, Flask-WTF Part 2 - doesn't exist yet","title":"Some Good Links For Additional Tutorials"},{"location":"notes/python/jupyter/basics/","text":"Jupyter Notebook ...Reference Lab for essentially the same behavior Lab Run jupyter remotely from container Source: benjlindsay To run Jupyter Lab on a login node, you need to open 2 terminal windows. In the first window: Note to self, when doing this for containers, run this remembering we are mapping the container port 5678 to server port 5678 with -p 5678:5678 . Example command: docker run -it --ipc=host --name=ben_pytorch_tutorial -p 5678:5678 --runtime=nvidia -v /home/bb927/Documents/pytorch:/workspace pytorch/pytorch:latest bash Note, additional step to install jupyter could be required $ pip install jupyterlab . (You'll need a container with python in it) Then inside the container start jupyter: $ jupyter lab --ip 0.0.0.0 --port 5678 --no-browser --allow-root and see it running... [I 22:05:13.843 LabApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret [I 22:05:14.047 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab [I 22:05:14.047 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 22:05:14.050 LabApp] Serving notebooks from local directory: /workspace [I 22:05:14.050 LabApp] The Jupyter Notebook is running at: [I 22:05:14.050 LabApp] http://353e1fa49ecd:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [I 22:05:14.050 LabApp] or http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [I 22:05:14.050 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 22:05:14.053 LabApp] To access the notebook, open this file in a browser: file:///root/.local/share/jupyter/runtime/nbserver-45-open.html Or copy and paste one of these URLs: http://353e1fa49ecd:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX or http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then in the second terminal window run: $ ssh -CNL localhost:5678:localhost:5678 username@hostname Finally copy the link from the jupyter lab step into your browser (http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX). Run jupyter remotely from server Log into server and start jupyter: $ ssh username@hostname $ jupyter lab --no-browser --port=5678 ... [I 10:17:14.160 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 10:17:14.160 LabApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then in the second window (local machine) run: $ ssh -CNL localhost:5678:localhost:5678 username@hostname You should be able to go to http://localhost:5678 to see jupyter lab now.","title":"Run Remotely"},{"location":"notes/python/jupyter/basics/#jupyter","text":"","title":"Jupyter"},{"location":"notes/python/jupyter/basics/#notebook","text":"...Reference Lab for essentially the same behavior","title":"Notebook"},{"location":"notes/python/jupyter/basics/#lab","text":"","title":"Lab"},{"location":"notes/python/jupyter/basics/#run-jupyter-remotely-from-container","text":"Source: benjlindsay To run Jupyter Lab on a login node, you need to open 2 terminal windows. In the first window: Note to self, when doing this for containers, run this remembering we are mapping the container port 5678 to server port 5678 with -p 5678:5678 . Example command: docker run -it --ipc=host --name=ben_pytorch_tutorial -p 5678:5678 --runtime=nvidia -v /home/bb927/Documents/pytorch:/workspace pytorch/pytorch:latest bash Note, additional step to install jupyter could be required $ pip install jupyterlab . (You'll need a container with python in it) Then inside the container start jupyter: $ jupyter lab --ip 0.0.0.0 --port 5678 --no-browser --allow-root and see it running... [I 22:05:13.843 LabApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret [I 22:05:14.047 LabApp] JupyterLab extension loaded from /opt/conda/lib/python3.7/site-packages/jupyterlab [I 22:05:14.047 LabApp] JupyterLab application directory is /opt/conda/share/jupyter/lab [I 22:05:14.050 LabApp] Serving notebooks from local directory: /workspace [I 22:05:14.050 LabApp] The Jupyter Notebook is running at: [I 22:05:14.050 LabApp] http://353e1fa49ecd:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [I 22:05:14.050 LabApp] or http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX [I 22:05:14.050 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 22:05:14.053 LabApp] To access the notebook, open this file in a browser: file:///root/.local/share/jupyter/runtime/nbserver-45-open.html Or copy and paste one of these URLs: http://353e1fa49ecd:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX or http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then in the second terminal window run: $ ssh -CNL localhost:5678:localhost:5678 username@hostname Finally copy the link from the jupyter lab step into your browser (http://127.0.0.1:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX).","title":"Run jupyter remotely from container"},{"location":"notes/python/jupyter/basics/#run-jupyter-remotely-from-server","text":"Log into server and start jupyter: $ ssh username@hostname $ jupyter lab --no-browser --port=5678 ... [I 10:17:14.160 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 10:17:14.160 LabApp] Copy/paste this URL into your browser when you connect for the first time, to login with a token: http://localhost:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX Then in the second window (local machine) run: $ ssh -CNL localhost:5678:localhost:5678 username@hostname You should be able to go to http://localhost:5678 to see jupyter lab now.","title":"Run jupyter remotely from server"},{"location":"notes/python/pandas/basics/","text":"Pandas Some Notes pandas.pydata... Display pd.set_option('display.max_columns', 50) # How many to show pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show pd.set_option('display.width', 1000) # How far across the screen pd.set_option('display.max_colwidth', 1) # Column width in px pd.set_option('display.max_colwidth', 40) # Column width in px pd.set_option('expand_frame_repr', True) # allows for the representation of dataframes to stretch across pages, wrapped over the full column vs row-wise Making DataFrames From Dictionaries # Method 1: each key is a column with array datatype for values >>> data = {'a':[1,4], 'b':[2,5], 'c':[3,6]} >>> pandas.DataFrame(data) a b c 0 1 2 3 1 4 5 6 # Use 'columns' to specify order >>> pandas.DataFrame(data, columns=['b','c','a']) b c a 0 2 3 1 1 5 6 4 # Method 2: Creating Dataframe from list of dicts >>> data = [{'a': 1, 'b': 2, 'c': 3}, {'a': 10, 'b': 20, 'c': 30}] >>> pandas.DataFrame(data) a b c 0 1 2 3 1 10 20 30 From lists >>> rows = [[1,2,3],[4,5,6]] >>> pandas.DataFrame(rows, columns=['a','b','c']) a b c 0 1 2 3 1 4 5 6 From excel import pandas as pd cars = pd.read_excel(r'C:\\Users\\Ron\\Desktop\\Cars.xlsx') df = pd.DataFrame(cars, columns = ['Brand', 'Price']) Apply Function x = pd.DataFrame({'A':[1,1,1,1,2,2,2,2], 'B':[0,0,0,0,0,1,0,1], 'C':['a' for i in range(8)]}) y = pd.DataFrame({'A':[1,1,1,1,1,2,2,2,2,2], 'B':[1,0,1,0,1,0,1,0,1,1], 'C':['a' for i in range(10)]}) z = pd.DataFrame({'A':[1,1,1,1,2,2,2,2], 'B':[2,3,6,9,1,1,6,7], 'C':['a' for i in range(8)]}) def some_agg_function(df_group): # It's important for the values to be sorted to make this easier df_group = df_group.sort_values('B') nrows = len(df_group) if nrows % 2 == 0: middle_index = nrows / 2 - 1; # for 0 based inded purposes; chooses 2 out of (1,[2],3,4) else: middle_index = math.floor(nrows / 2) # pdb.set_trace() return df_group.iloc[int(middle_index),:] print(x); print(x[['A','B']].groupby('A').apply(some_agg_function)) print(y);print(y[['A','B']].groupby('A').apply(some_agg_function)) print(z);print(z[['A','B']].groupby('A').apply(some_agg_function)) sqlite3 Intgration Source: dataquest Just sqlite3 library In order to work with a SQLite database from Python, we first have to connect to it. We can do that using the connect function, which returns a Connection object: import sqlite3 conn = sqlite3.connect(\"flights.db\") Once we have a Connection object, we can then create a Cursor object. Cursors allow us to execute SQL queries against a database: cur = conn.cursor() Once we have a Cursor object, we can use it to execute a query against the database with the aptly named execute method. The below code will fetch the first 5 rows from the airlines table: cur.execute(\"select * from airlines limit 5;\") You may have noticed that we didn\u2019t assign the result of the above query to a variable. This is because we need to run another command to actually fetch the results. We can use the fetchall method to fetch all of the results of a query: results = cur.fetchall() print(results) [(0, '1', 'Private flight', '\\\\N', '-', None, None, None, 'Y'), (1, '2', '135 Airways', '\\\\N', None, 'GNL', 'GENERAL', 'United States', 'N'), (2, '3', '1Time Airline', '\\\\N', '1T', 'RNX', 'NEXTIME', 'South Africa', 'Y'), (3, '4', '2 Sqn No 1 Elementary Flying Training School', '\\\\N', None, 'WYT', None, 'United Kingdom', 'N'), (4, '5', '213 Flight Unit', '\\\\N', None, 'TFU', None, 'Russia', 'N')] As you can see, the results are formatted as a list of tuples. Each tuple corresponds to a row in the database that we accessed. Dealing with data this way is fairly painful. We\u2019d need to manually add column heads, and manually parse the data. Luckily, the pandas library has an easier way, which we\u2019ll look at in the next section. Before we move on, it\u2019s good practice to close Connection objects and Cursor objects that are open. This prevents the SQLite database from being locked. When a SQLite database is locked, you may be unable to update the database, and may get errors. We can close the Cursor and the Connection like this: cur.close() conn.close() Add Pandas import pandas as pd import sqlite3 conn = sqlite3.connect(\"flights.db\") df = pd.read_sql_query(\"select * from airlines limit 5;\", conn) Inserting rows with Python To insert a row, we need to write an INSERT query. The below code will add a new row to the airlines table. We specify 9 values to insert, one for each column in airlines. This will add a new row to the table. cur = conn.cursor() cur.execute(\"insert into airlines values (6048, 19846, 'Test flight', '', '', null, null, null, 'Y')\") conn.commit() You want to avoid doing this! Inserting values with Python string formatting makes your program vulnerable to SQL Injection attacks. Luckily, sqlite3 has a straightforward way to inject dynamic values without relying on string formatting: cur = conn.cursor() values = ('Test Flight', 'Y') cur.execute(\"insert into airlines values (6049, 19847, ?, '', '', null, null, null, ?)\", values) conn.commit() Updating rows We can modify rows in a SQLite table using the execute method: cur = conn.cursor() values = ('USA', 19847) cur.execute(\"update airlines set country=? where id=?\", values) conn.commit() Deleting rows Finally, we can delete the rows in a database using the execute method: cur = conn.cursor()values = (19847, ) cur.execute(\"delete from airlines where id=?\", values)conn.commit() Creating tables with pandas The pandas package gives us a much faster way to create tables. We just have to create a DataFrame first, then export it to a SQL table. First, we\u2019ll create a DataFrame: from datetime import datetime df = pd.DataFrame( [[1, datetime(2016, 9, 29, 0, 0) , datetime(2016, 9, 29, 12, 0), 'T1', 1]], columns=[\"id\", \"departure\", \"arrival\", \"number\", \"route_id\"]) Then, we\u2019ll be able to call the to_sql method to convert df to a table in a database. We set the keep_exists parameter to replace to delete and replace any existing tables named daily_flights: df.to_sql(\"daily_flights\", conn, if_exists=\"replace\")","title":"Pandas"},{"location":"notes/python/pandas/basics/#pandas","text":"Some Notes pandas.pydata...","title":"Pandas"},{"location":"notes/python/pandas/basics/#display","text":"pd.set_option('display.max_columns', 50) # How many to show pd.set_option('display.min_rows', 25) # How many to show pd.set_option('display.max_rows', 25) # How many to show pd.set_option('display.width', 1000) # How far across the screen pd.set_option('display.max_colwidth', 1) # Column width in px pd.set_option('display.max_colwidth', 40) # Column width in px pd.set_option('expand_frame_repr', True) # allows for the representation of dataframes to stretch across pages, wrapped over the full column vs row-wise","title":"Display"},{"location":"notes/python/pandas/basics/#making-dataframes","text":"","title":"Making DataFrames"},{"location":"notes/python/pandas/basics/#from-dictionaries","text":"# Method 1: each key is a column with array datatype for values >>> data = {'a':[1,4], 'b':[2,5], 'c':[3,6]} >>> pandas.DataFrame(data) a b c 0 1 2 3 1 4 5 6 # Use 'columns' to specify order >>> pandas.DataFrame(data, columns=['b','c','a']) b c a 0 2 3 1 1 5 6 4 # Method 2: Creating Dataframe from list of dicts >>> data = [{'a': 1, 'b': 2, 'c': 3}, {'a': 10, 'b': 20, 'c': 30}] >>> pandas.DataFrame(data) a b c 0 1 2 3 1 10 20 30","title":"From Dictionaries"},{"location":"notes/python/pandas/basics/#from-lists","text":">>> rows = [[1,2,3],[4,5,6]] >>> pandas.DataFrame(rows, columns=['a','b','c']) a b c 0 1 2 3 1 4 5 6","title":"From lists"},{"location":"notes/python/pandas/basics/#from-excel","text":"import pandas as pd cars = pd.read_excel(r'C:\\Users\\Ron\\Desktop\\Cars.xlsx') df = pd.DataFrame(cars, columns = ['Brand', 'Price'])","title":"From excel"},{"location":"notes/python/pandas/basics/#apply-function","text":"x = pd.DataFrame({'A':[1,1,1,1,2,2,2,2], 'B':[0,0,0,0,0,1,0,1], 'C':['a' for i in range(8)]}) y = pd.DataFrame({'A':[1,1,1,1,1,2,2,2,2,2], 'B':[1,0,1,0,1,0,1,0,1,1], 'C':['a' for i in range(10)]}) z = pd.DataFrame({'A':[1,1,1,1,2,2,2,2], 'B':[2,3,6,9,1,1,6,7], 'C':['a' for i in range(8)]}) def some_agg_function(df_group): # It's important for the values to be sorted to make this easier df_group = df_group.sort_values('B') nrows = len(df_group) if nrows % 2 == 0: middle_index = nrows / 2 - 1; # for 0 based inded purposes; chooses 2 out of (1,[2],3,4) else: middle_index = math.floor(nrows / 2) # pdb.set_trace() return df_group.iloc[int(middle_index),:] print(x); print(x[['A','B']].groupby('A').apply(some_agg_function)) print(y);print(y[['A','B']].groupby('A').apply(some_agg_function)) print(z);print(z[['A','B']].groupby('A').apply(some_agg_function))","title":"Apply Function"},{"location":"notes/python/pandas/basics/#sqlite3-intgration","text":"Source: dataquest","title":"sqlite3 Intgration"},{"location":"notes/python/pandas/basics/#just-sqlite3-library","text":"In order to work with a SQLite database from Python, we first have to connect to it. We can do that using the connect function, which returns a Connection object: import sqlite3 conn = sqlite3.connect(\"flights.db\") Once we have a Connection object, we can then create a Cursor object. Cursors allow us to execute SQL queries against a database: cur = conn.cursor() Once we have a Cursor object, we can use it to execute a query against the database with the aptly named execute method. The below code will fetch the first 5 rows from the airlines table: cur.execute(\"select * from airlines limit 5;\") You may have noticed that we didn\u2019t assign the result of the above query to a variable. This is because we need to run another command to actually fetch the results. We can use the fetchall method to fetch all of the results of a query: results = cur.fetchall() print(results) [(0, '1', 'Private flight', '\\\\N', '-', None, None, None, 'Y'), (1, '2', '135 Airways', '\\\\N', None, 'GNL', 'GENERAL', 'United States', 'N'), (2, '3', '1Time Airline', '\\\\N', '1T', 'RNX', 'NEXTIME', 'South Africa', 'Y'), (3, '4', '2 Sqn No 1 Elementary Flying Training School', '\\\\N', None, 'WYT', None, 'United Kingdom', 'N'), (4, '5', '213 Flight Unit', '\\\\N', None, 'TFU', None, 'Russia', 'N')] As you can see, the results are formatted as a list of tuples. Each tuple corresponds to a row in the database that we accessed. Dealing with data this way is fairly painful. We\u2019d need to manually add column heads, and manually parse the data. Luckily, the pandas library has an easier way, which we\u2019ll look at in the next section. Before we move on, it\u2019s good practice to close Connection objects and Cursor objects that are open. This prevents the SQLite database from being locked. When a SQLite database is locked, you may be unable to update the database, and may get errors. We can close the Cursor and the Connection like this: cur.close() conn.close()","title":"Just sqlite3 library"},{"location":"notes/python/pandas/basics/#add-pandas","text":"import pandas as pd import sqlite3 conn = sqlite3.connect(\"flights.db\") df = pd.read_sql_query(\"select * from airlines limit 5;\", conn)","title":"Add Pandas"},{"location":"notes/python/pandas/basics/#inserting-rows-with-python","text":"To insert a row, we need to write an INSERT query. The below code will add a new row to the airlines table. We specify 9 values to insert, one for each column in airlines. This will add a new row to the table. cur = conn.cursor() cur.execute(\"insert into airlines values (6048, 19846, 'Test flight', '', '', null, null, null, 'Y')\") conn.commit() You want to avoid doing this! Inserting values with Python string formatting makes your program vulnerable to SQL Injection attacks. Luckily, sqlite3 has a straightforward way to inject dynamic values without relying on string formatting: cur = conn.cursor() values = ('Test Flight', 'Y') cur.execute(\"insert into airlines values (6049, 19847, ?, '', '', null, null, null, ?)\", values) conn.commit()","title":"Inserting rows with Python"},{"location":"notes/python/pandas/basics/#updating-rows","text":"We can modify rows in a SQLite table using the execute method: cur = conn.cursor() values = ('USA', 19847) cur.execute(\"update airlines set country=? where id=?\", values) conn.commit()","title":"Updating rows"},{"location":"notes/python/pandas/basics/#deleting-rows","text":"Finally, we can delete the rows in a database using the execute method: cur = conn.cursor()values = (19847, ) cur.execute(\"delete from airlines where id=?\", values)conn.commit()","title":"Deleting rows"},{"location":"notes/python/pandas/basics/#creating-tables-with-pandas","text":"The pandas package gives us a much faster way to create tables. We just have to create a DataFrame first, then export it to a SQL table. First, we\u2019ll create a DataFrame: from datetime import datetime df = pd.DataFrame( [[1, datetime(2016, 9, 29, 0, 0) , datetime(2016, 9, 29, 12, 0), 'T1', 1]], columns=[\"id\", \"departure\", \"arrival\", \"number\", \"route_id\"]) Then, we\u2019ll be able to call the to_sql method to convert df to a table in a database. We set the keep_exists parameter to replace to delete and replace any existing tables named daily_flights: df.to_sql(\"daily_flights\", conn, if_exists=\"replace\")","title":"Creating tables with pandas"},{"location":"notes/python/pysaml2/saml2/","text":"Saml2 in Python Source readthedocs","title":"Saml2 in Python"},{"location":"notes/python/pysaml2/saml2/#saml2-in-python","text":"Source readthedocs","title":"Saml2 in Python"},{"location":"notes/python/rabbitmq/hello_world/","text":"Hello World Source: rabbitmq.com RabbitMQ libraries RabbitMQ speaks multiple protocols. This tutorial uses AMQP 0-9-1, which is an open, general-purpose protocol for messaging. There are a number of clients for RabbitMQ in many different languages. In this tutorial series we're going to use Pika 1.0.0, which is the Python client recommended by the RabbitMQ team. To install it you can use the pip package management tool: python -m pip install pika --upgrade Our first program send.py will send a single message to the queue. The first thing we need to do is to establish a connection with RabbitMQ server. send.py: #!/usr/bin/env python import pika connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() # Next, before sending we need to make sure the recipient queue exists channel.queue_declare(queue='hello') # In RabbitMQ a message can never be sent directly to the queue, it always needs to go through an exchange. # The queue name needs to be specified in the routing_key parameter: channel.basic_publish(exchange='', routing_key='hello', body='Hello World!') print(\" [x] Sent 'Hello World!'\") # Before exiting the program we need to make sure the # network buffers were flushed and our message was actually delivered to RabbitMQ. # We can do it by gently closing the connection. connection.close() receive.py: #!/usr/bin/env python import pika connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() # The next step, just like before, is to make sure that the queue exists. # Creating a queue using queue_declare is idempotent \u2012 # - we can run the command as many times as we like, and only one will be created. channel.queue_declare(queue='hello') # Receiving messages from the queue is more complex. # It works by subscribing a callback function to a queue. # Whenever we receive a message, this callback function is called by # the Pika library. In our case this function will print on the screen the contents of the message. def callback(ch, method, properties, body): print(\" [x] Received %r\" % body) channel.basic_consume(queue='hello', auto_ack=True, on_message_callback=callback) # And finally, we enter a never-ending loop that waits for data and runs callbacks whenever necessary. print(' [*] Waiting for messages. To exit press CTRL+C') channel.start_consuming()","title":"Hellow World"},{"location":"notes/python/rabbitmq/hello_world/#hello-world","text":"Source: rabbitmq.com RabbitMQ libraries RabbitMQ speaks multiple protocols. This tutorial uses AMQP 0-9-1, which is an open, general-purpose protocol for messaging. There are a number of clients for RabbitMQ in many different languages. In this tutorial series we're going to use Pika 1.0.0, which is the Python client recommended by the RabbitMQ team. To install it you can use the pip package management tool: python -m pip install pika --upgrade Our first program send.py will send a single message to the queue. The first thing we need to do is to establish a connection with RabbitMQ server. send.py: #!/usr/bin/env python import pika connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() # Next, before sending we need to make sure the recipient queue exists channel.queue_declare(queue='hello') # In RabbitMQ a message can never be sent directly to the queue, it always needs to go through an exchange. # The queue name needs to be specified in the routing_key parameter: channel.basic_publish(exchange='', routing_key='hello', body='Hello World!') print(\" [x] Sent 'Hello World!'\") # Before exiting the program we need to make sure the # network buffers were flushed and our message was actually delivered to RabbitMQ. # We can do it by gently closing the connection. connection.close() receive.py: #!/usr/bin/env python import pika connection = pika.BlockingConnection(pika.ConnectionParameters('localhost')) channel = connection.channel() # The next step, just like before, is to make sure that the queue exists. # Creating a queue using queue_declare is idempotent \u2012 # - we can run the command as many times as we like, and only one will be created. channel.queue_declare(queue='hello') # Receiving messages from the queue is more complex. # It works by subscribing a callback function to a queue. # Whenever we receive a message, this callback function is called by # the Pika library. In our case this function will print on the screen the contents of the message. def callback(ch, method, properties, body): print(\" [x] Received %r\" % body) channel.basic_consume(queue='hello', auto_ack=True, on_message_callback=callback) # And finally, we enter a never-ending loop that waits for data and runs callbacks whenever necessary. print(' [*] Waiting for messages. To exit press CTRL+C') channel.start_consuming()","title":"Hello World"},{"location":"notes/python/seaborn/basics/","text":"Seaborn Source import seaborn as sns Bar Plot Source seaborn sns.set(style=\"whitegrid\") tips = sns.load_dataset(\"tips\") ax = sns.barplot(x=\"day\", y=\"total_bill\", data=tips) ax = sns.barplot(x=\"day\", y=\"total_bill\", hue=\"sex\", data=tips) ax = sns.barplot(x=\"day\", y=\"tip\", data=tips, capsize=.2) ax = sns.barplot(\"size\", y=\"total_bill\", data=tips, palette=\"Blues_d\") Heatmap ax = sns.heatmap(A_V_A) Axes and Plot Options Source drawingfromdata Rotate Axes import seaborn as sns planets = sns.load_dataset(\"planets\") g = sns.factorplot(\"year\", data=planets, aspect=1.5, kind=\"count\", color=\"b\") g.set_xticklabels(rotation=30)","title":"Seaborn"},{"location":"notes/python/seaborn/basics/#seaborn","text":"Source import seaborn as sns","title":"Seaborn"},{"location":"notes/python/seaborn/basics/#bar-plot","text":"Source seaborn sns.set(style=\"whitegrid\") tips = sns.load_dataset(\"tips\") ax = sns.barplot(x=\"day\", y=\"total_bill\", data=tips) ax = sns.barplot(x=\"day\", y=\"total_bill\", hue=\"sex\", data=tips) ax = sns.barplot(x=\"day\", y=\"tip\", data=tips, capsize=.2) ax = sns.barplot(\"size\", y=\"total_bill\", data=tips, palette=\"Blues_d\")","title":"Bar Plot"},{"location":"notes/python/seaborn/basics/#heatmap","text":"ax = sns.heatmap(A_V_A)","title":"Heatmap"},{"location":"notes/python/seaborn/basics/#axes-and-plot-options","text":"Source drawingfromdata","title":"Axes and Plot Options"},{"location":"notes/python/seaborn/basics/#rotate-axes","text":"import seaborn as sns planets = sns.load_dataset(\"planets\") g = sns.factorplot(\"year\", data=planets, aspect=1.5, kind=\"count\", color=\"b\") g.set_xticklabels(rotation=30)","title":"Rotate Axes"},{"location":"notes/python/strings/Strings/","text":"Strings Padding: center(), rjust(), ljust() The center() method returns a string which is padded with the specified character. The rjust()\\ljust() methods return a string which is padded on either side. >>> 'A'.center(1, '*') 'A' >>> 'A'.center(2, '*') 'A*' >>> 'A'.center(3, '*') '*A*' >>> 'A'.rjust(1, '*') 'A' >>> 'A'.rjust(2, '*') '*A' >>> 'A'.rjust(3, '*') '**A' >>> 'A'.ljust(1, '*') 'A' >>> 'A'.ljust(2, '*') 'A*' >>> 'A'.ljust(3, '*') 'A**' str.format() Different common uses of the str.format() function format(value[, format_spec]) First example: >>> \"{}\".format('value') 'value' Multiple values: >>> \"{} {}\".format('value1','value2') 'value1 value2' Reverse order: >>> \"{1} {0}\".format('value1','value2') 'value2 value1' Key Word Arguments >>> print(\"{kwarg} is {0} used for {1}\" .format(\"being\", \"string formatting\", kwarg =\"Some Key Word Argument\")) ... Some Key Word Argument is being used for string formatting We can also use types to further format values: Syntax: {field_name:conversion}.format(value) Note: field_name can be the index (0) or name of key word argument conversion values: s \u2013 strings d \u2013 decimal integers (base-10) f \u2013 floating point display c \u2013 character b \u2013 binary o \u2013 octal x \u2013 hexadecimal with lowercase letters after 9 X \u2013 hexadecimal with uppercase letters after 9 e \u2013 exponent notation Use like this: s - string >>> '{kwarg:s}'.format(kwarg='5') '5' >>> '{kwarg:s}'.format(kwarg=5) Traceback (most recent call last): File \"\", line 1, in ValueError: Unknown format code 's' for object of type 'int' Notice how we got an error for trying to convert an int to a string d - decimal integers >>> # This works print(\"Convert {0} to decimal integer: {0:d}.\".format(100)) # Notice the Error print(\"Convert {0} to decimal integer: {0:d}.\".format(100.0)) ... Convert 100 to decimal integer: 100. >>> ... Traceback (most recent call last): File \"\", line 2, in ValueError: Unknown format code 'd' for object of type 'float' f - floats >>> # Default decimal precision to 0.000001 print(\"Convert {0} to float: {0:f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.123457. >>> # Change decimal precision to 0.01 print(\"Convert {0} to float: {0:.2f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.12. >>> # Change decimal precision to 0.1 print(\"Convert {0} to float: {0:.1f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.1. >>> # Change decimal precision to 0.000000001 print(\"Convert {0} to float: {0:.9f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.123456789. c - single character (accepts integer or single character string). Use this link for a unicode character lookup table Brief sample... NUL SOH STX ETX EOT ENQ ACK BEL BS HT LF VT FF CR SO SI DLE DC1 DC2 DC3 DC4 NAK SYN ETB CAN EM SUB ESC FS GS RS US ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\ ] ^ _ ... #000 >>> ' '.join(['{0:c}'.format(_) for _ in range(16)]) '\\x00 \\x01 \\x02 \\x03 \\x04 \\x05 \\x06 \\x07 \\x08 \\t \\n \\x0b \\x0c \\r \\x0e \\x0f' #001 >>> ' '.join(['{0:c}'.format(_) for _ in range(16,32)]) '\\x10 \\x11 \\x12 \\x13 \\x14 \\x15 \\x16 \\x17 \\x18 \\x19 \\x1a \\x1b \\x1c \\x1d \\x1e \\x1f' #002 >>> ' '.join(['{0:c}'.format(_) for _ in range(32,48)]) ' ! \" # $ % & \\' ( ) * + , - . /' #003 >>> ' '.join(['{0:c}'.format(_) for _ in range(48,64)]) '0 1 2 3 4 5 6 7 8 9 : ; < = > ?' #004 >>> ' '.join(['{0:c}'.format(_) for _ in range(64,80)]) '@ A B C D E F G H I J K L M N O' #005 >>> ' '.join(['{0:c}'.format(_) for _ in range(80,96)]) 'P Q R S T U V W X Y Z [ \\\\ ] ^ _' b - binary >>> print(\"{0:b}\".format(0)) 0 >>> print(\"{0:b}\".format(1)) 1 >>> print(\"{0:b}\".format(2)) 10 >>> print(\"{0:b}\".format(3)) 11 >>> print(\"{0:b}\".format(100)) 1100100 o - octal >>> print(\"{0:o}\".format(0)) 0 >>> print(\"{0:o}\".format(8)) 10 >>> print(\"{0:o}\".format(8*2)) 20 >>> print(\"{0:o}\".format(8*3)) 30 >>> print(\"{0:o}\".format(8*4)) 40 >>> print(\"{0:o}\".format(8 ** 1)) 10 >>> print(\"{0:o}\".format(8 ** 2)) 100 >>> print(\"{0:o}\".format(8 ** 3)) 1000 x\\H - hex\\HEX print(\"{0:x}\".format(0*16)) # 0 print(\"{0:x}\".format(1*16)) # 10 print(\"{0:x}\".format(2*16)) # 20 print(\"{0:x}\".format(16 ** 1)) # 10 print(\"{0:x}\".format(16 ** 2)) # 100 print(\"{0:x}\".format(16 ** 3)) # 1000 print(\"{0:x}\".format(10)) # a print(\"{0:X}\".format(10)) # A print(\"{0:x}\".format(15)) # f print(\"{0:X}\".format(15)) # F e - exponent print(\"{0:e}\".format(10)) # 1.000000e+01 print(\"{0:e}\".format(100)) # 1.000000e+02 print(\"{0:e}\".format(1000)) # 1.000000e+03 str.find() Use a substring and start-end range to identify where a substring resides in a larger string str.find(sub[, start[, end]]) s = 'ABCBCBCBCBC' # str.find() will return the index found or -1. # with only a start value the rest of the string is searched print(s.find('BC', 0)) # >>> 1 print(s.find('BC', 1)) # >>> 1 print(s.find('BC', 2)) # >>> 3 print(s.find('BC', 3)) # >>> 3 print(s.find('BC', 4)) # >>> 5 print(s.find('BC', 5)) # >>> 5 print(s.find('BC', 6)) # >>> 7 print(s.find('BC', 7)) # >>> 7 print(s.find('BC', 8)) # >>> 9 print(s.find('BC', 9)) # >>> 9 print(s.find('BC', 10)) # >>> -1 # with start and end, the substring must be within the range print(s.find('BC', 0,1)) # >>> -1 print(s.find('BC', 0,2)) # >>> -1 print(s.find('BC', 0,3)) # >>> 1","title":"Strings"},{"location":"notes/python/strings/Strings/#strings","text":"","title":"Strings"},{"location":"notes/python/strings/Strings/#padding-center-rjust-ljust","text":"The center() method returns a string which is padded with the specified character. The rjust()\\ljust() methods return a string which is padded on either side. >>> 'A'.center(1, '*') 'A' >>> 'A'.center(2, '*') 'A*' >>> 'A'.center(3, '*') '*A*' >>> 'A'.rjust(1, '*') 'A' >>> 'A'.rjust(2, '*') '*A' >>> 'A'.rjust(3, '*') '**A' >>> 'A'.ljust(1, '*') 'A' >>> 'A'.ljust(2, '*') 'A*' >>> 'A'.ljust(3, '*') 'A**'","title":"Padding: center(), rjust(), ljust()"},{"location":"notes/python/strings/Strings/#strformat","text":"Different common uses of the str.format() function format(value[, format_spec]) First example: >>> \"{}\".format('value') 'value' Multiple values: >>> \"{} {}\".format('value1','value2') 'value1 value2' Reverse order: >>> \"{1} {0}\".format('value1','value2') 'value2 value1' Key Word Arguments >>> print(\"{kwarg} is {0} used for {1}\" .format(\"being\", \"string formatting\", kwarg =\"Some Key Word Argument\")) ... Some Key Word Argument is being used for string formatting We can also use types to further format values: Syntax: {field_name:conversion}.format(value) Note: field_name can be the index (0) or name of key word argument conversion values: s \u2013 strings d \u2013 decimal integers (base-10) f \u2013 floating point display c \u2013 character b \u2013 binary o \u2013 octal x \u2013 hexadecimal with lowercase letters after 9 X \u2013 hexadecimal with uppercase letters after 9 e \u2013 exponent notation Use like this: s - string >>> '{kwarg:s}'.format(kwarg='5') '5' >>> '{kwarg:s}'.format(kwarg=5) Traceback (most recent call last): File \"\", line 1, in ValueError: Unknown format code 's' for object of type 'int' Notice how we got an error for trying to convert an int to a string d - decimal integers >>> # This works print(\"Convert {0} to decimal integer: {0:d}.\".format(100)) # Notice the Error print(\"Convert {0} to decimal integer: {0:d}.\".format(100.0)) ... Convert 100 to decimal integer: 100. >>> ... Traceback (most recent call last): File \"\", line 2, in ValueError: Unknown format code 'd' for object of type 'float' f - floats >>> # Default decimal precision to 0.000001 print(\"Convert {0} to float: {0:f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.123457. >>> # Change decimal precision to 0.01 print(\"Convert {0} to float: {0:.2f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.12. >>> # Change decimal precision to 0.1 print(\"Convert {0} to float: {0:.1f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.1. >>> # Change decimal precision to 0.000000001 print(\"Convert {0} to float: {0:.9f}.\".format(100.123456789)) ... Convert 100.123456789 to float: 100.123456789. c - single character (accepts integer or single character string). Use this link for a unicode character lookup table Brief sample... NUL SOH STX ETX EOT ENQ ACK BEL BS HT LF VT FF CR SO SI DLE DC1 DC2 DC3 DC4 NAK SYN ETB CAN EM SUB ESC FS GS RS US ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\ ] ^ _ ... #000 >>> ' '.join(['{0:c}'.format(_) for _ in range(16)]) '\\x00 \\x01 \\x02 \\x03 \\x04 \\x05 \\x06 \\x07 \\x08 \\t \\n \\x0b \\x0c \\r \\x0e \\x0f' #001 >>> ' '.join(['{0:c}'.format(_) for _ in range(16,32)]) '\\x10 \\x11 \\x12 \\x13 \\x14 \\x15 \\x16 \\x17 \\x18 \\x19 \\x1a \\x1b \\x1c \\x1d \\x1e \\x1f' #002 >>> ' '.join(['{0:c}'.format(_) for _ in range(32,48)]) ' ! \" # $ % & \\' ( ) * + , - . /' #003 >>> ' '.join(['{0:c}'.format(_) for _ in range(48,64)]) '0 1 2 3 4 5 6 7 8 9 : ; < = > ?' #004 >>> ' '.join(['{0:c}'.format(_) for _ in range(64,80)]) '@ A B C D E F G H I J K L M N O' #005 >>> ' '.join(['{0:c}'.format(_) for _ in range(80,96)]) 'P Q R S T U V W X Y Z [ \\\\ ] ^ _' b - binary >>> print(\"{0:b}\".format(0)) 0 >>> print(\"{0:b}\".format(1)) 1 >>> print(\"{0:b}\".format(2)) 10 >>> print(\"{0:b}\".format(3)) 11 >>> print(\"{0:b}\".format(100)) 1100100 o - octal >>> print(\"{0:o}\".format(0)) 0 >>> print(\"{0:o}\".format(8)) 10 >>> print(\"{0:o}\".format(8*2)) 20 >>> print(\"{0:o}\".format(8*3)) 30 >>> print(\"{0:o}\".format(8*4)) 40 >>> print(\"{0:o}\".format(8 ** 1)) 10 >>> print(\"{0:o}\".format(8 ** 2)) 100 >>> print(\"{0:o}\".format(8 ** 3)) 1000 x\\H - hex\\HEX print(\"{0:x}\".format(0*16)) # 0 print(\"{0:x}\".format(1*16)) # 10 print(\"{0:x}\".format(2*16)) # 20 print(\"{0:x}\".format(16 ** 1)) # 10 print(\"{0:x}\".format(16 ** 2)) # 100 print(\"{0:x}\".format(16 ** 3)) # 1000 print(\"{0:x}\".format(10)) # a print(\"{0:X}\".format(10)) # A print(\"{0:x}\".format(15)) # f print(\"{0:X}\".format(15)) # F e - exponent print(\"{0:e}\".format(10)) # 1.000000e+01 print(\"{0:e}\".format(100)) # 1.000000e+02 print(\"{0:e}\".format(1000)) # 1.000000e+03","title":"str.format()"},{"location":"notes/python/strings/Strings/#strfind","text":"Use a substring and start-end range to identify where a substring resides in a larger string str.find(sub[, start[, end]]) s = 'ABCBCBCBCBC' # str.find() will return the index found or -1. # with only a start value the rest of the string is searched print(s.find('BC', 0)) # >>> 1 print(s.find('BC', 1)) # >>> 1 print(s.find('BC', 2)) # >>> 3 print(s.find('BC', 3)) # >>> 3 print(s.find('BC', 4)) # >>> 5 print(s.find('BC', 5)) # >>> 5 print(s.find('BC', 6)) # >>> 7 print(s.find('BC', 7)) # >>> 7 print(s.find('BC', 8)) # >>> 9 print(s.find('BC', 9)) # >>> 9 print(s.find('BC', 10)) # >>> -1 # with start and end, the substring must be within the range print(s.find('BC', 0,1)) # >>> -1 print(s.find('BC', 0,2)) # >>> -1 print(s.find('BC', 0,3)) # >>> 1","title":"str.find()"},{"location":"notes/python/tinydb/basics/","text":"TinyDB This is great small json db, an amazing python package, for light json databases. There is an equivalent nodejs one too. python - jsondb tinydb Installing TinyDB To install TinyDB from PyPI, run: $ pip install tinydb You can also grab the latest development version from GitHub . After downloading and unpacking it, you can install it using: $ python setup.py install Basic Usage Let\u2019s cover the basics before going more into detail. We\u2019ll start by setting up a TinyDB database: >>> from tinydb import TinyDB, Query >>> db = TinyDB('db.json') You now have a TinyDB database that stores its data in db.json. What about inserting some data? TinyDB expects the data to be Python dicts: >>> db.insert({'type': 'apple', 'count': 7}) >>> db.insert({'type': 'peach', 'count': 3}) Note The insert method returns the inserted document\u2019s ID. Read more about it here: Using Document IDs . Now you can get all documents stored in the database by running: >>> db.all() [{'count': 7, 'type': 'apple'}, {'count': 3, 'type': 'peach'}] You can also iter over stored documents: >>> for item in db: >>> print(item) {'count': 7, 'type': 'apple'} {'count': 3, 'type': 'peach'} Of course you\u2019ll also want to search for specific documents. Let\u2019s try: >>> Fruit = Query() >>> db.search(Fruit.type == 'peach') [{'count': 3, 'type': 'peach'}] >>> db.search(Fruit.count > 5) [{'count': 7, 'type': 'apple'}] Next we\u2019ll update the count field of the apples: >>> db.update({'count': 10}, Fruit.type == 'apple') >>> db.all() [{'count': 10, 'type': 'apple'}, {'count': 3, 'type': 'peach'}] In the same manner you can also remove documents: >>> db.remove(Fruit.count < 5) >>> db.all() [{'count': 10, 'type': 'apple'}] And of course you can throw away all data to start with an empty database: >>> db.purge() Advanced Usage Source readthedocs - advanced tinydb nodejs - jsondb simple-json-db A simple, no-frills, JSON storage engine for Node.JS with full test coverage. Installation (bash) npm install --save simple-json-db Usage (javascript) Instantiation const JSONdb = require('simple-json-db'); const db = new JSONdb('/path/to/your/database.json'); The prototype of the constructor is new JSONdb(string, [object]) , and you can supply the optional options object by giving it as second parameter: const db = new JSONdb('/path/to/your/database.json', { ... }); See the Options section for more details. Options Key Value Description Default Value asyncWrite Boolean Enables the storage to be asynchronously written to disk. false (synchronous behaviour) syncOnWrite Boolean Makes the storage be written to disk after every modification. true Set a key db.set('key', 'value'); The key parameter must be a string, value can be whatever kind of object can be stored in JSON format. JSON.stringify() is your friend! Get a key db.get('key'); The key parameter must be a string. If the key exists its value is returned, if it doesn't the function returns undefined . Check a key db.has('key'); The key parameter must be a string. If the key exhists true is returned, if it doesn't the function returns false . Delete a key db.delete('key'); The key parameter must be a string. The function returns as per the delete operator if the key exhists, else it returns undefined . Sync to disk db.sync(); This function writes the JSON storage object to the file path specified as the parameter of the main constructor. Consult the Options section for usage details; on default options there is no need to manually invoke it. Access JSON storage db.JSON(); This will return a copy of the internal JSON storage object, for you to tinker with and loop over. Replace JSON storage db.JSON({ data }); Giving a parameter to the JSON function makes the object passed replace the internal one. Be careful, as there's no way to recover the old object if the changes have already been written to disk .","title":"TinyDB"},{"location":"notes/python/tinydb/basics/#tinydb","text":"This is great small json db, an amazing python package, for light json databases. There is an equivalent nodejs one too.","title":"TinyDB"},{"location":"notes/python/tinydb/basics/#python-jsondb","text":"tinydb","title":"python - jsondb"},{"location":"notes/python/tinydb/basics/#installing-tinydb","text":"To install TinyDB from PyPI, run: $ pip install tinydb You can also grab the latest development version from GitHub . After downloading and unpacking it, you can install it using: $ python setup.py install","title":"Installing TinyDB"},{"location":"notes/python/tinydb/basics/#basic-usage","text":"Let\u2019s cover the basics before going more into detail. We\u2019ll start by setting up a TinyDB database: >>> from tinydb import TinyDB, Query >>> db = TinyDB('db.json') You now have a TinyDB database that stores its data in db.json. What about inserting some data? TinyDB expects the data to be Python dicts: >>> db.insert({'type': 'apple', 'count': 7}) >>> db.insert({'type': 'peach', 'count': 3}) Note The insert method returns the inserted document\u2019s ID. Read more about it here: Using Document IDs . Now you can get all documents stored in the database by running: >>> db.all() [{'count': 7, 'type': 'apple'}, {'count': 3, 'type': 'peach'}] You can also iter over stored documents: >>> for item in db: >>> print(item) {'count': 7, 'type': 'apple'} {'count': 3, 'type': 'peach'} Of course you\u2019ll also want to search for specific documents. Let\u2019s try: >>> Fruit = Query() >>> db.search(Fruit.type == 'peach') [{'count': 3, 'type': 'peach'}] >>> db.search(Fruit.count > 5) [{'count': 7, 'type': 'apple'}] Next we\u2019ll update the count field of the apples: >>> db.update({'count': 10}, Fruit.type == 'apple') >>> db.all() [{'count': 10, 'type': 'apple'}, {'count': 3, 'type': 'peach'}] In the same manner you can also remove documents: >>> db.remove(Fruit.count < 5) >>> db.all() [{'count': 10, 'type': 'apple'}] And of course you can throw away all data to start with an empty database: >>> db.purge()","title":"Basic Usage"},{"location":"notes/python/tinydb/basics/#advanced-usage","text":"Source readthedocs - advanced tinydb","title":"Advanced Usage"},{"location":"notes/python/tinydb/basics/#nodejs-jsondb","text":"simple-json-db A simple, no-frills, JSON storage engine for Node.JS with full test coverage.","title":"nodejs - jsondb"},{"location":"notes/python/tinydb/basics/#installation-bash","text":"npm install --save simple-json-db","title":"Installation (bash)"},{"location":"notes/python/tinydb/basics/#usage-javascript","text":"Instantiation const JSONdb = require('simple-json-db'); const db = new JSONdb('/path/to/your/database.json'); The prototype of the constructor is new JSONdb(string, [object]) , and you can supply the optional options object by giving it as second parameter: const db = new JSONdb('/path/to/your/database.json', { ... }); See the Options section for more details. Options Key Value Description Default Value asyncWrite Boolean Enables the storage to be asynchronously written to disk. false (synchronous behaviour) syncOnWrite Boolean Makes the storage be written to disk after every modification. true Set a key db.set('key', 'value'); The key parameter must be a string, value can be whatever kind of object can be stored in JSON format. JSON.stringify() is your friend! Get a key db.get('key'); The key parameter must be a string. If the key exists its value is returned, if it doesn't the function returns undefined . Check a key db.has('key'); The key parameter must be a string. If the key exhists true is returned, if it doesn't the function returns false . Delete a key db.delete('key'); The key parameter must be a string. The function returns as per the delete operator if the key exhists, else it returns undefined . Sync to disk db.sync(); This function writes the JSON storage object to the file path specified as the parameter of the main constructor. Consult the Options section for usage details; on default options there is no need to manually invoke it. Access JSON storage db.JSON(); This will return a copy of the internal JSON storage object, for you to tinker with and loop over. Replace JSON storage db.JSON({ data }); Giving a parameter to the JSON function makes the object passed replace the internal one. Be careful, as there's no way to recover the old object if the changes have already been written to disk .","title":"Usage (javascript)"},{"location":"notes/python/useful/Args%20and%20Kwargs/","text":"Args and Kwargs Passing unknown amounts of inputs to a function *args - any number of inputs of any data type. They will be referenced in order with indicies being of the form args[0]...etc. **kwargs - The same as above but with key work arguments, so you would be able to access elements by key name. kwargs The * is important as it signifies if args or kwargs are being used. Otherwise this happens: >>> def prac(*args): ... for i in args: ... print(\"This arg is :{}\".format(i)) ... >>> prac([1,2,3,4,5]) This arg is :[1, 2, 3, 4, 5] # ooooops! >>> prac(*[1,2,3,4,5]) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> def prac(**kwargs): ... for k in kwargs.keys(): ... print(kwargs[k]) ... >>> prac(**{'A':1,'B':2}) 1 2 Also note iterables are acceptible inputs: >>> prac(*[1,2,3,4,5]) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*(1,2,3,4,5)) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*{1,2,3,4,5}) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*range(1,6)) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 Below are additional examples def f(*args,**kwargs): print(args, kwargs) l = [1,2,3] t = (4,5,6) d = {'a':7,'b':8,'c':9} f() # () {} f(1,2,3) # (1, 2, 3) {} f(1,2,3,\"groovy\") # (1, 2, 3, 'groovy') {} f(a=1,b=2,c=3) # () {'a': 1, 'c': 3, 'b': 2} f(a=1,b=2,c=3,zzz=\"hi\") # () {'a': 1, 'c': 3, 'b': 2, 'zzz': 'hi'} f(1,2,3,a=1,b=2,c=3) # (1, 2, 3) {'a': 1, 'c': 3, 'b': 2} f(*l,**d) # (1, 2, 3) {'a': 7, 'c': 9, 'b': 8} f(*t,**d) # (4, 5, 6) {'a': 7, 'c': 9, 'b': 8} f(1,2,*t) # (1, 2, 4, 5, 6) {} f(q=\"winning\",**d) # () {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} f(1,2,*t,q=\"winning\",**d) # (1, 2, 4, 5, 6) {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} def f2(arg1,arg2,*args,**kwargs): print(arg1,arg2, args, kwargs) f2(1,2,3) # 1 2 (3,) {} f2(1,2,3,\"groovy\") # 1 2 (3, 'groovy') {} f2(arg1=1,arg2=2,c=3) # 1 2 () {'c': 3} f2(arg1=1,arg2=2,c=3,zzz=\"hi\") # 1 2 () {'c': 3, 'zzz': 'hi'} f2(1,2,3,a=1,b=2,c=3) # 1 2 (3,) {'a': 1, 'c': 3, 'b': 2} f2(*l,**d) # 1 2 (3,) {'a': 7, 'c': 9, 'b': 8} f2(*t,**d) # 4 5 (6,) {'a': 7, 'c': 9, 'b': 8} f2(1,2,*t) # 1 2 (4, 5, 6) {} f2(1,1,q=\"winning\",**d) # 1 1 () {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} f2(1,2,*t,q=\"winning\",**d) # 1 2 (4, 5, 6) {'a': 7, 'q': 'winning', 'c': 9, 'b': 8}","title":"args, kwargs"},{"location":"notes/python/useful/Args%20and%20Kwargs/#args-and-kwargs","text":"Passing unknown amounts of inputs to a function *args - any number of inputs of any data type. They will be referenced in order with indicies being of the form args[0]...etc. **kwargs - The same as above but with key work arguments, so you would be able to access elements by key name. kwargs The * is important as it signifies if args or kwargs are being used. Otherwise this happens: >>> def prac(*args): ... for i in args: ... print(\"This arg is :{}\".format(i)) ... >>> prac([1,2,3,4,5]) This arg is :[1, 2, 3, 4, 5] # ooooops! >>> prac(*[1,2,3,4,5]) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> def prac(**kwargs): ... for k in kwargs.keys(): ... print(kwargs[k]) ... >>> prac(**{'A':1,'B':2}) 1 2 Also note iterables are acceptible inputs: >>> prac(*[1,2,3,4,5]) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*(1,2,3,4,5)) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*{1,2,3,4,5}) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 >>> prac(*range(1,6)) This arg is :1 This arg is :2 This arg is :3 This arg is :4 This arg is :5 Below are additional examples def f(*args,**kwargs): print(args, kwargs) l = [1,2,3] t = (4,5,6) d = {'a':7,'b':8,'c':9} f() # () {} f(1,2,3) # (1, 2, 3) {} f(1,2,3,\"groovy\") # (1, 2, 3, 'groovy') {} f(a=1,b=2,c=3) # () {'a': 1, 'c': 3, 'b': 2} f(a=1,b=2,c=3,zzz=\"hi\") # () {'a': 1, 'c': 3, 'b': 2, 'zzz': 'hi'} f(1,2,3,a=1,b=2,c=3) # (1, 2, 3) {'a': 1, 'c': 3, 'b': 2} f(*l,**d) # (1, 2, 3) {'a': 7, 'c': 9, 'b': 8} f(*t,**d) # (4, 5, 6) {'a': 7, 'c': 9, 'b': 8} f(1,2,*t) # (1, 2, 4, 5, 6) {} f(q=\"winning\",**d) # () {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} f(1,2,*t,q=\"winning\",**d) # (1, 2, 4, 5, 6) {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} def f2(arg1,arg2,*args,**kwargs): print(arg1,arg2, args, kwargs) f2(1,2,3) # 1 2 (3,) {} f2(1,2,3,\"groovy\") # 1 2 (3, 'groovy') {} f2(arg1=1,arg2=2,c=3) # 1 2 () {'c': 3} f2(arg1=1,arg2=2,c=3,zzz=\"hi\") # 1 2 () {'c': 3, 'zzz': 'hi'} f2(1,2,3,a=1,b=2,c=3) # 1 2 (3,) {'a': 1, 'c': 3, 'b': 2} f2(*l,**d) # 1 2 (3,) {'a': 7, 'c': 9, 'b': 8} f2(*t,**d) # 4 5 (6,) {'a': 7, 'c': 9, 'b': 8} f2(1,2,*t) # 1 2 (4, 5, 6) {} f2(1,1,q=\"winning\",**d) # 1 1 () {'a': 7, 'q': 'winning', 'c': 9, 'b': 8} f2(1,2,*t,q=\"winning\",**d) # 1 2 (4, 5, 6) {'a': 7, 'q': 'winning', 'c': 9, 'b': 8}","title":"Args and Kwargs"},{"location":"notes/python/useful/Input/","text":"Input Here we show how to accept user input print('Enter your name:') x = input() print('Hello, ' + x) >>> Enter your name: Ben Hello, Ben","title":"input()"},{"location":"notes/python/useful/Input/#input","text":"Here we show how to accept user input print('Enter your name:') x = input() print('Hello, ' + x) >>> Enter your name: Ben Hello, Ben","title":"Input"},{"location":"notes/python/useful/Map/","text":"Map map() allows us to apply a function to a list of items # Simple Example >>> list(map(float, ['1.0', '2.0'])) [1.0, 2.0] # Turn this: items = [1, 2, 3, 4, 5] squared = [] for i in items: squared.append(i**2) # Into this with a lambda function: items = [1, 2, 3, 4, 5] squared = list(map(lambda x: x**2, items))","title":"map()"},{"location":"notes/python/useful/Map/#map","text":"map() allows us to apply a function to a list of items # Simple Example >>> list(map(float, ['1.0', '2.0'])) [1.0, 2.0] # Turn this: items = [1, 2, 3, 4, 5] squared = [] for i in items: squared.append(i**2) # Into this with a lambda function: items = [1, 2, 3, 4, 5] squared = list(map(lambda x: x**2, items))","title":"Map"},{"location":"notes/python/useful/Pass%20By%20Reference/","text":"Pass By Reference Python will pass values between objects as references to their memory position. Addresses not values are passed...except for primitives.. An example: def f(x,l=[]): for i in range(x): l.append(i*i) print(l) f(2) # [0, 1] f(3,[3,2,1]) # [3, 2, 1, 0, 1, 4] Here python treats the variable l as a fresh list because a fresh list or more accurately, it's location in memory was passed. See python created [3, 2, 1] separately and passed it's location to the function. Watch what happends when we call the function again with no list. f(3) # [0, 1, 0, 1, 4] why not [0,1,4]?. When the function was first called we created the default list \"l=[]\", and according to the function's directions, by default this function refers to that list's memory position. Check out the following examples of pythons memory usage for this example: l_mem = [] l = l_mem # the first call for i in range(2): l.append(i*i) print(l) # [0, 1] l = [3,2,1] # the second call for i in range(3): l.append(i*i) print(l) # [3, 2, 1, 0, 1, 4] l = l_mem # the third call for i in range(3): l.append(i*i) print(l) # [0, 1, 0, 1, 4]","title":"P.B. Reference"},{"location":"notes/python/useful/Pass%20By%20Reference/#pass-by-reference","text":"Python will pass values between objects as references to their memory position. Addresses not values are passed...except for primitives.. An example: def f(x,l=[]): for i in range(x): l.append(i*i) print(l) f(2) # [0, 1] f(3,[3,2,1]) # [3, 2, 1, 0, 1, 4] Here python treats the variable l as a fresh list because a fresh list or more accurately, it's location in memory was passed. See python created [3, 2, 1] separately and passed it's location to the function. Watch what happends when we call the function again with no list. f(3) # [0, 1, 0, 1, 4] why not [0,1,4]?. When the function was first called we created the default list \"l=[]\", and according to the function's directions, by default this function refers to that list's memory position. Check out the following examples of pythons memory usage for this example: l_mem = [] l = l_mem # the first call for i in range(2): l.append(i*i) print(l) # [0, 1] l = [3,2,1] # the second call for i in range(3): l.append(i*i) print(l) # [3, 2, 1, 0, 1, 4] l = l_mem # the third call for i in range(3): l.append(i*i) print(l) # [0, 1, 0, 1, 4]","title":"Pass By Reference"},{"location":"notes/python/useful/Sets/","text":"Sets Sets In Python A Set is an unordered collection data type that is iterable, mutable, and has no duplicate elements. Python\u2019s set class represents the mathematical notion of a set. The major advantage of using a set, as opposed to a list, is that it has a highly optimized method for checking whether a specific element is contained in the set. This is based on a data structure known as a hash table. Methods: add() add(x) Method: Adds the item x to set if it is not already present in the set. >>> s3.add(9) >>> s3 {9, 3} 2. union(s) Method: Returns a union of two set.Using the \u2018|\u2019 operator between 2 sets is the same as writing set1.union(set2) union() >>> s1.union(s2) {1, 2, 3, 4, 5} >>> s1 | s2 {1, 2, 3, 4, 5} 3. intersect(s) Method: Returns an intersection of two sets.The \u2018&\u2019 operator comes can also be used in this case. intersection() >>> s1.intersection(s2) {3} >>> s1 & s2 {3} difference difference(s) Method: Returns a set containing all the elements of invoking set but not of the second set. We can use \u2018-\u2018 operator here. >>> s1.difference(s2) {1, 2} >>> s1 - s2 {1, 2} 5. clear() Method: Empties the whole set. >>> s1.clear() >>> s2.clear() >>> s1 set() >>> s2 set() discard() discard() Method: The discard() method takes a single element x and removes it from the set (if present). >>> s1 {1, 2, 3} >>> s1.discard(1) >>> s1 {2, 3} issubset() issubset() Method: The issubset() method returns True if all elements of a set are present in another set (passed as an argument). If not, it returns False. >>> s1 {1, 2, 3} >>> s1.issubset({1,2,3,4}) True Operators for Sets Sets and frozen sets support the following operators: >>> k = 1 >>> key in s1 # containment check True >>> key not in s1 # non-containment check False >>> s1 == s2 # s1 is equivalent to s2 False >>> s1 != s2 # s1 is not equivalent to s2 True >>> s1 <= s2 # s1 is subset of s2 False >>> s1 < s2 # s1 is proper subset of s2 False >>> s1 >= s2 # s1 is superset of s2 False >>> s1 > s2 # s1 is proper superset of s2 False >>> s1 | s2 # the union of s1 and s2 {1, 2, 3, 4, 5} >>> s1 & s2 # the intersection of s1 and s2 {3} >>> s1 - s2 # the set of elements in s1 but not s2 {1, 2} >>> s1 ^ s2 # the set of elements in precisely one of s1 or s2 {1, 2, 4, 5}","title":"Sets"},{"location":"notes/python/useful/Sets/#sets","text":"Sets In Python A Set is an unordered collection data type that is iterable, mutable, and has no duplicate elements. Python\u2019s set class represents the mathematical notion of a set. The major advantage of using a set, as opposed to a list, is that it has a highly optimized method for checking whether a specific element is contained in the set. This is based on a data structure known as a hash table. Methods:","title":"Sets"},{"location":"notes/python/useful/Sets/#add","text":"add(x) Method: Adds the item x to set if it is not already present in the set. >>> s3.add(9) >>> s3 {9, 3} 2. union(s) Method: Returns a union of two set.Using the \u2018|\u2019 operator between 2 sets is the same as writing set1.union(set2)","title":"add()"},{"location":"notes/python/useful/Sets/#union","text":">>> s1.union(s2) {1, 2, 3, 4, 5} >>> s1 | s2 {1, 2, 3, 4, 5} 3. intersect(s) Method: Returns an intersection of two sets.The \u2018&\u2019 operator comes can also be used in this case.","title":"union()"},{"location":"notes/python/useful/Sets/#intersection","text":">>> s1.intersection(s2) {3} >>> s1 & s2 {3}","title":"intersection()"},{"location":"notes/python/useful/Sets/#difference","text":"difference(s) Method: Returns a set containing all the elements of invoking set but not of the second set. We can use \u2018-\u2018 operator here. >>> s1.difference(s2) {1, 2} >>> s1 - s2 {1, 2} 5. clear() Method: Empties the whole set. >>> s1.clear() >>> s2.clear() >>> s1 set() >>> s2 set()","title":"difference"},{"location":"notes/python/useful/Sets/#discard","text":"discard() Method: The discard() method takes a single element x and removes it from the set (if present). >>> s1 {1, 2, 3} >>> s1.discard(1) >>> s1 {2, 3}","title":"discard()"},{"location":"notes/python/useful/Sets/#issubset","text":"issubset() Method: The issubset() method returns True if all elements of a set are present in another set (passed as an argument). If not, it returns False. >>> s1 {1, 2, 3} >>> s1.issubset({1,2,3,4}) True Operators for Sets Sets and frozen sets support the following operators: >>> k = 1 >>> key in s1 # containment check True >>> key not in s1 # non-containment check False >>> s1 == s2 # s1 is equivalent to s2 False >>> s1 != s2 # s1 is not equivalent to s2 True >>> s1 <= s2 # s1 is subset of s2 False >>> s1 < s2 # s1 is proper subset of s2 False >>> s1 >= s2 # s1 is superset of s2 False >>> s1 > s2 # s1 is proper superset of s2 False >>> s1 | s2 # the union of s1 and s2 {1, 2, 3, 4, 5} >>> s1 & s2 # the intersection of s1 and s2 {3} >>> s1 - s2 # the set of elements in s1 but not s2 {1, 2} >>> s1 ^ s2 # the set of elements in precisely one of s1 or s2 {1, 2, 4, 5}","title":"issubset()"},{"location":"notes/python/useful/Sorted/","text":"Sorted Sort Arrays Want to sort something, sorted() is a great start. Syntax: sorted(iterable, key, reverse) Parameters: sorted takes three parameters from which two are optional. Iterable: sequence (list, tuple, string) or collection (dictionary, set, frozenset) or any other iterator that needs to be sorted Key(optional) : A function that would server as a key or a basis of sort comparison Reverse(optional) : If set true, then the iterable would be sorted in reverse (descending) order, by default it is set as false Note: A list also has sort() method which performs the same way as sorted(). Only difference being, sort() method doesn't return any value and changes the original list itself. x = [2,44,3,87,5] print(x) # [2, 44, 3, 87, 5] print(sorted(x)) #[2, 3, 5, 44, 87] print(sorted(x, reverse=True)) # [87, 44, 5, 3, 2] Custom Sorting using the key parameter: sorted() function has an optional parameter called \u2018key\u2019 which takes a function as its value. This key function transforms each element before sorting, it takes the value and returns 1 value which is then used within sort instead of the original value. For example, if we pass a list of strings in sorted(), it gets sorted alphabetically . But if we specify key = len, i.e. give len function as key, then the strings would be passed to len, and the value it returns, i.e. the length of strings will be sorted. Which means that the strings would be sorted based on their lengths instead # sort by your own criteria L = [\"cccc\", \"b\", \"dd\", \"aaa\"] print(\"Normal sort :\", sorted(L)) print(\"Sort with len :\", sorted(L, key = len))","title":"sorted()"},{"location":"notes/python/useful/Sorted/#sorted","text":"Sort Arrays Want to sort something, sorted() is a great start. Syntax: sorted(iterable, key, reverse) Parameters: sorted takes three parameters from which two are optional. Iterable: sequence (list, tuple, string) or collection (dictionary, set, frozenset) or any other iterator that needs to be sorted Key(optional) : A function that would server as a key or a basis of sort comparison Reverse(optional) : If set true, then the iterable would be sorted in reverse (descending) order, by default it is set as false Note: A list also has sort() method which performs the same way as sorted(). Only difference being, sort() method doesn't return any value and changes the original list itself. x = [2,44,3,87,5] print(x) # [2, 44, 3, 87, 5] print(sorted(x)) #[2, 3, 5, 44, 87] print(sorted(x, reverse=True)) # [87, 44, 5, 3, 2] Custom Sorting using the key parameter: sorted() function has an optional parameter called \u2018key\u2019 which takes a function as its value. This key function transforms each element before sorting, it takes the value and returns 1 value which is then used within sort instead of the original value. For example, if we pass a list of strings in sorted(), it gets sorted alphabetically . But if we specify key = len, i.e. give len function as key, then the strings would be passed to len, and the value it returns, i.e. the length of strings will be sorted. Which means that the strings would be sorted based on their lengths instead # sort by your own criteria L = [\"cccc\", \"b\", \"dd\", \"aaa\"] print(\"Normal sort :\", sorted(L)) print(\"Sort with len :\", sorted(L, key = len))","title":"Sorted"},{"location":"notes/python/useful/Zip/","text":"Zip Combine iterables and return them as tuple sets. The zip() function returns an iterator of tuples based on the iterable object. If no parameters are passed, zip() returns an empty iterator If a single iterable is passed, zip() returns an iterator of 1-tuples. Meaning, the number of elements in each tuple is 1 If multiple iterables are passed, ith tuple contains ith iterable values from all iterables. Suppose, two iterables are passed; one iterable containing 3 and other containing 5 elements, then the returned iterator will have 3 tuples >>> zip() <zip object at 0x102c9be48> # length 0 >>> list(zip()) [] # length 1 >>> list(zip([1,2,3])) [(1,), (2,), (3,)] # same length iterables >>> x, y = [1,2,3], [4,5,6] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] # different length iterables >>> x, y = [1,2,3], [4,5,6,7] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] iterables - can be built-in iterables (like: list, set, tuple, string, dict...), or user-defined iterables (object that has iter method). x, y, z = [1,2,3], [4,5,6], {'a':4,'b':5,'c':6} # dictionaries use keys by default results_default = set(zip(x,y,z)) results_2 = set(zip(x,y,z.keys())) results_3 = set(zip(x,y,z.values())) print(results_default) # {(3, 6, 'c'), (1, 4, 'a'), (2, 5, 'b')} print(results_2) # {(3, 6, 'c'), (1, 4, 'a'), (2, 5, 'b')} print(results_3) # {(1, 4, 4), (3, 6, 6), (2, 5, 5)} View the zip contents with a list or set or tuple data type: >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] >>> set(zip(x,y)) {(2, 5), (3, 6), (1, 4)} >>> tuple(zip(x,y)) ((1, 4), (2, 5), (3, 6)) Unzipping is possible too: > x [1, 2, 3] >>> y [4, 5, 6, 7] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] # Use * to unzip a zip object. You have to call zip() as a wrapper for *zip(). >>> list(zip(*zip(x,y))) [(1, 2, 3), (4, 5, 6)] >>> zip(*zip(x,y)) <zip object at 0x102ccb888> # The zip object is automatically unpacked into a nd b. >>> a,b = zip(*zip(x,y)) >>> a (1, 2, 3) >>> b (4, 5, 6) >>> Watch what happens when you try to view a zip object with a dictionary: >>> dict(zip([1,2],['a','b'])) {1: 'a', 2: 'b'} We can create dictionaries from individual unassociated key, values lists!","title":"zip()"},{"location":"notes/python/useful/Zip/#zip","text":"Combine iterables and return them as tuple sets. The zip() function returns an iterator of tuples based on the iterable object. If no parameters are passed, zip() returns an empty iterator If a single iterable is passed, zip() returns an iterator of 1-tuples. Meaning, the number of elements in each tuple is 1 If multiple iterables are passed, ith tuple contains ith iterable values from all iterables. Suppose, two iterables are passed; one iterable containing 3 and other containing 5 elements, then the returned iterator will have 3 tuples >>> zip() <zip object at 0x102c9be48> # length 0 >>> list(zip()) [] # length 1 >>> list(zip([1,2,3])) [(1,), (2,), (3,)] # same length iterables >>> x, y = [1,2,3], [4,5,6] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] # different length iterables >>> x, y = [1,2,3], [4,5,6,7] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] iterables - can be built-in iterables (like: list, set, tuple, string, dict...), or user-defined iterables (object that has iter method). x, y, z = [1,2,3], [4,5,6], {'a':4,'b':5,'c':6} # dictionaries use keys by default results_default = set(zip(x,y,z)) results_2 = set(zip(x,y,z.keys())) results_3 = set(zip(x,y,z.values())) print(results_default) # {(3, 6, 'c'), (1, 4, 'a'), (2, 5, 'b')} print(results_2) # {(3, 6, 'c'), (1, 4, 'a'), (2, 5, 'b')} print(results_3) # {(1, 4, 4), (3, 6, 6), (2, 5, 5)} View the zip contents with a list or set or tuple data type: >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] >>> set(zip(x,y)) {(2, 5), (3, 6), (1, 4)} >>> tuple(zip(x,y)) ((1, 4), (2, 5), (3, 6)) Unzipping is possible too: > x [1, 2, 3] >>> y [4, 5, 6, 7] >>> list(zip(x,y)) [(1, 4), (2, 5), (3, 6)] # Use * to unzip a zip object. You have to call zip() as a wrapper for *zip(). >>> list(zip(*zip(x,y))) [(1, 2, 3), (4, 5, 6)] >>> zip(*zip(x,y)) <zip object at 0x102ccb888> # The zip object is automatically unpacked into a nd b. >>> a,b = zip(*zip(x,y)) >>> a (1, 2, 3) >>> b (4, 5, 6) >>> Watch what happens when you try to view a zip object with a dictionary: >>> dict(zip([1,2],['a','b'])) {1: 'a', 2: 'b'} We can create dictionaries from individual unassociated key, values lists!","title":"Zip"},{"location":"notes/python/useful/basic_servers/","text":"Some Quick Python Based Servers Update Anvileight has a better article covering more topics Courtesy of 2ality Below notes are from second link Python2 SimpleHTTPServer: a quick way to serve a directory Using SimpleHTTPServer SimpleHTTPServer is invoked like this (the parameter is optional): python -m SimpleHTTPServer <port> (On OS X, Python is pre-installed and this command works out of the box.) Let\u2019s look at an example of using SimpleHTTPServer: During the following Unix shell interaction, I first list the files in the current directory and then start SimpleHTTPServer to serve it. $ ls . foo.html $ python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... Afterwards, I can access the following URLs: http://localhost:8000/ lists the files in the current directory (namely, just foo.html). If there were a file index.html, it would be displayed, instead. http://localhost:8000/foo.html displays the file foo.html in the current directory. Customizing SimpleHTTPServer The following Unix shell script demonstrates how to customize SimpleHTTPServer so that it serves files that have a given file name extension with a given media type. One case where that matters is Firefox being picky about the media type of the webapp.manifest. #!/usr/bin/python import SimpleHTTPServer import SocketServer PORT = 8000 Handler = SimpleHTTPServer.SimpleHTTPRequestHandler Handler.extensions_map.update({ '.webapp': 'application/x-web-app-manifest+json', }); httpd = SocketServer.TCPServer((\"\", PORT), Handler) print \"Serving at port\", PORT httpd.serve_forever() Python3 In python 3 you can run: python3 -m http.server 8080 to create a server that will serve to the folder you are currntly in.","title":"servers"},{"location":"notes/python/useful/basic_servers/#some-quick-python-based-servers","text":"Update Anvileight has a better article covering more topics Courtesy of 2ality Below notes are from second link","title":"Some Quick Python Based Servers"},{"location":"notes/python/useful/basic_servers/#python2","text":"SimpleHTTPServer: a quick way to serve a directory","title":"Python2"},{"location":"notes/python/useful/basic_servers/#using-simplehttpserver","text":"SimpleHTTPServer is invoked like this (the parameter is optional): python -m SimpleHTTPServer <port> (On OS X, Python is pre-installed and this command works out of the box.) Let\u2019s look at an example of using SimpleHTTPServer: During the following Unix shell interaction, I first list the files in the current directory and then start SimpleHTTPServer to serve it. $ ls . foo.html $ python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... Afterwards, I can access the following URLs: http://localhost:8000/ lists the files in the current directory (namely, just foo.html). If there were a file index.html, it would be displayed, instead. http://localhost:8000/foo.html displays the file foo.html in the current directory.","title":"Using SimpleHTTPServer"},{"location":"notes/python/useful/basic_servers/#customizing-simplehttpserver","text":"The following Unix shell script demonstrates how to customize SimpleHTTPServer so that it serves files that have a given file name extension with a given media type. One case where that matters is Firefox being picky about the media type of the webapp.manifest. #!/usr/bin/python import SimpleHTTPServer import SocketServer PORT = 8000 Handler = SimpleHTTPServer.SimpleHTTPRequestHandler Handler.extensions_map.update({ '.webapp': 'application/x-web-app-manifest+json', }); httpd = SocketServer.TCPServer((\"\", PORT), Handler) print \"Serving at port\", PORT httpd.serve_forever()","title":"Customizing SimpleHTTPServer"},{"location":"notes/python/useful/basic_servers/#python3","text":"In python 3 you can run: python3 -m http.server 8080 to create a server that will serve to the folder you are currntly in.","title":"Python3"},{"location":"notes/sass/sass/","text":"Sass Courtesy of sass-lang.com Install Instructions SCSS or SASS? There are two syntaxes available for Sass. The first, known as SCSS (Sassy CSS) and used throughout this reference, is an extension of the syntax of CSS. This means that every valid CSS stylesheet is a valid SCSS file with the same meaning. This syntax is enhanced with the Sass features described below. Files using this syntax have the .scss extension. The second and older syntax , known as the indented syntax (or sometimes just \u201c Sass \u201d), provides a more concise way of writing CSS. It uses indentation rather than brackets to indicate nesting of selectors, and newlines rather than semicolons to separate properties. Files using this syntax have the .sass extension. Preprocessing CSS on its own can be fun, but stylesheets are getting larger, more complex, and harder to maintain. This is where a preprocessor can help. Sass lets you use features that don't exist in CSS yet like variables, nesting, mixins, inheritance and other nifty goodies that make writing CSS fun again. Once you start tinkering with Sass, it will take your preprocessed Sass file and save it as a normal CSS file that you can use in your website. The most direct way to make this happen is in your terminal. Once Sass is installed, you can compile your Sass to CSS using the sass command. You'll need to tell Sass which file to build from, and where to output CSS to. For example, running sass input.scss output.css from your terminal would take a single Sass file, input.scss , and compile that file to output.css . You can also watch individual files or directories with the --watch flag. The watch flag tells Sass to watch your source files for changes, and re-compile CSS each time you save your Sass. If you wanted to watch (instead of manually build) your input.scss file, you'd just add the watch flag to your command, like so: sass --watch input.scss output.css You can watch and output to directories by using folder paths as your input and output, and separating them with a colon. In this example: sass --watch app/sass:public/stylesheets Sass would watch all files in the app/sass folder for changes, and compile CSS to the public/stylesheets folder. More... There is more, but it is so well documented on sass-lang, that it would be redundant to put it here.","title":"SASS"},{"location":"notes/sass/sass/#sass","text":"Courtesy of sass-lang.com Install Instructions","title":"Sass"},{"location":"notes/sass/sass/#scss-or-sass","text":"There are two syntaxes available for Sass. The first, known as SCSS (Sassy CSS) and used throughout this reference, is an extension of the syntax of CSS. This means that every valid CSS stylesheet is a valid SCSS file with the same meaning. This syntax is enhanced with the Sass features described below. Files using this syntax have the .scss extension. The second and older syntax , known as the indented syntax (or sometimes just \u201c Sass \u201d), provides a more concise way of writing CSS. It uses indentation rather than brackets to indicate nesting of selectors, and newlines rather than semicolons to separate properties. Files using this syntax have the .sass extension.","title":"SCSS or SASS?"},{"location":"notes/sass/sass/#preprocessing","text":"CSS on its own can be fun, but stylesheets are getting larger, more complex, and harder to maintain. This is where a preprocessor can help. Sass lets you use features that don't exist in CSS yet like variables, nesting, mixins, inheritance and other nifty goodies that make writing CSS fun again. Once you start tinkering with Sass, it will take your preprocessed Sass file and save it as a normal CSS file that you can use in your website. The most direct way to make this happen is in your terminal. Once Sass is installed, you can compile your Sass to CSS using the sass command. You'll need to tell Sass which file to build from, and where to output CSS to. For example, running sass input.scss output.css from your terminal would take a single Sass file, input.scss , and compile that file to output.css . You can also watch individual files or directories with the --watch flag. The watch flag tells Sass to watch your source files for changes, and re-compile CSS each time you save your Sass. If you wanted to watch (instead of manually build) your input.scss file, you'd just add the watch flag to your command, like so: sass --watch input.scss output.css You can watch and output to directories by using folder paths as your input and output, and separating them with a colon. In this example: sass --watch app/sass:public/stylesheets Sass would watch all files in the app/sass folder for changes, and compile CSS to the public/stylesheets folder.","title":"Preprocessing"},{"location":"notes/sass/sass/#more","text":"There is more, but it is so well documented on sass-lang, that it would be redundant to put it here.","title":"More..."},{"location":"notes/singularity/basics/","text":"Singularity Source Sylabs Inc. Why the name \u201cSingularity\u201d? A \u201cSingularity\u201d is an astrophysics phenomenon in which a single point becomes infinitely dense. This type of a singularity can thus contain massive quantities of universe within it and thus encapsulating an infinite amount of data within it. Build from Recipe Do you need administrator privileges to use Singularity? You generally do not need admin/sudo to use Singularity containers but you do however need admin/root access to install Singularity and for some container build functions (for example, building from a recipe, or a writable image). The Recipe File Bootstrap let's us know that we are using a container from dockerhub to build off of. This should be similar to the Dockerfile from docker. Most of this should be self explainatory. Bootstrap: docker From: ubuntu:18.04 %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / %post apt-get -qq -y update apt-get -qq -y install python > /dev/null %runscript python /test.py test.py: import os if __name__ == '__main__': print('import os module and clearly python is installed!') print(os.environ['MY_VAR']) Sections in recipe file: help: Give some help labels: Impart meta-data into container environment: Environment variables files: Copy files into container post: Run commands once container is created runscript: A command to run by default Build Images from Scratch Use this command to build the recipe file into a *.simg sudo singularity build ubuntu.simg Singularity.recipe Here the \"ubuntu.simg singularity.recipe\" part of the command is \"<image-name>.simg <recipe-file-name>.recipe\" You can build from multiple sources and even create interactive containers that you can add to. We will cover these soon, but here is a picture illustrating the many sources for singularity containers: Red arrows represent operations that must be carried out with root privileges. Also I believe looking at the documentation that --writable and --sandbox containers are now 1 type singularity v3+. The difference is that you can either a --writable or non writable sandbox. Interactive Builds Singularity v3.0 and above produces immutable images in the Singularity Image File (SIF) format. This ensures reproducible and verifiable images and allows for many extra benefits such as the ability to sign and verify your containers. However, during testing and debugging you may want an image format that is writable. This way you can shell into the image and install software and dependencies until you are satisfied that your container will fulfill your needs. For these scenarios, Singularity also supports the sandbox format (which is really just a directory). Let's say I start with ubuntu: singularity build ubuntu.simg docker://ubuntu:latest bbearce@pop-os:~/$ ls ubuntu.simg Now this is read only and can just be run or you can shell into it: bbearce@pop-os:~/$ singularity shell ubuntu.simg Singularity> ls ubuntu.simg Notice that since the user's home directory is automatically mounted it shows the *.simg with ls . Watch as writing anything triggers an error: Singularity> touch anything.txt Singularity> ls anything.txt ubuntu.simg How come that worked? Well the mounted home directories are considered outside the container and you can read and write to those. Let's try again in /mnt : Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system ahhh, there we go. So how do we write inside a singularity container? Enter the --sandbox First we have to create a sandbox out of the original container: bbearce@pop-os:~/$ singularity build --sandbox s_ubuntu ubuntu.simg INFO: Starting build... INFO: Creating sandbox directory... INFO: Build complete: s_ubuntu Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub or dockerhub images as well. so let's see what is inside: bbearce@pop-os:~/$ ls s_ubuntu/ bin etc lib64 opt sbin tmp boot home libx32 proc singularity usr dev lib media root srv var environment lib32 mnt run sys An entire ubuntu os! Cool so now let's change stuff in it: bbearce@pop-os:~/$ singularity shell s_ubuntu Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system So even though we made a sandbox which is desinged to be interactive, we still need the --writable flag. bbearce@pop-os:~/$ singularity shell --writable s_ubuntu Singularity> touch /mnt/anything.txt Singularity> ls /mnt anything.txt We did it! Ok now how do we make this change into a new container: bbearce@pop-os:~/$ singularity build s_ubuntu_modified s_ubuntu INFO: Starting build... INFO: Creating SIF file... INFO: Build complete: s_ubuntu_modified bbearce@pop-os:~/$ ls s_ubuntu_modified ubuntu.simg s_ubuntu Now let's see if our change made it: bbearce@pop-os:~/$ singularity shell s_ubuntu_modified Singularity> ls /mnt anything.txt Boom! So to recap, you need to build a --sandbox image from just about any image source, be it a hub or recipe file. Then shell into that container directory with flag --writable . Run Continue from the Build section above with files test.py and Singularity.recipe. Run the image with run : bbearce@pop-os:~/$ singularity run ubuntu.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ or treat as an executable: bbearce@pop-os:~/$ ./ubuntu.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ run also works with shub:// and docker:// URIs. This creates an ephemeral container that runs and then disappears. $ singularity run shub://GodloveD/lolcow Files on the host are reachable from within the container. $ echo \"Hello World\" > $HOME/hello-kitty.txt $ singularity exec vsoch-hello-world-master.simg cat $HOME/hello-kitty.txt Hello World This example works because hello-kitty.txt exists in the user\u2019s home directory. By default singularity bind mounts /home/$USER , /tmp , and $PWD into your container at runtime. Mounting You can specify additional directories to bind mount into your container with the --bind option. In this example, the /data directory on the host system is bind mounted to the /mnt directory inside the container. $ echo \"I am your father\" >/data/vader.sez $ ~/sing-dev/bin/singularity exec --bind /data:/mnt hello-world.simg cat /mnt/vader.sez I am your father Inspect Look at meta-data with inspect . Notice the %labels sections details shows up under \"CREATOR\": \"Ben\" bbearce@bryce:~/singularity$ singularity inspect ubuntu.simg { \"org.label-schema.usage.singularity.deffile.bootstrap\": \"docker\", \"org.label-schema.usage.singularity.deffile\": \"singularity.recipe\", \"org.label-schema.usage\": \"/.singularity.d/runscript.help\", \"org.label-schema.schema-version\": \"1.0\", \"CREATOR\": \"Ben\", \"org.label-schema.usage.singularity.deffile.from\": \"ubuntu:18.04\", \"org.label-schema.build-date\": \"Tue,_04_Feb_2020_15:36:00_-0500\", \"org.label-schema.usage.singularity.runscript.help\": \"/.singularity.d/runscript.help\", \"org.label-schema.usage.singularity.version\": \"2.5.2-dist\", \"org.label-schema.build-size\": \"135MB\" }","title":"Basics"},{"location":"notes/singularity/basics/#singularity","text":"Source Sylabs Inc. Why the name \u201cSingularity\u201d? A \u201cSingularity\u201d is an astrophysics phenomenon in which a single point becomes infinitely dense. This type of a singularity can thus contain massive quantities of universe within it and thus encapsulating an infinite amount of data within it.","title":"Singularity"},{"location":"notes/singularity/basics/#build-from-recipe","text":"Do you need administrator privileges to use Singularity? You generally do not need admin/sudo to use Singularity containers but you do however need admin/root access to install Singularity and for some container build functions (for example, building from a recipe, or a writable image).","title":"Build from Recipe"},{"location":"notes/singularity/basics/#the-recipe-file","text":"Bootstrap let's us know that we are using a container from dockerhub to build off of. This should be similar to the Dockerfile from docker. Most of this should be self explainatory. Bootstrap: docker From: ubuntu:18.04 %help This is where you can add some useful info. %labels Creator Ben %environment export MY_VAR='~~~~~some environment variable~~~~~' %files test.py / %post apt-get -qq -y update apt-get -qq -y install python > /dev/null %runscript python /test.py test.py: import os if __name__ == '__main__': print('import os module and clearly python is installed!') print(os.environ['MY_VAR']) Sections in recipe file: help: Give some help labels: Impart meta-data into container environment: Environment variables files: Copy files into container post: Run commands once container is created runscript: A command to run by default","title":"The Recipe File"},{"location":"notes/singularity/basics/#build-images-from-scratch","text":"Use this command to build the recipe file into a *.simg sudo singularity build ubuntu.simg Singularity.recipe Here the \"ubuntu.simg singularity.recipe\" part of the command is \"<image-name>.simg <recipe-file-name>.recipe\" You can build from multiple sources and even create interactive containers that you can add to. We will cover these soon, but here is a picture illustrating the many sources for singularity containers: Red arrows represent operations that must be carried out with root privileges. Also I believe looking at the documentation that --writable and --sandbox containers are now 1 type singularity v3+. The difference is that you can either a --writable or non writable sandbox.","title":"Build Images from Scratch"},{"location":"notes/singularity/basics/#interactive-builds","text":"Singularity v3.0 and above produces immutable images in the Singularity Image File (SIF) format. This ensures reproducible and verifiable images and allows for many extra benefits such as the ability to sign and verify your containers. However, during testing and debugging you may want an image format that is writable. This way you can shell into the image and install software and dependencies until you are satisfied that your container will fulfill your needs. For these scenarios, Singularity also supports the sandbox format (which is really just a directory). Let's say I start with ubuntu: singularity build ubuntu.simg docker://ubuntu:latest bbearce@pop-os:~/$ ls ubuntu.simg Now this is read only and can just be run or you can shell into it: bbearce@pop-os:~/$ singularity shell ubuntu.simg Singularity> ls ubuntu.simg Notice that since the user's home directory is automatically mounted it shows the *.simg with ls . Watch as writing anything triggers an error: Singularity> touch anything.txt Singularity> ls anything.txt ubuntu.simg How come that worked? Well the mounted home directories are considered outside the container and you can read and write to those. Let's try again in /mnt : Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system ahhh, there we go. So how do we write inside a singularity container?","title":"Interactive Builds"},{"location":"notes/singularity/basics/#enter-the-sandbox","text":"First we have to create a sandbox out of the original container: bbearce@pop-os:~/$ singularity build --sandbox s_ubuntu ubuntu.simg INFO: Starting build... INFO: Creating sandbox directory... INFO: Build complete: s_ubuntu Notice how we build it out of the original *.simg container we had. You can base a sandbox on shub or dockerhub images as well. so let's see what is inside: bbearce@pop-os:~/$ ls s_ubuntu/ bin etc lib64 opt sbin tmp boot home libx32 proc singularity usr dev lib media root srv var environment lib32 mnt run sys An entire ubuntu os! Cool so now let's change stuff in it: bbearce@pop-os:~/$ singularity shell s_ubuntu Singularity> touch /mnt/anything.txt touch: cannot touch '/mnt/anything.txt': Read-only file system So even though we made a sandbox which is desinged to be interactive, we still need the --writable flag. bbearce@pop-os:~/$ singularity shell --writable s_ubuntu Singularity> touch /mnt/anything.txt Singularity> ls /mnt anything.txt We did it! Ok now how do we make this change into a new container: bbearce@pop-os:~/$ singularity build s_ubuntu_modified s_ubuntu INFO: Starting build... INFO: Creating SIF file... INFO: Build complete: s_ubuntu_modified bbearce@pop-os:~/$ ls s_ubuntu_modified ubuntu.simg s_ubuntu Now let's see if our change made it: bbearce@pop-os:~/$ singularity shell s_ubuntu_modified Singularity> ls /mnt anything.txt Boom! So to recap, you need to build a --sandbox image from just about any image source, be it a hub or recipe file. Then shell into that container directory with flag --writable .","title":"Enter the --sandbox"},{"location":"notes/singularity/basics/#run","text":"Continue from the Build section above with files test.py and Singularity.recipe. Run the image with run : bbearce@pop-os:~/$ singularity run ubuntu.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ or treat as an executable: bbearce@pop-os:~/$ ./ubuntu.simg import os module and clearly python is installed! ~~~~~some environment variable~~~~~ run also works with shub:// and docker:// URIs. This creates an ephemeral container that runs and then disappears. $ singularity run shub://GodloveD/lolcow Files on the host are reachable from within the container. $ echo \"Hello World\" > $HOME/hello-kitty.txt $ singularity exec vsoch-hello-world-master.simg cat $HOME/hello-kitty.txt Hello World This example works because hello-kitty.txt exists in the user\u2019s home directory. By default singularity bind mounts /home/$USER , /tmp , and $PWD into your container at runtime.","title":"Run"},{"location":"notes/singularity/basics/#mounting","text":"You can specify additional directories to bind mount into your container with the --bind option. In this example, the /data directory on the host system is bind mounted to the /mnt directory inside the container. $ echo \"I am your father\" >/data/vader.sez $ ~/sing-dev/bin/singularity exec --bind /data:/mnt hello-world.simg cat /mnt/vader.sez I am your father","title":"Mounting"},{"location":"notes/singularity/basics/#inspect","text":"Look at meta-data with inspect . Notice the %labels sections details shows up under \"CREATOR\": \"Ben\" bbearce@bryce:~/singularity$ singularity inspect ubuntu.simg { \"org.label-schema.usage.singularity.deffile.bootstrap\": \"docker\", \"org.label-schema.usage.singularity.deffile\": \"singularity.recipe\", \"org.label-schema.usage\": \"/.singularity.d/runscript.help\", \"org.label-schema.schema-version\": \"1.0\", \"CREATOR\": \"Ben\", \"org.label-schema.usage.singularity.deffile.from\": \"ubuntu:18.04\", \"org.label-schema.build-date\": \"Tue,_04_Feb_2020_15:36:00_-0500\", \"org.label-schema.usage.singularity.runscript.help\": \"/.singularity.d/runscript.help\", \"org.label-schema.usage.singularity.version\": \"2.5.2-dist\", \"org.label-schema.build-size\": \"135MB\" }","title":"Inspect"},{"location":"notes/singularity/install/","text":"Installing Singularity Source: sylabs.io Terminal Install: Pre-requisites: On Ubuntu or Debian install the following dependencies: sudo apt-get update && sudo apt-get install -y \\ build-essential \\ uuid-dev \\ libgpgme-dev \\ squashfs-tools \\ libseccomp-dev \\ wget \\ pkg-config \\ git \\ cryptsetup-bin Install Go: export VERSION=1.5.12 OS=linux ARCH=amd64 && \\ wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \\ sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \\ rm go$VERSION.$OS-$ARCH.tar.gz echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \\ source ~/.bashrc Install Singularity: export VERSION=3.6.3 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz && \\ tar -xzf singularity-${VERSION}.tar.gz && \\ cd singularity then check out code from git: git clone https://github.com/sylabs/singularity.git && \\ cd singularity && \\ git checkout v3.6.3 Now compile Singularity: ./mconfig && \\ make -C ./builddir && \\ sudo make -C ./builddir install","title":"Install"},{"location":"notes/singularity/install/#installing-singularity","text":"Source: sylabs.io","title":"Installing Singularity"},{"location":"notes/singularity/install/#terminal-install","text":"","title":"Terminal Install:"},{"location":"notes/singularity/install/#pre-requisites","text":"On Ubuntu or Debian install the following dependencies: sudo apt-get update && sudo apt-get install -y \\ build-essential \\ uuid-dev \\ libgpgme-dev \\ squashfs-tools \\ libseccomp-dev \\ wget \\ pkg-config \\ git \\ cryptsetup-bin Install Go: export VERSION=1.5.12 OS=linux ARCH=amd64 && \\ wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \\ sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \\ rm go$VERSION.$OS-$ARCH.tar.gz echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \\ source ~/.bashrc","title":"Pre-requisites:"},{"location":"notes/singularity/install/#install-singularity","text":"export VERSION=3.6.3 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz && \\ tar -xzf singularity-${VERSION}.tar.gz && \\ cd singularity then check out code from git: git clone https://github.com/sylabs/singularity.git && \\ cd singularity && \\ git checkout v3.6.3 Now compile Singularity: ./mconfig && \\ make -C ./builddir && \\ sudo make -C ./builddir install","title":"Install Singularity:"},{"location":"notes/singularity/use_with_docker/","text":"Use with docker Spoiler, while using docker is possible, I think it's going to be harder in the long run. I believe there is nothing wrong with starting on docker on bryce and in fact I recommend that. Once on Carlsbad or Dotter, I believe that just using singularity is easier (than using docker on these machines). #I_Love_Docker Use on Martinos machines: Setup simlinks: Source Martinos Docs ls -la ~ ... lrwxrwxrwx 1 bb927 bb927 33 Sep 29 12:48 .share -> /space/dotter/2/users/bb927/share lrwxrwxrwx 1 bb927 bb927 39 Sep 29 12:44 .singularity -> /space/dotter/2/users/bb927/singularity ... Once you have this setup, you can proceed, otherwise you are in for a world of hurt if you accidentally fill up your real home dir. Martinos gotchas: Dotter for example: Pure Singularity: Your home directory and drive directories are actually simlinks and this will mess up singularity on dotter. dotter[0]:bb927$ ls -la /homes/3 lrwxrwxrwx 1 root root 17 Sep 17 2016 /homes/3 -> /autofs/homes/003 dotter[0]:bb927$ ls -la /space/dotter total 32 drwxr-xr-x 2 root root 4096 Jul 9 12:00 . drwx--x--x 984 root root 24576 Sep 19 18:12 .. lrwxrwxrwx 1 root root 24 Jul 9 12:00 1 -> /autofs/space/dotter_001 lrwxrwxrwx 1 root root 24 Jul 9 12:00 2 -> /autofs/space/dotter_002 lrwxrwxrwx 1 root root 24 Jul 9 12:00 3 -> /autofs/space/dotter_003 lrwxrwxrwx 1 root root 24 Jul 9 12:00 4 -> /autofs/space/dotter_004 lrwxrwxrwx 1 root root 24 Jul 9 12:00 5 -> /autofs/space/dotter_005 lrwxrwxrwx 1 root root 24 Jul 9 12:00 6 -> /autofs/space/dotter_006 Remember Singularity wants to auto --bind mount these 3 directories: /home/$USER /tmp $PWD I have a sanbox made from a regular ubuntu image (docker://ubuntu:latest) that was first pulled to ubuntu.simg and then the sandbox is called s_ubuntu . Let's try to shell into it. This is my current $PWD: dotter[0]:bb927$ pwd /space/dotter/2/users/bb927 dotter[0]:bb927$ ls in out README.md share singularity s_ubuntu ubuntu.simg After shelling in let's look at the mounts: dotter[0]:bb927$ singularity shell s_ubuntu Singularity> ls ~ Desktop Downloads Pictures Templates matlab Documents Music Public Videos matlab_crash_dump.224407-1 Singularity> ls /tmp hsperfdata_bb927 hsperfdata_sra24 idmap.sh.log krb5ccmachine_PARTNERS.ORG systemd-private-9eb3a577ab90415d9ff57c178714ca99-bolt.service-eEU8AX systemd-private-9eb3a577ab90415d9ff57c178714ca99-chronyd.service-YED8Bf systemd-private-9eb3a577ab90415d9ff57c178714ca99-colord.service-yBgBOp systemd-private-9eb3a577ab90415d9ff57c178714ca99-cups.service-eaN0xa systemd-private-9eb3a577ab90415d9ff57c178714ca99-rtkit-daemon.service-AtHDqM Singularity> pwd /space/dotter/2/users/bb927 Singularity> exit exit Now outside the container: dotter[0]:bb927$ ls ~ Desktop Downloads matlab_crash_dump.224407-1 Pictures Templates Documents matlab Music Public Videos dotter[0]:bb927$ ls /tmp hsperfdata_bb927 hsperfdata_sra24 idmap.sh.log krb5ccmachine_PARTNERS.ORG systemd-private-9eb3a577ab90415d9ff57c178714ca99-bolt.service-eEU8AX systemd-private-9eb3a577ab90415d9ff57c178714ca99-chronyd.service-YED8Bf systemd-private-9eb3a577ab90415d9ff57c178714ca99-colord.service-yBgBOp systemd-private-9eb3a577ab90415d9ff57c178714ca99-cups.service-eaN0xa systemd-private-9eb3a577ab90415d9ff57c178714ca99-rtkit-daemon.service-AtHDqM dotter[0]:bb927$ pwd /space/dotter/2/users/bb927 dotter[0]:bb927$ We see those three locations mounted perfectly! :) Now let's shell in with --writable dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /homes destination automatically without overlay or underlay FATAL: container creation failed: mount /var/singularity/mnt/session/homes->/homes error: while mounting /var/singularity/mnt/session/homes: destination /homes doesn't exist in container dotter[0]:bb927$ Ugh, what happend? Well the WARNINGS were sort of ignored but the FATAL error is because /homes doesn't exist in the container. We can work around this by creating it before hand: dotter[0]:bb927$ mkdir s_ubuntu/homes dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: Skipping mount /space/dotter/2/users/bb927 [cwd]: /autofs/space/dotter_002/users/bb927 doesn't exist in container Singularity> ls /homes 3 Singularity> ls /homes/3 bb927 Singularity> ls /homes/3/bb927/ Desktop Downloads Pictures Templates matlab Documents Music Public Videos matlab_crash_dump.224407-1 Singularity> Now even though the warning says we might not be able to write to our home directory, you can. This is probably because dotter has an environment variable HOME=/homes/3/bb927 and that is also set inside the singularity container. Even though that is a simlink you can access it inside the container. But we know that Help Desk has asked us not to use our home directories because they have limited storage, so let's create the drive mount location and get rid of another warning: dotter[0]:bb927$ mkdir -p s_ubuntu/autofs/space/dotter_002/users/bb927 dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> So we know we can ignore the first WARNING because there isn't much we can do, and as far as the second one is concerned, you have the tools to make it disappear if you need it to. Mounting, does it work?!?!? In my $PWD I have in and out folders: dotter[0]:bb927$ ls in out README.md share singularity s_ubuntu ubuntu.simg dotter[0]:bb927$ ls in in.txt dotter[0]:bb927$ ls out out.txt Let's see if we can read and write to them: Note: Just from playing around I've noticed in --writable mode you need to have created the mount points ahead of time. It seems that when running the container, you will not need to do this as the mounts will be made on the fly as shown below: dotter[0]:bb927$ singularity shell --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu Singularity> ls /mnt in out Singularity> ls /mnt/in in.txt Singularity> ls /mnt/out out.txt watch in --writable mode: dotter[0]:bb927$ singularity shell --writable --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu/ WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /mnt/in destination automatically without overlay or underlay FATAL: container creation failed: mount /autofs/space/dotter_002/users/bb927/in->/mnt/in error: while mounting /autofs/space/dotter_002/users/bb927/in: destination /mnt/in doesn't exist in container So create them: dotter[0]:bb927$ mkdir -p s_ubuntu/mnt/in dotter[0]:bb927$ mkdir -p s_ubuntu/mnt/out and try again: dotter[0]:bb927$ singularity shell --writable --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu/ WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> ls /mnt in out Singularity> ls /mnt/in in.txt Singularity> ls /mnt/out out.txt Nice! Ok, now the 1st of two important questions; can we read/write? Singularity> cd /mnt/in Singularity> touch new_file.txt Singularity> ls in.txt new_file.txt out.txt Singularity> pwd /mnt/in Singularity> exit exit dotter[0]:bb927$ ls in in.txt new_file.txt out.txt dotter[0]:bb927$ ls -la in total 16 drwxrwsr-x 2 bb927 pimi 4096 Oct 3 20:58 . drwxrwsr-x 7 bb927 pimi 4096 Oct 3 19:55 .. -rw-rw-r-- 1 bb927 pimi 10 Oct 3 20:56 in.txt -rw-r--r-- 1 bb927 pimi 0 Oct 3 20:58 new_file.txt -rw-rw-r-- 1 bb927 pimi 11 Oct 3 20:55 out.txt Nice! Ok so now can we access GPUs? Let's pull pytorch: dotter[0]:bb927$ singularity pull docker://pytorch/pytorch:latest INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 23884877105a done Copying blob bc38caa0f5b9 done Copying blob 2910811b6c42 done Copying blob 36505266dcc6 done Copying blob 3472d01858ba done Copying blob 4a98b57681ff done Copying blob f3b419d1e6d5 done Copying config c35e69071c done Writing manifest to image destination Storing signatures 2020/10/03 21:02:21 info unpack layer: sha256:23884877105a7ff84a910895cd044061a4561385ff6c36480ee080b76ec0e771 2020/10/03 21:02:23 info unpack layer: sha256:bc38caa0f5b94141276220daaf428892096e4afd24b05668cd188311e00a635f 2020/10/03 21:02:23 info unpack layer: sha256:2910811b6c4227c2f42aaea9a3dd5f53b1d469f67e2cf7e601f631b119b61ff7 2020/10/03 21:02:23 info unpack layer: sha256:36505266dcc64eeb1010bd2112e6f73981e1a8246e4f6d4e287763b57f101b0b 2020/10/03 21:02:23 info unpack layer: sha256:3472d01858ba9ce89d2a493bb3964eaf26a63500a3b47ad0a4c72d5e5fe40e11 2020/10/03 21:02:23 info unpack layer: sha256:4a98b57681ffdd1fcba82e9aeb95171dbe0837980deb82464e4893f4449c4459 2020/10/03 21:03:08 info unpack layer: sha256:f3b419d1e6d54391719bc37b641b2c133915f207103d8dd28c776607e5d477d8 INFO: Creating SIF file... INFO: Build complete: pytorch_latest.sif dotter[0]:bb927$ Alright deep breath: dotter[0]:bb927$ singularity shell pytorch_latest.sif Singularity> python Python 3.7.7 (default, May 7 2020, 21:25:33) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() False >>> NOOOOOooooooooooo...Ok there must be a flag: dotter[0]:bb927$ singularity shell --nv pytorch_latest.sif Singularity> python Python 3.7.7 (default, May 7 2020, 21:25:33) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() True Swish Should we use Docker??? Actually I think this could be a problem. See as root inside docker if you write to mounts it shows up ownded by either root or 10000..:1000..., some weird user of numbers...Anyways you won't be able to manipulate that data once you log out of the container as you are not them! It seems singularity is the best way to go mainly because you are the same user inside and outside the singularity... Note I haven't seen this per se, but Kevin has and DeepNeuro made files with a cron job that had those user characteristics... Update: tried it and actually you can't write even though you are root: dotter[0]:bb927$ docker run -it --rm -v /space/dotter/2/users/bb927/in:/mnt/in pytorch/pytorch:latest bash root@7b3a11b77784:/workspace# ls /mnt/in in.txt new_file.txt out.txt root@7b3a11b77784:/workspace# touch /mnt/in/example_of_what_not_to_do.txt touch: cannot touch '/mnt/in/example_of_what_not_to_do.txt': Permission denied So if you know how to add users in your Dockerfile, you could in theory make the user inside the image your Carlsbad or Dotter user, but that's a lot of work. I hope that using singularity just works smoothly...Stay tuned for how to use SHUB, Singularities Dockerhub...","title":"Use With Docker"},{"location":"notes/singularity/use_with_docker/#use-with-docker","text":"Spoiler, while using docker is possible, I think it's going to be harder in the long run. I believe there is nothing wrong with starting on docker on bryce and in fact I recommend that. Once on Carlsbad or Dotter, I believe that just using singularity is easier (than using docker on these machines). #I_Love_Docker","title":"Use with docker"},{"location":"notes/singularity/use_with_docker/#use-on-martinos-machines","text":"","title":"Use on Martinos machines:"},{"location":"notes/singularity/use_with_docker/#setup-simlinks","text":"Source Martinos Docs ls -la ~ ... lrwxrwxrwx 1 bb927 bb927 33 Sep 29 12:48 .share -> /space/dotter/2/users/bb927/share lrwxrwxrwx 1 bb927 bb927 39 Sep 29 12:44 .singularity -> /space/dotter/2/users/bb927/singularity ... Once you have this setup, you can proceed, otherwise you are in for a world of hurt if you accidentally fill up your real home dir.","title":"Setup simlinks:"},{"location":"notes/singularity/use_with_docker/#martinos-gotchas","text":"","title":"Martinos gotchas:"},{"location":"notes/singularity/use_with_docker/#dotter-for-example","text":"","title":"Dotter for example:"},{"location":"notes/singularity/use_with_docker/#pure-singularity","text":"Your home directory and drive directories are actually simlinks and this will mess up singularity on dotter. dotter[0]:bb927$ ls -la /homes/3 lrwxrwxrwx 1 root root 17 Sep 17 2016 /homes/3 -> /autofs/homes/003 dotter[0]:bb927$ ls -la /space/dotter total 32 drwxr-xr-x 2 root root 4096 Jul 9 12:00 . drwx--x--x 984 root root 24576 Sep 19 18:12 .. lrwxrwxrwx 1 root root 24 Jul 9 12:00 1 -> /autofs/space/dotter_001 lrwxrwxrwx 1 root root 24 Jul 9 12:00 2 -> /autofs/space/dotter_002 lrwxrwxrwx 1 root root 24 Jul 9 12:00 3 -> /autofs/space/dotter_003 lrwxrwxrwx 1 root root 24 Jul 9 12:00 4 -> /autofs/space/dotter_004 lrwxrwxrwx 1 root root 24 Jul 9 12:00 5 -> /autofs/space/dotter_005 lrwxrwxrwx 1 root root 24 Jul 9 12:00 6 -> /autofs/space/dotter_006 Remember Singularity wants to auto --bind mount these 3 directories: /home/$USER /tmp $PWD I have a sanbox made from a regular ubuntu image (docker://ubuntu:latest) that was first pulled to ubuntu.simg and then the sandbox is called s_ubuntu . Let's try to shell into it. This is my current $PWD: dotter[0]:bb927$ pwd /space/dotter/2/users/bb927 dotter[0]:bb927$ ls in out README.md share singularity s_ubuntu ubuntu.simg After shelling in let's look at the mounts: dotter[0]:bb927$ singularity shell s_ubuntu Singularity> ls ~ Desktop Downloads Pictures Templates matlab Documents Music Public Videos matlab_crash_dump.224407-1 Singularity> ls /tmp hsperfdata_bb927 hsperfdata_sra24 idmap.sh.log krb5ccmachine_PARTNERS.ORG systemd-private-9eb3a577ab90415d9ff57c178714ca99-bolt.service-eEU8AX systemd-private-9eb3a577ab90415d9ff57c178714ca99-chronyd.service-YED8Bf systemd-private-9eb3a577ab90415d9ff57c178714ca99-colord.service-yBgBOp systemd-private-9eb3a577ab90415d9ff57c178714ca99-cups.service-eaN0xa systemd-private-9eb3a577ab90415d9ff57c178714ca99-rtkit-daemon.service-AtHDqM Singularity> pwd /space/dotter/2/users/bb927 Singularity> exit exit Now outside the container: dotter[0]:bb927$ ls ~ Desktop Downloads matlab_crash_dump.224407-1 Pictures Templates Documents matlab Music Public Videos dotter[0]:bb927$ ls /tmp hsperfdata_bb927 hsperfdata_sra24 idmap.sh.log krb5ccmachine_PARTNERS.ORG systemd-private-9eb3a577ab90415d9ff57c178714ca99-bolt.service-eEU8AX systemd-private-9eb3a577ab90415d9ff57c178714ca99-chronyd.service-YED8Bf systemd-private-9eb3a577ab90415d9ff57c178714ca99-colord.service-yBgBOp systemd-private-9eb3a577ab90415d9ff57c178714ca99-cups.service-eaN0xa systemd-private-9eb3a577ab90415d9ff57c178714ca99-rtkit-daemon.service-AtHDqM dotter[0]:bb927$ pwd /space/dotter/2/users/bb927 dotter[0]:bb927$ We see those three locations mounted perfectly! :) Now let's shell in with --writable dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /homes destination automatically without overlay or underlay FATAL: container creation failed: mount /var/singularity/mnt/session/homes->/homes error: while mounting /var/singularity/mnt/session/homes: destination /homes doesn't exist in container dotter[0]:bb927$ Ugh, what happend? Well the WARNINGS were sort of ignored but the FATAL error is because /homes doesn't exist in the container. We can work around this by creating it before hand: dotter[0]:bb927$ mkdir s_ubuntu/homes dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: Skipping mount /space/dotter/2/users/bb927 [cwd]: /autofs/space/dotter_002/users/bb927 doesn't exist in container Singularity> ls /homes 3 Singularity> ls /homes/3 bb927 Singularity> ls /homes/3/bb927/ Desktop Downloads Pictures Templates matlab Documents Music Public Videos matlab_crash_dump.224407-1 Singularity> Now even though the warning says we might not be able to write to our home directory, you can. This is probably because dotter has an environment variable HOME=/homes/3/bb927 and that is also set inside the singularity container. Even though that is a simlink you can access it inside the container. But we know that Help Desk has asked us not to use our home directories because they have limited storage, so let's create the drive mount location and get rid of another warning: dotter[0]:bb927$ mkdir -p s_ubuntu/autofs/space/dotter_002/users/bb927 dotter[0]:bb927$ singularity shell --writable s_ubuntu WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> So we know we can ignore the first WARNING because there isn't much we can do, and as far as the second one is concerned, you have the tools to make it disappear if you need it to. Mounting, does it work?!?!? In my $PWD I have in and out folders: dotter[0]:bb927$ ls in out README.md share singularity s_ubuntu ubuntu.simg dotter[0]:bb927$ ls in in.txt dotter[0]:bb927$ ls out out.txt Let's see if we can read and write to them: Note: Just from playing around I've noticed in --writable mode you need to have created the mount points ahead of time. It seems that when running the container, you will not need to do this as the mounts will be made on the fly as shown below: dotter[0]:bb927$ singularity shell --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu Singularity> ls /mnt in out Singularity> ls /mnt/in in.txt Singularity> ls /mnt/out out.txt watch in --writable mode: dotter[0]:bb927$ singularity shell --writable --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu/ WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container WARNING: By using --writable, Singularity can't create /mnt/in destination automatically without overlay or underlay FATAL: container creation failed: mount /autofs/space/dotter_002/users/bb927/in->/mnt/in error: while mounting /autofs/space/dotter_002/users/bb927/in: destination /mnt/in doesn't exist in container So create them: dotter[0]:bb927$ mkdir -p s_ubuntu/mnt/in dotter[0]:bb927$ mkdir -p s_ubuntu/mnt/out and try again: dotter[0]:bb927$ singularity shell --writable --bind /autofs/space/dotter_002/users/bb927/in:/mnt/in --bind /autofs/space/dotter_002/users/bb927/out:/mnt/out s_ubuntu/ WARNING: Your current working directory is a symlink and may not be available in container, you should use real path with --writable when possible WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container Singularity> ls /mnt in out Singularity> ls /mnt/in in.txt Singularity> ls /mnt/out out.txt Nice! Ok, now the 1st of two important questions; can we read/write? Singularity> cd /mnt/in Singularity> touch new_file.txt Singularity> ls in.txt new_file.txt out.txt Singularity> pwd /mnt/in Singularity> exit exit dotter[0]:bb927$ ls in in.txt new_file.txt out.txt dotter[0]:bb927$ ls -la in total 16 drwxrwsr-x 2 bb927 pimi 4096 Oct 3 20:58 . drwxrwsr-x 7 bb927 pimi 4096 Oct 3 19:55 .. -rw-rw-r-- 1 bb927 pimi 10 Oct 3 20:56 in.txt -rw-r--r-- 1 bb927 pimi 0 Oct 3 20:58 new_file.txt -rw-rw-r-- 1 bb927 pimi 11 Oct 3 20:55 out.txt Nice! Ok so now can we access GPUs? Let's pull pytorch: dotter[0]:bb927$ singularity pull docker://pytorch/pytorch:latest INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 23884877105a done Copying blob bc38caa0f5b9 done Copying blob 2910811b6c42 done Copying blob 36505266dcc6 done Copying blob 3472d01858ba done Copying blob 4a98b57681ff done Copying blob f3b419d1e6d5 done Copying config c35e69071c done Writing manifest to image destination Storing signatures 2020/10/03 21:02:21 info unpack layer: sha256:23884877105a7ff84a910895cd044061a4561385ff6c36480ee080b76ec0e771 2020/10/03 21:02:23 info unpack layer: sha256:bc38caa0f5b94141276220daaf428892096e4afd24b05668cd188311e00a635f 2020/10/03 21:02:23 info unpack layer: sha256:2910811b6c4227c2f42aaea9a3dd5f53b1d469f67e2cf7e601f631b119b61ff7 2020/10/03 21:02:23 info unpack layer: sha256:36505266dcc64eeb1010bd2112e6f73981e1a8246e4f6d4e287763b57f101b0b 2020/10/03 21:02:23 info unpack layer: sha256:3472d01858ba9ce89d2a493bb3964eaf26a63500a3b47ad0a4c72d5e5fe40e11 2020/10/03 21:02:23 info unpack layer: sha256:4a98b57681ffdd1fcba82e9aeb95171dbe0837980deb82464e4893f4449c4459 2020/10/03 21:03:08 info unpack layer: sha256:f3b419d1e6d54391719bc37b641b2c133915f207103d8dd28c776607e5d477d8 INFO: Creating SIF file... INFO: Build complete: pytorch_latest.sif dotter[0]:bb927$ Alright deep breath: dotter[0]:bb927$ singularity shell pytorch_latest.sif Singularity> python Python 3.7.7 (default, May 7 2020, 21:25:33) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() False >>> NOOOOOooooooooooo...Ok there must be a flag: dotter[0]:bb927$ singularity shell --nv pytorch_latest.sif Singularity> python Python 3.7.7 (default, May 7 2020, 21:25:33) [GCC 7.3.0] :: Anaconda, Inc. on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import torch >>> torch.cuda.is_available() True Swish","title":"Pure Singularity:"},{"location":"notes/singularity/use_with_docker/#should-we-use-docker","text":"Actually I think this could be a problem. See as root inside docker if you write to mounts it shows up ownded by either root or 10000..:1000..., some weird user of numbers...Anyways you won't be able to manipulate that data once you log out of the container as you are not them! It seems singularity is the best way to go mainly because you are the same user inside and outside the singularity... Note I haven't seen this per se, but Kevin has and DeepNeuro made files with a cron job that had those user characteristics... Update: tried it and actually you can't write even though you are root: dotter[0]:bb927$ docker run -it --rm -v /space/dotter/2/users/bb927/in:/mnt/in pytorch/pytorch:latest bash root@7b3a11b77784:/workspace# ls /mnt/in in.txt new_file.txt out.txt root@7b3a11b77784:/workspace# touch /mnt/in/example_of_what_not_to_do.txt touch: cannot touch '/mnt/in/example_of_what_not_to_do.txt': Permission denied So if you know how to add users in your Dockerfile, you could in theory make the user inside the image your Carlsbad or Dotter user, but that's a lot of work. I hope that using singularity just works smoothly...Stay tuned for how to use SHUB, Singularities Dockerhub...","title":"Should we use Docker???"},{"location":"notes/sql/mysql/mySQL/","text":"mySQL Basics First, connect to your MySQL database using your MySQL client from your operating system command line: $ mysql -u root -p Next, after you're logged into your MySQL database, tell MySQL which database you want to use: mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | MySQL_DevDB | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.01 sec) mysql> use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +---------------------------+ | Tables_in_mysql | +---------------------------+ | columns_priv | | db | | event | | func | | general_log | | help_category | | help_keyword | | help_relation | | help_topic | | innodb_index_stats | | innodb_table_stats | | ndb_binlog_index | | plugin | | proc | | procs_priv | | proxies_priv | | servers | | slave_master_info | | slave_relay_log_info | | slave_worker_info | | slow_log | | tables_priv | | time_zone | | time_zone_leap_second | | time_zone_name | | time_zone_transition | | time_zone_transition_type | | user | +---------------------------+ 28 rows in set (0.00 sec) Create Database You can now work with the database. For example, the following commands demonstrate how to create a basic table named example , and how to insert some data into it: CREATE TABLE example ( id smallint unsigned not null auto_increment, name varchar(20) not null, constraint pk_example primary key (id) ); INSERT INTO example ( id, name ) VALUES ( null, 'Sample data' ); Drop Database\\Table DROP DATABASE dbname; DROP TABLE tablename; Type \\q to exit the mysql program. New User Courtesty of a2hosting.com To create a database user, type the following command. Replace username with the user you want to create, and replace password with the user's password: GRANT ALL PRIVILEGES ON *.* TO 'username'@'localhost' IDENTIFIED BY 'password'; This command grants the user all permissions. However, you can grant specific permissions to maintain precise control over database access. For example, to explicitly grant the SELECT permission, you would use the following command: GRANT SELECT ON *.* TO 'username'@'localhost' To log in to MySQL as the user you just created, type the following command. Replace username with the name of the user you created in step 3: mysql -u username -p Delete User To view a list of all users, type the following command from the mysql> prompt: SELECT user FROM mysql.user GROUP BY user; To delete a specific user, type the following command from the mysql> prompt. Replace username with the name of the user that you want to delete: DELETE FROM mysql.user WHERE user = 'username'; Using SQL script files Create a file named example.sql and open it in your preferred text edtior. Copy and paste the following text into the file: CREATE DATABASE dbname; USE dbname; CREATE TABLE tablename ( id smallint unsigned not null auto_increment, name varchar(20) not null, constraint pk_example primary key (id) ); INSERT INTO tablename ( id, name ) VALUES ( null, 'Sample data' ); To process the SQL script, type the following command. Replace username with the name of the user you just created: mysql -u username -p < example.sql MySQL Root Password Guide Courtest of a2hosting To reset the root password for MySQL, follow these steps: Log in to your account using SSH. You must runUsing SQL script files the commands in the following steps as the root user. Therefore, you can either log in directly as the root user (which is not recommended for security reasons), or use the su or sudo commands to run the commands as the root user. Stop the MySQL server using the appropriate command for your Linux distribution: For Debian and Ubuntu, type: service mysql stop For CentOS and Fedora, type: service mysqld stop Restart the MySQL server with the \u2014skip-grant-tables option. To do this, type the following command: mysqld_safe --skip-grant-tables & Make sure you type the ampersand ( & ) at the end of the command. This runs the command in the background and allows you to type the commands in the following steps. Running MySQL with the \u2014skip-grant-tables option enabled is highly insecure, and should only be done for a brief period while you reset the password. The steps below show you how to stop the mysqld_safe server instance safely and start the MySQL server securely after you have reset the root password. Log into MySQL using the following command: mysql At the mysql> prompt, reset the password. To do this, type the following command, replacing NEW-PASSWORD with the new root password: UPDATE mysql.user SET Password=PASSWORD('NEW-PASSWORD') WHERE User='root'; At the mysql> prompt, type the following commands: FLUSH PRIVILEGES; exit; Stop the MySQL server using the following command. You will be prompted to enter the new MySQL root password before the MySQL server shuts down: mysqladmin -u root -p shutdown Start the MySQL server normally. To do this, type the appropriate command for your Linux distribution: For Debian and Ubuntu, type: service mysql start For CentOS and Fedora, type: service mysqld start","title":"mySQL"},{"location":"notes/sql/mysql/mySQL/#mysql","text":"","title":"mySQL"},{"location":"notes/sql/mysql/mySQL/#basics","text":"First, connect to your MySQL database using your MySQL client from your operating system command line: $ mysql -u root -p Next, after you're logged into your MySQL database, tell MySQL which database you want to use: mysql> show databases; +--------------------+ | Database | +--------------------+ | information_schema | | MySQL_DevDB | | mysql | | performance_schema | +--------------------+ 4 rows in set (0.01 sec) mysql> use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql> show tables; +---------------------------+ | Tables_in_mysql | +---------------------------+ | columns_priv | | db | | event | | func | | general_log | | help_category | | help_keyword | | help_relation | | help_topic | | innodb_index_stats | | innodb_table_stats | | ndb_binlog_index | | plugin | | proc | | procs_priv | | proxies_priv | | servers | | slave_master_info | | slave_relay_log_info | | slave_worker_info | | slow_log | | tables_priv | | time_zone | | time_zone_leap_second | | time_zone_name | | time_zone_transition | | time_zone_transition_type | | user | +---------------------------+ 28 rows in set (0.00 sec)","title":"Basics"},{"location":"notes/sql/mysql/mySQL/#create-database","text":"You can now work with the database. For example, the following commands demonstrate how to create a basic table named example , and how to insert some data into it: CREATE TABLE example ( id smallint unsigned not null auto_increment, name varchar(20) not null, constraint pk_example primary key (id) ); INSERT INTO example ( id, name ) VALUES ( null, 'Sample data' );","title":"Create Database"},{"location":"notes/sql/mysql/mySQL/#drop-databasetable","text":"DROP DATABASE dbname; DROP TABLE tablename; Type \\q to exit the mysql program.","title":"Drop Database\\Table"},{"location":"notes/sql/mysql/mySQL/#new-user","text":"Courtesty of a2hosting.com To create a database user, type the following command. Replace username with the user you want to create, and replace password with the user's password: GRANT ALL PRIVILEGES ON *.* TO 'username'@'localhost' IDENTIFIED BY 'password'; This command grants the user all permissions. However, you can grant specific permissions to maintain precise control over database access. For example, to explicitly grant the SELECT permission, you would use the following command: GRANT SELECT ON *.* TO 'username'@'localhost' To log in to MySQL as the user you just created, type the following command. Replace username with the name of the user you created in step 3: mysql -u username -p","title":"New User"},{"location":"notes/sql/mysql/mySQL/#delete-user","text":"To view a list of all users, type the following command from the mysql> prompt: SELECT user FROM mysql.user GROUP BY user; To delete a specific user, type the following command from the mysql> prompt. Replace username with the name of the user that you want to delete: DELETE FROM mysql.user WHERE user = 'username';","title":"Delete User"},{"location":"notes/sql/mysql/mySQL/#using-sql-script-files","text":"Create a file named example.sql and open it in your preferred text edtior. Copy and paste the following text into the file: CREATE DATABASE dbname; USE dbname; CREATE TABLE tablename ( id smallint unsigned not null auto_increment, name varchar(20) not null, constraint pk_example primary key (id) ); INSERT INTO tablename ( id, name ) VALUES ( null, 'Sample data' ); To process the SQL script, type the following command. Replace username with the name of the user you just created: mysql -u username -p < example.sql","title":"Using SQL script files"},{"location":"notes/sql/mysql/mySQL/#mysql-root-password-guide","text":"Courtest of a2hosting To reset the root password for MySQL, follow these steps:","title":"MySQL Root Password Guide"},{"location":"notes/sql/mysql/mySQL/#log-in-to-your-account-using-ssh","text":"You must runUsing SQL script files the commands in the following steps as the root user. Therefore, you can either log in directly as the root user (which is not recommended for security reasons), or use the su or sudo commands to run the commands as the root user.","title":"Log in to your account using SSH."},{"location":"notes/sql/mysql/mySQL/#stop-the-mysql-server-using-the-appropriate-command-for-your-linux-distribution","text":"For Debian and Ubuntu, type: service mysql stop For CentOS and Fedora, type: service mysqld stop","title":"Stop the MySQL server using the appropriate command for your Linux distribution:"},{"location":"notes/sql/mysql/mySQL/#restart-the-mysql-server-with-the-skip-grant-tables-option-to-do-this-type-the-following-command","text":"mysqld_safe --skip-grant-tables & Make sure you type the ampersand ( & ) at the end of the command. This runs the command in the background and allows you to type the commands in the following steps. Running MySQL with the \u2014skip-grant-tables option enabled is highly insecure, and should only be done for a brief period while you reset the password. The steps below show you how to stop the mysqld_safe server instance safely and start the MySQL server securely after you have reset the root password.","title":"Restart the MySQL server with the \u2014skip-grant-tables option. To do this, type the following command:"},{"location":"notes/sql/mysql/mySQL/#log-into-mysql-using-the-following-command","text":"mysql At the mysql> prompt, reset the password. To do this, type the following command, replacing NEW-PASSWORD with the new root password: UPDATE mysql.user SET Password=PASSWORD('NEW-PASSWORD') WHERE User='root'; At the mysql> prompt, type the following commands: FLUSH PRIVILEGES; exit; Stop the MySQL server using the following command. You will be prompted to enter the new MySQL root password before the MySQL server shuts down: mysqladmin -u root -p shutdown Start the MySQL server normally. To do this, type the appropriate command for your Linux distribution: For Debian and Ubuntu, type: service mysql start For CentOS and Fedora, type: service mysqld start","title":"Log into MySQL using the following command:"},{"location":"notes/sql/postgres/Postgres/","text":"Postgres Courtesy of Stackoverflow Installation Whoops...I'm sure this is easy...I'll get to it eventually. First Time Use and Setup Here's what worked for postgresql-9.1 on Xubuntu 12.04.1 LTS. Connect to the default database with user postgres : sudo -u postgres psql template1 Set the password for user postgres , then exit psql (Ctrl-D) : postgres=# ALTER USER postgres with encrypted password 'xxxxxxx'; Edit the pg_hba.conf file: sudo vim /etc/postgresql/9.1/main/pg_hba.conf And change peer to md5 on the line concerning postgres : local all postgres peer md5 Note: you need sudo or the file will appear blank Restart the database: postgres=# sudo /etc/init.d/postgresql restart\\ (Here you can check it worked with psql -U postgres .) Create a user having the same name as you (to find it, you can type whoami ) : postgres=# createuser -U postgres -d -e -E -l -P -r -s <my_name> The options tell postgresql to create a user that can login, create databases, create new roles, is a superuser, and will have an encrypted password. The really important ones are -P -E , so that you're asked to type the password that will be encrypted, and -d so that you can do a createdb . Beware of passwords : it will first ask you twice the new password (for the new user), repeated, and then once the postgres password (the one specified on step 2). Again, edit the pg_hba.conf file (see step 3 above), and change peer to md5 on the line concerning \"all\" other users : local all all peer md5 Restart (like in step 4), and check that you can login without -U postgres : psql template1 Note that if you do a mere psql, it will fail since it will try to connect you to a default database having the same name as you (ie. whoami ). template1 is the admin database that is here from the start. Now createdb <dbname> should work. Display all DBs: Use \\list or \\l to display databases. postgres=# \\list List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------------+----------+----------+------------+------------+----------------------- codalab_website | postgres | UTF8 | en_US.utf8 | en_US.utf8 | postgres | postgres | UTF8 | en_US.utf8 | en_US.utf8 | template0 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres + | | | | | postgres=CTc/postgres (4 rows) Display Tables Use \\dt to list all public tables, and \\dt * for all tables in the current DB you are connected to. postgres=# \\dt List of relations Schema | Name | Type | Owner --------+-------------------------------------+-------+------- public | account_emailaddress | table | root public | account_emailconfirmation | table | root public | auth_group | table | root public | auth_group_permissions | table | root public | auth_permission | table | root public | authenz_cluser | table | root public | authenz_cluser_groups | table | root public | authenz_cluser_user_permissions | table | root public | captcha_captchastore | table | root public | coopetitions_dislike | table | root public | coopetitions_downloadrecord | table | root public | coopetitions_like | table | root public | customizer_configuration | table | root public | django_admin_log | table | root ... codalab_website=# \\dt * List of relations Schema | Name | Type | Owner ------------+-------------------------------------+-------+---------- pg_catalog | pg_aggregate | table | postgres pg_catalog | pg_am | table | postgres pg_catalog | pg_amop | table | postgres pg_catalog | pg_amproc | table | postgres pg_catalog | pg_attrdef | table | postgres pg_catalog | pg_attribute | table | postgres pg_catalog | pg_auth_members | table | postgres pg_catalog | pg_authid | table | postgres pg_catalog | pg_cast | table | postgres pg_catalog | pg_class | table | postgres ... Display Schema Source objectrocket SELECT column_name FROM information_schema.columns WHERE TABLE_NAME = 'some_table'; Display Connection Information codalab_website-# \\conninfo You are connected to database \"codalab_website\" as user \"root\" via socket in \"/var/run/postgresql\" at port \"5432\". You will never see tables in other databases, these tables aren't visible. You have to connect to the correct database to see its tables (and other objects). Switch DBs: To switch databases: postgres=# \\connect database_name Selects all non-template DBs: postgres=# SELECT datname FROM pg_database WHERE datistemplate = false; Select all tables in current DB connection postgres=# SELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name; Delete Database: postgres=# DROP DATABASE [ IF EXISTS ] name Create Database: postgres=# CREATE DATABASE testdb; Create User: See all current users: postgres=# SELECT usename FROM pg_user; Create New User: Bash: $ createuser name or Postgres: postgres=# CREATE USER testuser; Drop User: Bash: $ dropuser name or Postgres: postgres=# DROP USER testuser; Bulk Insert: https://stackoverflow.com/questions/12856377/the-correct-copy-command-to-load-postgresql-data-from-csv-file-that-has-single-q Double single quotes (if standard_conforming_strings is on, see the docs) COPY my_table FROM 'c:\\downloads\\file.csv' DELIMITERS ',' CSV QUOTE ''''; or use the non-standard PostgreSQL-specific escape string: COPY my_table FROM 'c:\\downloads\\file.csv' DELIMITERS ',' CSV QUOTE E'\\''; If you have a header: COPY my_table FROM 'c:\\downloads\\file.csv' WITH DELIMITER ',' CSV HEADER; use \\copy in bash","title":"Postgres"},{"location":"notes/sql/postgres/Postgres/#postgres","text":"Courtesy of Stackoverflow","title":"Postgres"},{"location":"notes/sql/postgres/Postgres/#installation","text":"Whoops...I'm sure this is easy...I'll get to it eventually.","title":"Installation"},{"location":"notes/sql/postgres/Postgres/#first-time-use-and-setup","text":"Here's what worked for postgresql-9.1 on Xubuntu 12.04.1 LTS. Connect to the default database with user postgres : sudo -u postgres psql template1 Set the password for user postgres , then exit psql (Ctrl-D) : postgres=# ALTER USER postgres with encrypted password 'xxxxxxx'; Edit the pg_hba.conf file: sudo vim /etc/postgresql/9.1/main/pg_hba.conf And change peer to md5 on the line concerning postgres : local all postgres peer md5 Note: you need sudo or the file will appear blank Restart the database: postgres=# sudo /etc/init.d/postgresql restart\\ (Here you can check it worked with psql -U postgres .) Create a user having the same name as you (to find it, you can type whoami ) : postgres=# createuser -U postgres -d -e -E -l -P -r -s <my_name> The options tell postgresql to create a user that can login, create databases, create new roles, is a superuser, and will have an encrypted password. The really important ones are -P -E , so that you're asked to type the password that will be encrypted, and -d so that you can do a createdb . Beware of passwords : it will first ask you twice the new password (for the new user), repeated, and then once the postgres password (the one specified on step 2). Again, edit the pg_hba.conf file (see step 3 above), and change peer to md5 on the line concerning \"all\" other users : local all all peer md5 Restart (like in step 4), and check that you can login without -U postgres : psql template1 Note that if you do a mere psql, it will fail since it will try to connect you to a default database having the same name as you (ie. whoami ). template1 is the admin database that is here from the start. Now createdb <dbname> should work.","title":"First Time Use and Setup"},{"location":"notes/sql/postgres/Postgres/#display-all-dbs","text":"Use \\list or \\l to display databases. postgres=# \\list List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------------+----------+----------+------------+------------+----------------------- codalab_website | postgres | UTF8 | en_US.utf8 | en_US.utf8 | postgres | postgres | UTF8 | en_US.utf8 | en_US.utf8 | template0 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | en_US.utf8 | en_US.utf8 | =c/postgres + | | | | | postgres=CTc/postgres (4 rows)","title":"Display all DBs:"},{"location":"notes/sql/postgres/Postgres/#display-tables","text":"Use \\dt to list all public tables, and \\dt * for all tables in the current DB you are connected to. postgres=# \\dt List of relations Schema | Name | Type | Owner --------+-------------------------------------+-------+------- public | account_emailaddress | table | root public | account_emailconfirmation | table | root public | auth_group | table | root public | auth_group_permissions | table | root public | auth_permission | table | root public | authenz_cluser | table | root public | authenz_cluser_groups | table | root public | authenz_cluser_user_permissions | table | root public | captcha_captchastore | table | root public | coopetitions_dislike | table | root public | coopetitions_downloadrecord | table | root public | coopetitions_like | table | root public | customizer_configuration | table | root public | django_admin_log | table | root ... codalab_website=# \\dt * List of relations Schema | Name | Type | Owner ------------+-------------------------------------+-------+---------- pg_catalog | pg_aggregate | table | postgres pg_catalog | pg_am | table | postgres pg_catalog | pg_amop | table | postgres pg_catalog | pg_amproc | table | postgres pg_catalog | pg_attrdef | table | postgres pg_catalog | pg_attribute | table | postgres pg_catalog | pg_auth_members | table | postgres pg_catalog | pg_authid | table | postgres pg_catalog | pg_cast | table | postgres pg_catalog | pg_class | table | postgres ...","title":"Display Tables"},{"location":"notes/sql/postgres/Postgres/#display-schema","text":"Source objectrocket SELECT column_name FROM information_schema.columns WHERE TABLE_NAME = 'some_table';","title":"Display Schema"},{"location":"notes/sql/postgres/Postgres/#display-connection-information","text":"codalab_website-# \\conninfo You are connected to database \"codalab_website\" as user \"root\" via socket in \"/var/run/postgresql\" at port \"5432\". You will never see tables in other databases, these tables aren't visible. You have to connect to the correct database to see its tables (and other objects).","title":"Display Connection Information"},{"location":"notes/sql/postgres/Postgres/#switch-dbs","text":"To switch databases: postgres=# \\connect database_name Selects all non-template DBs: postgres=# SELECT datname FROM pg_database WHERE datistemplate = false; Select all tables in current DB connection postgres=# SELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name;","title":"Switch DBs:"},{"location":"notes/sql/postgres/Postgres/#delete-database","text":"postgres=# DROP DATABASE [ IF EXISTS ] name","title":"Delete Database:"},{"location":"notes/sql/postgres/Postgres/#create-database","text":"postgres=# CREATE DATABASE testdb;","title":"Create Database:"},{"location":"notes/sql/postgres/Postgres/#create-user","text":"","title":"Create User:"},{"location":"notes/sql/postgres/Postgres/#see-all-current-users","text":"postgres=# SELECT usename FROM pg_user;","title":"See all current users:"},{"location":"notes/sql/postgres/Postgres/#create-new-user","text":"Bash: $ createuser name or Postgres: postgres=# CREATE USER testuser;","title":"Create New User:"},{"location":"notes/sql/postgres/Postgres/#drop-user","text":"Bash: $ dropuser name or Postgres: postgres=# DROP USER testuser;","title":"Drop User:"},{"location":"notes/sql/postgres/Postgres/#bulk-insert","text":"https://stackoverflow.com/questions/12856377/the-correct-copy-command-to-load-postgresql-data-from-csv-file-that-has-single-q Double single quotes (if standard_conforming_strings is on, see the docs) COPY my_table FROM 'c:\\downloads\\file.csv' DELIMITERS ',' CSV QUOTE ''''; or use the non-standard PostgreSQL-specific escape string: COPY my_table FROM 'c:\\downloads\\file.csv' DELIMITERS ',' CSV QUOTE E'\\''; If you have a header: COPY my_table FROM 'c:\\downloads\\file.csv' WITH DELIMITER ',' CSV HEADER; use \\copy in bash","title":"Bulk Insert:"},{"location":"notes/sql/postgres/pgadmin/","text":"pgAdmin Running the program Navigate to: /home/bbearce/pgAdmin4/pgAdmin4 note: pgAdmin4/pgAdmin4 is a virtual environment(venv). Activate venv: $ . bin/activate Run pgAdmin: $ python lib/python2.7/site-packages/pgadmin4/pgAdmin4.py","title":"pgAdmin"},{"location":"notes/sql/postgres/pgadmin/#pgadmin","text":"","title":"pgAdmin"},{"location":"notes/sql/postgres/pgadmin/#running-the-program","text":"Navigate to: /home/bbearce/pgAdmin4/pgAdmin4 note: pgAdmin4/pgAdmin4 is a virtual environment(venv). Activate venv: $ . bin/activate Run pgAdmin: $ python lib/python2.7/site-packages/pgadmin4/pgAdmin4.py","title":"Running the program"},{"location":"notes/sql/sql_server/Sql%20Server/","text":"SQL Server Testing the addition of a new language SELECT TOP 10000 sum(colC) FROM DB_Server.dbo.DB_Table WHERE colA = 'some value' GROUP BY colB","title":"SQL server"},{"location":"notes/sql/sql_server/Sql%20Server/#sql-server","text":"Testing the addition of a new language SELECT TOP 10000 sum(colC) FROM DB_Server.dbo.DB_Table WHERE colA = 'some value' GROUP BY colB","title":"SQL Server"},{"location":"notes/sql/sqlite/Basics/","text":"Sqlite create database Syntax Following is the basic syntax of sqlite3 command to create a database: $sqlite3 DatabaseName.db Always, database name should be unique within the RDBMS. Example If you want to create a new database , then SQLITE3 statement would be as follows \u2212 $sqlite3 testDB.db SQLite version 3.7.15.2 2013-01-09 11:53:05 Enter \".help\" for instructions Enter SQL statements terminated with a \";\" sqlite> The above command will create a file testDB.db in the current directory. This file will be used as database by SQLite engine. If you have noticed while creating database, sqlite3 command will provide a sqlite> prompt after creating a database file successfully. Once a database is created, you can verify it in the list of databases using the following SQLite .databases command. sqlite>.databases seq name file --- --------------- ---------------------- 0 main /home/sqlite/testDB.db You will use SQLite .quit command to come out of the sqlite prompt as follows: sqlite>.quit $ The .dump Command You can use .dump dot command to export complete database in a text file using the following SQLite command at the command prompt. $sqlite3 testDB.db .dump > testDB.sql The above command will convert the entire contents of testDB.db database into SQLite statements and dump it into ASCII text file testDB.sql. You can perform restoration from the generated testDB.sql in a simple way as follows: $sqlite3 testDB.db < testDB.sql At this moment your database is empty, so you can try above two procedures once you have few tables and data in your database. For now, let's proceed to the next chapter. .tables The following notes are from sqlitetutorial.net List tables after connecting using .tables $ sqlite3 kaggle_rsna SQLite version 3.11.0 2016-02-15 17:29:24 Enter \".help\" for usage hints. sqlite> .tables all_images final_annotation_list all_images_corrected remove_exams annotations remove_series annotations_corrected study_and_instance_annotation_ids The .tables command also can be used to show temporary tables. See the following example: First, create a new temporary table named temp_table1: sqlite> CREATE TEMPORARY TABLE temp_table1( name TEXT ); Second, list all tables from the database: sqlite> .tables The following shows the output: albums employees invoices playlists artists genres media_types temp.temp_table1 customers invoice_items playlist_track tracks Because the schema of temporary tables is temp, the command showed the names of schema and table of the temporary table such as temp.temp_table1 . If you want to show tables with the specific name, you can add a matching pattern: .tables pattern The command works the same as LIKE operator. The pattern must be surrounded by single quotation marks ('). For example, to find tables whose names start with the letter \u2018a\u2019, you use the following command: sqlite> .table 'a%' Here is the output: albums artists To shows the tables whose name contains the string ck, you use the %ck% pattern as shown in the following command: sqlite> .tables '%ck%' The output is as follows: playlist_track tracks Showing tables using SQL statement: Another way to list all tables in a database is to query them from the sqlite_master table. SELECT name FROM sqlite_master WHERE type ='table' AND name NOT LIKE 'sqlite_%'; Here is the output: name albums customers employees genres invoices invoice_items media_types playlists playlist_track tracks .schema / create table View the schema of a particular table: sqlite> .schema table sqlite> .schema all_images CREATE TABLE all_images( \"InstanceID\" TEXT, \"SeriesID\" TEXT, \"StudyID\" TEXT ); bulk insert If the table doesn't exist, sqlite will try to create it and it's scheme assuming a header. .mode csv assumes a ',' separator. sqlite> .mode csv <table_name> sqlite> .import <path_to_csv> <table_name> if you want to specify a different separator use this instead (I used a specific csv for this...): sqlite> .separator , sqlite> .import <path_to_csv> <table_name> sqlite> .schema CREATE TABLE measurements( \"seriesUID\" TEXT, \"instanceUID\" TEXT, \"length\" TEXT, \"start_x\" TEXT, \"start_y\" TEXT, \"end_x\" TEXT, \"end_y\" TEXT, \"annotator\" TEXT ); I noticed that it was lazy about assigning data types. csv export An example export from the rad table. Any query can be used as the basis for the export, including multi-line complex ones. sqlite> .headers on sqlite> .mode csv sqlite> .output rad_data.csv sqlite> select * from rad; sqlite>","title":"Basics"},{"location":"notes/sql/sqlite/Basics/#sqlite","text":"","title":"Sqlite"},{"location":"notes/sql/sqlite/Basics/#create-database","text":"Syntax Following is the basic syntax of sqlite3 command to create a database: $sqlite3 DatabaseName.db Always, database name should be unique within the RDBMS. Example If you want to create a new database , then SQLITE3 statement would be as follows \u2212 $sqlite3 testDB.db SQLite version 3.7.15.2 2013-01-09 11:53:05 Enter \".help\" for instructions Enter SQL statements terminated with a \";\" sqlite> The above command will create a file testDB.db in the current directory. This file will be used as database by SQLite engine. If you have noticed while creating database, sqlite3 command will provide a sqlite> prompt after creating a database file successfully. Once a database is created, you can verify it in the list of databases using the following SQLite .databases command. sqlite>.databases seq name file --- --------------- ---------------------- 0 main /home/sqlite/testDB.db You will use SQLite .quit command to come out of the sqlite prompt as follows: sqlite>.quit $","title":"create database"},{"location":"notes/sql/sqlite/Basics/#the-dump-command","text":"You can use .dump dot command to export complete database in a text file using the following SQLite command at the command prompt. $sqlite3 testDB.db .dump > testDB.sql The above command will convert the entire contents of testDB.db database into SQLite statements and dump it into ASCII text file testDB.sql. You can perform restoration from the generated testDB.sql in a simple way as follows: $sqlite3 testDB.db < testDB.sql At this moment your database is empty, so you can try above two procedures once you have few tables and data in your database. For now, let's proceed to the next chapter.","title":"The .dump Command"},{"location":"notes/sql/sqlite/Basics/#tables","text":"The following notes are from sqlitetutorial.net List tables after connecting using .tables $ sqlite3 kaggle_rsna SQLite version 3.11.0 2016-02-15 17:29:24 Enter \".help\" for usage hints. sqlite> .tables all_images final_annotation_list all_images_corrected remove_exams annotations remove_series annotations_corrected study_and_instance_annotation_ids The .tables command also can be used to show temporary tables. See the following example: First, create a new temporary table named temp_table1: sqlite> CREATE TEMPORARY TABLE temp_table1( name TEXT ); Second, list all tables from the database: sqlite> .tables The following shows the output: albums employees invoices playlists artists genres media_types temp.temp_table1 customers invoice_items playlist_track tracks Because the schema of temporary tables is temp, the command showed the names of schema and table of the temporary table such as temp.temp_table1 . If you want to show tables with the specific name, you can add a matching pattern: .tables pattern The command works the same as LIKE operator. The pattern must be surrounded by single quotation marks ('). For example, to find tables whose names start with the letter \u2018a\u2019, you use the following command: sqlite> .table 'a%' Here is the output: albums artists To shows the tables whose name contains the string ck, you use the %ck% pattern as shown in the following command: sqlite> .tables '%ck%' The output is as follows: playlist_track tracks Showing tables using SQL statement: Another way to list all tables in a database is to query them from the sqlite_master table. SELECT name FROM sqlite_master WHERE type ='table' AND name NOT LIKE 'sqlite_%'; Here is the output: name albums customers employees genres invoices invoice_items media_types playlists playlist_track tracks","title":".tables"},{"location":"notes/sql/sqlite/Basics/#schema-create-table","text":"View the schema of a particular table: sqlite> .schema table sqlite> .schema all_images CREATE TABLE all_images( \"InstanceID\" TEXT, \"SeriesID\" TEXT, \"StudyID\" TEXT );","title":".schema / create table"},{"location":"notes/sql/sqlite/Basics/#bulk-insert","text":"If the table doesn't exist, sqlite will try to create it and it's scheme assuming a header. .mode csv assumes a ',' separator. sqlite> .mode csv <table_name> sqlite> .import <path_to_csv> <table_name> if you want to specify a different separator use this instead (I used a specific csv for this...): sqlite> .separator , sqlite> .import <path_to_csv> <table_name> sqlite> .schema CREATE TABLE measurements( \"seriesUID\" TEXT, \"instanceUID\" TEXT, \"length\" TEXT, \"start_x\" TEXT, \"start_y\" TEXT, \"end_x\" TEXT, \"end_y\" TEXT, \"annotator\" TEXT ); I noticed that it was lazy about assigning data types.","title":"bulk insert"},{"location":"notes/sql/sqlite/Basics/#csv-export","text":"An example export from the rad table. Any query can be used as the basis for the export, including multi-line complex ones. sqlite> .headers on sqlite> .mode csv sqlite> .output rad_data.csv sqlite> select * from rad; sqlite>","title":"csv export"},{"location":"notes/sql/sqlite/installing/","text":"Installing Courtest of linuxfromscratch Introduction to SQLite The SQLite package is a software library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. This package is known to build and work properly using an LFS-9.0 platform. Package Information Download (HTTP): https://sqlite.org/2019/sqlite-autoconf-3300100.tar.gz Download MD5 sum: 51252dc6bc9094ba11ab151ba650ff3c Download size: 2.7 MB Estimated disk space required: 73 MB Estimated build time: 0.4 SBU (Using parallelism=4) Additional Downloads Optional Documentation Download (HTTP): https://sqlite.org/2019/sqlite-doc-3300100.zip Download MD5 sum: 0a631f0f293167c82be0c10831642469 Download size: 9.1 MB Installation of SQLite If you downloaded the optional documentation, issue the following command to install the documentation into the source tree: unzip -q ../sqlite-doc-3300100.zip Install SQLite by running the following commands: before that unzip with tar zxvf sqlite-autoconf-3300100.tar.gz and cd sqlite-autoconf-3300100 ./configure --prefix=/usr \\ --disable-static \\ --enable-fts5 \\ CFLAGS=\"-g -O2 \\ -DSQLITE_ENABLE_FTS3=1 \\ -DSQLITE_ENABLE_FTS4=1 \\ -DSQLITE_ENABLE_COLUMN_METADATA=1 \\ -DSQLITE_ENABLE_UNLOCK_NOTIFY=1 \\ -DSQLITE_ENABLE_DBSTAT_VTAB=1 \\ -DSQLITE_SECURE_DELETE=1 \\ -DSQLITE_ENABLE_FTS3_TOKENIZER=1\" && make This package does not come with a test suite. Now, as the root user run sudo make install : bbearce@bbearce-XPS-15-9560:~/Downloads/sqlite-autoconf-3300100$ sudo make install make[1]: Entering directory '/home/bbearce/Downloads/sqlite-autoconf-3300100' /bin/mkdir -p '/usr/lib' /bin/bash ./libtool --mode=install /usr/bin/install -c libsqlite3.la '/usr/lib' libtool: install: /usr/bin/install -c .libs/libsqlite3.so.0.8.6 /usr/lib/libsqlite3.so.0.8.6 libtool: install: (cd /usr/lib && { ln -s -f libsqlite3.so.0.8.6 libsqlite3.so.0 || { rm -f libsqlite3.so.0 && ln -s libsqlite3.so.0.8.6 libsqlite3.so.0; }; }) libtool: install: (cd /usr/lib && { ln -s -f libsqlite3.so.0.8.6 libsqlite3.so || { rm -f libsqlite3.so && ln -s libsqlite3.so.0.8.6 libsqlite3.so; }; }) libtool: install: /usr/bin/install -c .libs/libsqlite3.lai /usr/lib/libsqlite3.la libtool: install: /usr/bin/install -c .libs/libsqlite3.a /usr/lib/libsqlite3.a libtool: install: chmod 644 /usr/lib/libsqlite3.a libtool: install: ranlib /usr/lib/libsqlite3.a libtool: warning: remember to run 'libtool --finish /usr/local/lib' /bin/mkdir -p '/usr/bin' /bin/bash ./libtool --mode=install /usr/bin/install -c sqlite3 '/usr/bin' libtool: install: /usr/bin/install -c sqlite3 /usr/bin/sqlite3 /bin/mkdir -p '/usr/include' /usr/bin/install -c -m 644 sqlite3.h sqlite3ext.h '/usr/include' /bin/mkdir -p '/usr/share/man/man1' /usr/bin/install -c -m 644 sqlite3.1 '/usr/share/man/man1' /bin/mkdir -p '/usr/lib/pkgconfig' /usr/bin/install -c -m 644 sqlite3.pc '/usr/lib/pkgconfig' make[1]: Leaving directory '/home/bbearce/Downloads/sqlite-autoconf-3300100' Not sure what warning: remember to run 'libtool --finish /usr/local/lib' is for but I ran it and it didn't break anything and output this message: bbearce@bbearce-XPS-15-9560:~/Downloads/sqlite-autoconf-3300100$ libtool --finish /usr/local/lib libtool: finish: PATH=\"/home/bbearce/gems/bin:/home/bbearce/bin:/home/bbearce/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/sbin\" ldconfig -n /usr/local/lib ---------------------------------------------------------------------- Libraries have been installed in: /usr/local/lib If you ever happen to want to link against installed libraries in a given directory, LIBDIR, you must either use libtool, and specify the full pathname of the library, or use the '-LLIBDIR' flag during linking and do at least one of the following: - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable during execution - add LIBDIR to the 'LD_RUN_PATH' environment variable during linking - use the '-Wl,-rpath -Wl,LIBDIR' linker flag - have your system administrator add LIBDIR to '/etc/ld.so.conf' See any operating system documentation about shared libraries for more information, such as the ld(1) and ld.so(8) manual pages. ---------------------------------------------------------------------- ...a little background from gnu : GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. If you downloaded the optional documentation, issue the following commands as the root user to install it: install -v -m755 -d /usr/share/doc/sqlite-3.30.1 && cp -v -R sqlite-doc-3300100/* /usr/share/doc/sqlite-3.30.1","title":"Installing"},{"location":"notes/sql/sqlite/installing/#installing","text":"Courtest of linuxfromscratch","title":"Installing"},{"location":"notes/sql/sqlite/installing/#introduction-to-sqlite","text":"The SQLite package is a software library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. This package is known to build and work properly using an LFS-9.0 platform.","title":"Introduction to SQLite"},{"location":"notes/sql/sqlite/installing/#package-information","text":"Download (HTTP): https://sqlite.org/2019/sqlite-autoconf-3300100.tar.gz Download MD5 sum: 51252dc6bc9094ba11ab151ba650ff3c Download size: 2.7 MB Estimated disk space required: 73 MB Estimated build time: 0.4 SBU (Using parallelism=4)","title":"Package Information"},{"location":"notes/sql/sqlite/installing/#additional-downloads","text":"","title":"Additional Downloads"},{"location":"notes/sql/sqlite/installing/#optional-documentation","text":"Download (HTTP): https://sqlite.org/2019/sqlite-doc-3300100.zip Download MD5 sum: 0a631f0f293167c82be0c10831642469 Download size: 9.1 MB","title":"Optional Documentation"},{"location":"notes/sql/sqlite/installing/#installation-of-sqlite","text":"If you downloaded the optional documentation, issue the following command to install the documentation into the source tree: unzip -q ../sqlite-doc-3300100.zip Install SQLite by running the following commands: before that unzip with tar zxvf sqlite-autoconf-3300100.tar.gz and cd sqlite-autoconf-3300100 ./configure --prefix=/usr \\ --disable-static \\ --enable-fts5 \\ CFLAGS=\"-g -O2 \\ -DSQLITE_ENABLE_FTS3=1 \\ -DSQLITE_ENABLE_FTS4=1 \\ -DSQLITE_ENABLE_COLUMN_METADATA=1 \\ -DSQLITE_ENABLE_UNLOCK_NOTIFY=1 \\ -DSQLITE_ENABLE_DBSTAT_VTAB=1 \\ -DSQLITE_SECURE_DELETE=1 \\ -DSQLITE_ENABLE_FTS3_TOKENIZER=1\" && make This package does not come with a test suite. Now, as the root user run sudo make install : bbearce@bbearce-XPS-15-9560:~/Downloads/sqlite-autoconf-3300100$ sudo make install make[1]: Entering directory '/home/bbearce/Downloads/sqlite-autoconf-3300100' /bin/mkdir -p '/usr/lib' /bin/bash ./libtool --mode=install /usr/bin/install -c libsqlite3.la '/usr/lib' libtool: install: /usr/bin/install -c .libs/libsqlite3.so.0.8.6 /usr/lib/libsqlite3.so.0.8.6 libtool: install: (cd /usr/lib && { ln -s -f libsqlite3.so.0.8.6 libsqlite3.so.0 || { rm -f libsqlite3.so.0 && ln -s libsqlite3.so.0.8.6 libsqlite3.so.0; }; }) libtool: install: (cd /usr/lib && { ln -s -f libsqlite3.so.0.8.6 libsqlite3.so || { rm -f libsqlite3.so && ln -s libsqlite3.so.0.8.6 libsqlite3.so; }; }) libtool: install: /usr/bin/install -c .libs/libsqlite3.lai /usr/lib/libsqlite3.la libtool: install: /usr/bin/install -c .libs/libsqlite3.a /usr/lib/libsqlite3.a libtool: install: chmod 644 /usr/lib/libsqlite3.a libtool: install: ranlib /usr/lib/libsqlite3.a libtool: warning: remember to run 'libtool --finish /usr/local/lib' /bin/mkdir -p '/usr/bin' /bin/bash ./libtool --mode=install /usr/bin/install -c sqlite3 '/usr/bin' libtool: install: /usr/bin/install -c sqlite3 /usr/bin/sqlite3 /bin/mkdir -p '/usr/include' /usr/bin/install -c -m 644 sqlite3.h sqlite3ext.h '/usr/include' /bin/mkdir -p '/usr/share/man/man1' /usr/bin/install -c -m 644 sqlite3.1 '/usr/share/man/man1' /bin/mkdir -p '/usr/lib/pkgconfig' /usr/bin/install -c -m 644 sqlite3.pc '/usr/lib/pkgconfig' make[1]: Leaving directory '/home/bbearce/Downloads/sqlite-autoconf-3300100' Not sure what warning: remember to run 'libtool --finish /usr/local/lib' is for but I ran it and it didn't break anything and output this message: bbearce@bbearce-XPS-15-9560:~/Downloads/sqlite-autoconf-3300100$ libtool --finish /usr/local/lib libtool: finish: PATH=\"/home/bbearce/gems/bin:/home/bbearce/bin:/home/bbearce/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/sbin\" ldconfig -n /usr/local/lib ---------------------------------------------------------------------- Libraries have been installed in: /usr/local/lib If you ever happen to want to link against installed libraries in a given directory, LIBDIR, you must either use libtool, and specify the full pathname of the library, or use the '-LLIBDIR' flag during linking and do at least one of the following: - add LIBDIR to the 'LD_LIBRARY_PATH' environment variable during execution - add LIBDIR to the 'LD_RUN_PATH' environment variable during linking - use the '-Wl,-rpath -Wl,LIBDIR' linker flag - have your system administrator add LIBDIR to '/etc/ld.so.conf' See any operating system documentation about shared libraries for more information, such as the ld(1) and ld.so(8) manual pages. ---------------------------------------------------------------------- ...a little background from gnu : GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. If you downloaded the optional documentation, issue the following commands as the root user to install it: install -v -m755 -d /usr/share/doc/sqlite-3.30.1 && cp -v -R sqlite-doc-3300100/* /usr/share/doc/sqlite-3.30.1","title":"Installation of SQLite"},{"location":"notes/sql/sqlite/useful_everyday_useage/","text":"Useful Everyday Code Drop If Exists DROP TABLE [IF EXISTS] [schema_name.]table_name; Select Into source stackoverflow CREATE TABLE equipments_backup AS SELECT * FROM equipments Ranks / Row_Number Courtest of sqlitetutorial Window function support was first added to SQLite with release version 3.25.0 (2018-09-15). The SQLite developers used the PostgreSQL window function documentation as their primary reference for how window functions ought to behave. The RANK() function is a window function that assigns a rank to each row in a query\u2019s result set. The rank of a row is calculated by one plus the number of ranks that comes before it. The following shows the syntax of the RANK() function: RANK() OVER ( PARTITION BY <expression1>[{,<expression2>...}] ORDER BY <expression1> [ASC|DESC], [{,<expression1>...}] ) Courtesty of sqlite SQLite supports the following 11 built-in window functions: row_number() The number of the row within the current partition. Rows are numbered starting from 1 in the order defined by the ORDER BY clause in the window definition, or in arbitrary order otherwise. rank() The row_number() of the first peer in each group - the rank of the current row with gaps. If there is no ORDER BY clause, then all rows are considered peers and this function always returns 1. dense_rank() The number of the current row's peer group within its partition - the rank of the current row without gaps. Partitions are numbered starting from 1 in the order defined by the ORDER BY clause in the window definition. If there is no ORDER BY clause, then all rows are considered peers and this function always returns 1. percent_rank() Despite the name, this function always returns a value between 0.0 and 1.0 equal to (rank - 1)/(partition-rows - 1), where rank is the value returned by built-in window function rank() and partition-rows is the total number of rows in the partition. If the partition contains only one row, this function returns 0.0. cume_dist() The cumulative distribution. Calculated as row-number/partition-rows, where row-number is the value returned by row_number() for the last peer in the group and partition-rows the number of rows in the partition. ntile(N) Argument N is handled as an integer. This function divides the partition into N groups as evenly as possible and assigns an integer between 1 and N to each group, in the order defined by the ORDER BY clause, or in arbitrary order otherwise. If necessary, larger groups occur first. This function returns the integer value assigned to the group that the current row is a part of. Add/Change Column Data Type Source techonthenet Add Column The syntax to ADD A COLUMN in a table in SQLite (using the ALTER TABLE statement) is: ALTER TABLE table_name ADD new_column_name column_definition; Change Column You can not use the ALTER TABLE statement to modify a column in SQLite. Instead you will need to rename the table, create a new table, and copy the data into the new table. Syntax The syntax to MODIFY A COLUMN in a table in SQLite is: PRAGMA foreign_keys=off; BEGIN TRANSACTION; ALTER TABLE table1 RENAME TO _table1_old; CREATE TABLE table1 ( ( column1 datatype [ NULL | NOT NULL ], column2 datatype [ NULL | NOT NULL ], ... ); INSERT INTO table1 (column1, column2, ... column_n) SELECT column1, column2, ... column_n FROM _table1_old; COMMIT; PRAGMA foreign_keys=on; Pivot Table Courtesty of modern-sql Essentially what you need to do is a subquery in which the inner query has the data you want to source and pivot. The outer query will group by a key and explicitly define the columns that should be created from values in a particular row. Use FILTER to filter for the rows that will belong to each column. Note: This should be replaced by CASE statements in the future. SELECT year , SUM(revenue) FILTER (WHERE month = 1) jan_revenue , SUM(revenue) FILTER (WHERE month = 2) feb_revenue ... , SUM(revenue) FILTER (WHERE month = 12) dec_revenue FROM (SELECT invoices.* , EXTRACT(YEAR FROM invoice_date) year , EXTRACT(MONTH FROM invoice_date) month FROM invoices ) invoices GROUP BY year Julian Day Source techonthenet Syntax: The syntax for the julianday function in SQLite is: julianday(timestring [, modifier1, modifier2, ... modifier_n ] ) More concrete example: select mrn, date, julianday(date) as j_day from mrns_dates; output: # mrn date j_day 1 ####### 2011-04-19 2455670.5 2 ####### 2011-09-26 2455830.5 3 ####### 2012-02-01 2455958.5 4 ####### 2012-03-19 2456005.5 5 ####### 2012-04-24 2456041.5 6 ####### 2014-07-30 2456868.5 Notes: A date value. It can be one of the following: timestring Explanation now now is a literal used to return the current date YYYY-MM-DD Date value formatted as 'YYYY-MM-DD' YYYY-MM-DD HH:MM Date value formatted as 'YYYY-MM-DD HH:MM' YYYY-MM-DD HH:MM:SS Date value formatted as 'YYYY-MM-DD HH:MM:SS' YYYY-MM-DD HH:MM:SS.SSS Date value formatted as 'YYYY-MM-DD HH:MM:SS.SSS' HH:MM Date value formatted as 'HH:MM' HH:MM:SS Date value formatted as 'HH:MM:SS' HH:MM:SS.SSS Date value formatted as 'HH:MM:SS.SSS' YYYY-MM-DDTHH:MM Date value formatted as 'YYYY-MM-DDTHH:MM' where T is a literal character separating the date and time portions YYYY-MM-DDTHH:MM:SS Date value formatted as 'YYYY-MM-DDTHH:MM:SS' where T is a literal character separating the date and time portions YYYY-MM-DDTHH:MM:SS.SSS Date value formatted as 'YYYY-MM-DDTHH:MM:SS.SSS' where T is a literal character separating the date and time portions DDDDDDDDDD Julian date number","title":"Useful Code"},{"location":"notes/sql/sqlite/useful_everyday_useage/#useful-everyday-code","text":"","title":"Useful Everyday Code"},{"location":"notes/sql/sqlite/useful_everyday_useage/#drop-if-exists","text":"DROP TABLE [IF EXISTS] [schema_name.]table_name;","title":"Drop If Exists"},{"location":"notes/sql/sqlite/useful_everyday_useage/#select-into","text":"source stackoverflow CREATE TABLE equipments_backup AS SELECT * FROM equipments","title":"Select Into"},{"location":"notes/sql/sqlite/useful_everyday_useage/#ranks-row_number","text":"Courtest of sqlitetutorial Window function support was first added to SQLite with release version 3.25.0 (2018-09-15). The SQLite developers used the PostgreSQL window function documentation as their primary reference for how window functions ought to behave. The RANK() function is a window function that assigns a rank to each row in a query\u2019s result set. The rank of a row is calculated by one plus the number of ranks that comes before it. The following shows the syntax of the RANK() function: RANK() OVER ( PARTITION BY <expression1>[{,<expression2>...}] ORDER BY <expression1> [ASC|DESC], [{,<expression1>...}] ) Courtesty of sqlite SQLite supports the following 11 built-in window functions: row_number() The number of the row within the current partition. Rows are numbered starting from 1 in the order defined by the ORDER BY clause in the window definition, or in arbitrary order otherwise. rank() The row_number() of the first peer in each group - the rank of the current row with gaps. If there is no ORDER BY clause, then all rows are considered peers and this function always returns 1. dense_rank() The number of the current row's peer group within its partition - the rank of the current row without gaps. Partitions are numbered starting from 1 in the order defined by the ORDER BY clause in the window definition. If there is no ORDER BY clause, then all rows are considered peers and this function always returns 1. percent_rank() Despite the name, this function always returns a value between 0.0 and 1.0 equal to (rank - 1)/(partition-rows - 1), where rank is the value returned by built-in window function rank() and partition-rows is the total number of rows in the partition. If the partition contains only one row, this function returns 0.0. cume_dist() The cumulative distribution. Calculated as row-number/partition-rows, where row-number is the value returned by row_number() for the last peer in the group and partition-rows the number of rows in the partition. ntile(N) Argument N is handled as an integer. This function divides the partition into N groups as evenly as possible and assigns an integer between 1 and N to each group, in the order defined by the ORDER BY clause, or in arbitrary order otherwise. If necessary, larger groups occur first. This function returns the integer value assigned to the group that the current row is a part of.","title":"Ranks / Row_Number"},{"location":"notes/sql/sqlite/useful_everyday_useage/#addchange-column-data-type","text":"Source techonthenet","title":"Add/Change Column Data Type"},{"location":"notes/sql/sqlite/useful_everyday_useage/#add-column","text":"The syntax to ADD A COLUMN in a table in SQLite (using the ALTER TABLE statement) is: ALTER TABLE table_name ADD new_column_name column_definition;","title":"Add Column"},{"location":"notes/sql/sqlite/useful_everyday_useage/#change-column","text":"You can not use the ALTER TABLE statement to modify a column in SQLite. Instead you will need to rename the table, create a new table, and copy the data into the new table. Syntax The syntax to MODIFY A COLUMN in a table in SQLite is: PRAGMA foreign_keys=off; BEGIN TRANSACTION; ALTER TABLE table1 RENAME TO _table1_old; CREATE TABLE table1 ( ( column1 datatype [ NULL | NOT NULL ], column2 datatype [ NULL | NOT NULL ], ... ); INSERT INTO table1 (column1, column2, ... column_n) SELECT column1, column2, ... column_n FROM _table1_old; COMMIT; PRAGMA foreign_keys=on;","title":"Change Column"},{"location":"notes/sql/sqlite/useful_everyday_useage/#pivot-table","text":"Courtesty of modern-sql Essentially what you need to do is a subquery in which the inner query has the data you want to source and pivot. The outer query will group by a key and explicitly define the columns that should be created from values in a particular row. Use FILTER to filter for the rows that will belong to each column. Note: This should be replaced by CASE statements in the future. SELECT year , SUM(revenue) FILTER (WHERE month = 1) jan_revenue , SUM(revenue) FILTER (WHERE month = 2) feb_revenue ... , SUM(revenue) FILTER (WHERE month = 12) dec_revenue FROM (SELECT invoices.* , EXTRACT(YEAR FROM invoice_date) year , EXTRACT(MONTH FROM invoice_date) month FROM invoices ) invoices GROUP BY year","title":"Pivot Table"},{"location":"notes/sql/sqlite/useful_everyday_useage/#julian-day","text":"Source techonthenet Syntax: The syntax for the julianday function in SQLite is: julianday(timestring [, modifier1, modifier2, ... modifier_n ] ) More concrete example: select mrn, date, julianday(date) as j_day from mrns_dates; output: # mrn date j_day 1 ####### 2011-04-19 2455670.5 2 ####### 2011-09-26 2455830.5 3 ####### 2012-02-01 2455958.5 4 ####### 2012-03-19 2456005.5 5 ####### 2012-04-24 2456041.5 6 ####### 2014-07-30 2456868.5 Notes: A date value. It can be one of the following: timestring Explanation now now is a literal used to return the current date YYYY-MM-DD Date value formatted as 'YYYY-MM-DD' YYYY-MM-DD HH:MM Date value formatted as 'YYYY-MM-DD HH:MM' YYYY-MM-DD HH:MM:SS Date value formatted as 'YYYY-MM-DD HH:MM:SS' YYYY-MM-DD HH:MM:SS.SSS Date value formatted as 'YYYY-MM-DD HH:MM:SS.SSS' HH:MM Date value formatted as 'HH:MM' HH:MM:SS Date value formatted as 'HH:MM:SS' HH:MM:SS.SSS Date value formatted as 'HH:MM:SS.SSS' YYYY-MM-DDTHH:MM Date value formatted as 'YYYY-MM-DDTHH:MM' where T is a literal character separating the date and time portions YYYY-MM-DDTHH:MM:SS Date value formatted as 'YYYY-MM-DDTHH:MM:SS' where T is a literal character separating the date and time portions YYYY-MM-DDTHH:MM:SS.SSS Date value formatted as 'YYYY-MM-DDTHH:MM:SS.SSS' where T is a literal character separating the date and time portions DDDDDDDDDD Julian date number","title":"Julian Day"}]}